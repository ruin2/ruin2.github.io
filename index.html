<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="AIfuns">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="AIfuns">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="李忠宇">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>AIfuns</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AIfuns</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">为中华之富强而读书</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/14/GNN%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/14/GNN%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2/" class="post-title-link" itemprop="url">GNN相关博客</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-14 19:07:06" itemprop="dateCreated datePublished" datetime="2021-05-14T19:07:06+08:00">2021-05-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-15 11:33:50" itemprop="dateModified" datetime="2021-05-15T11:33:50+08:00">2021-05-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文是我学习GNN时浏览国内外博客所学习到的知识汇总，并总结成一些文章分享给大家。</p>
<h1 id="Expressive-power-of-graph-neural-networks-and-the-Weisfeiler-Lehman-test（GNN的表达能力以及WL算法）"><a href="#Expressive-power-of-graph-neural-networks-and-the-Weisfeiler-Lehman-test（GNN的表达能力以及WL算法）" class="headerlink" title="Expressive power of graph neural networks and the Weisfeiler-Lehman test（GNN的表达能力以及WL算法）"></a>Expressive power of graph neural networks and the Weisfeiler-Lehman test（GNN的表达能力以及WL算法）</h1><p><a href="https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49" target="_blank" rel="noopener">原文链接</a></p>
<p>帝国理工教授Michael Bronstein，是我目前看到的，GNN相关的博客数量最高的作者。所以就从这位教授出发，慢慢总结GNN。</p>
<p>传统的神经网络能够逼近任何光滑函数，所以可以得到一个较为理想的准确率。但是对于GNN来说，在有些数据上表现优秀的算法在其他数据集上就突然拉胯（很多GNN都是这样的）。所以我们就想问了，GNN的表达能力到底有多强？</p>
<p>其中一个主要的挑战就是：GNN所面向的图数据同时具有连续性和离散性两种特点。连续性表现在节点与节点之间有边相连，离散性表现在节点是离散的。所以，一个基本的问题就是，GNN是否能够分辨不同的图结构？这是一个经典图论问题，就是“同构判定”，主要是判断两个图是否等价（大家可以这样想，如果你的图神经网络能够表达一个图数据，这不就说明你的神经网络学习到了这组数据的精髓了吗？换个角度，就是神经网络与图数据的同构判定问题。如果神经网络足够优秀，那实际上应该是和你的数据是同构的）。</p>
<blockquote>
<p>同构是在数学对象之间定义的一类映射，它能揭示出在这些对象的属性或者操作之间存在的关系。若这两个数学结构之间存在同构映射，那么这两个结构叫做是同构的。一般来说，如果忽略掉同构的对象的属性或操作的具体定义，单从结构上讲，同构的对象是完全等价的</p>
</blockquote>
<p>而对于图结构来说，同构就是两个图的边集与点集相等，与顺序无关。</p>
<p>那么如何判断两个图是否同构呢？这里就需要介绍一下Weisfeiler-Lehman test<a href="#refer-anchor-1"><sup>1</sup></a>，简称WL。简单来说，WL的中心思想就是使用中心节点和其邻居节点做哈希运算，得到的哈希值赋予中心节点，将这两个步骤迭代N次直至收敛。不过WL只是一个充分但不必要条件，有些时候会失效。如下图所示：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2021/05/14/GNN%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2/2.jpg" width="65%" alt>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      Two non-isomorphic graphs on which the WL graph isomorphism test fails, as evident from the identical colouring it produces. In chemistry, these graphs represent the molecular structure of two different compounds, decalin (left) and bicyclopentyl (right). Figure adapted from [14].
      </div>
</center>





<p>说了这么多，我们就要说说WL和GNN之间的联系了。<br><strong>我们在文章开头时提到过，如果神经网络可以同构与一个图数据，那么这个神经网络就相当于学习到了图表示。</strong> （当然了，上述文字中的同构并不是数学意义上那种严格的同构。我们只是希望神经网络学习到的节点分布能和被学习的图数据的节点分布近似或者相同。这里我们提到了分布，有了分布，就可以引入统计学了呀）实际上我们已经看到，WL中包含了GNN的<font color="red">“消息传递”，“信息聚合”</font>这两个最基本的概念。不过WL是0参数的单层感知。更为具体的相关工作大家可以参考原博客。<br><a href="http://tkipf.github.io/" target="_blank" rel="noopener">参考1，kipf博客</a><br><a href="https://archwalker.github.io/blog/2019/06/22/GNN-Theory-WL.html" target="_blank" rel="noopener">参考2，WL和GNN</a></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2021/05/14/GNN%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2/1.jpg" width="65%" alt>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      WL算法：上面两个图结构的节点分布近似，所以这两个图很可能是同构的。（事实上就是同构的，大家稍微思考一下就能够发现）
      </div>
</center>

<p>当然了，k-WL是一个更强的收敛，区别在于每个节点都会有k个哈希值。<a href="#refer-anchor-2"><sup>2</sup></a></p>
<p>那么，我们就可以根据k-WL来构建一个多层GNN了。虽然从理论的角度出发，我们有了一个不错的模型，但是最近的研究成果<a href="#refer-anchor-3"><sup>3</sup></a>表明，先进的GNN模型实际上并不如以前的算法技术。这在机器学习中并不少见，因为理论和实践之间往往存在巨大差距。其中一个解释可能是基准本身的缺陷。但或许更为深刻的原因是,更好的表达能力不一定提供更好的概括(有时恰恰相反),此外,图同构可能无法准确的捕捉数据特征，我们后续再讨论这个问题。当然了，GNN的工作还是非常富有成效的，尤其是在还没有应用图方法的深度学习领域<a href="#refer-anchor-4"><sup>4</sup></a>。</p>
<div id="refer-anchor-1"></div>

<ul>
<li>[1] <a href="https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/#:~:text=The%20core%20idea%20of%20the,used%20to%20check%20for%20isomorphism." target="_blank" rel="noopener">B. Weisfeiler, A. Lehman, The reduction of a graph to canonical form and the algebra which appears therein (1968). Nauchno-Technicheskaya Informatsia 2(9):12–16. English translation and the original Russian version, which contains a pun in the form of an unusual Cyrillic notation (Операция „Ы“) referring to the eponymous Soviet blockbuster from three years earlier. See also a popular exposition in this blog post. Lehman is sometimes also spelled “Leman”, however, given the Germanic origin of the surname, I prefer the more accurate former variant.</a></li>
</ul>
<div id="refer-anchor-2"></div>

<ul>
<li><p>[2] <a href>Weisfeiler-Lehman hierarchy. One direction of extending the results of Xu and Morris was using more powerful graph isomorphism tests. Proposed by László Babai, the k-WL test is a higher-order extension of the Weisfeiler-Lehman algorithm that works on k-tuples instead of individual nodes. With the exception of 1-WL and 2-WL tests that are equivalent, (k+1)-WL is strictly stronger than k-WL, for any k≥2, i.e. there exist examples of graphs on which k-WL fails and (k+1)-WL succeeds, but not vice versa. k-WL is thus a hierarchy or increasingly more powerful graph isomorphism tests, sometimes referred to as the Weisfeiler-Lehman hierarchy.There exist several variants of WL tests that have different computational and theoretical properties, and the terminology is rather confusing: readers are advised to clearly understand what exactly different authors mean by “k-WL”. Some authors, e.g. Sato and Maron, distinguish between WL and “folklore” WL (FWL) tests, which mainly differ in the colour refinement step. The k-FWL test is equivalent to (k+1)-WL. The set k-WL algorithm used by Morris is another variant that has lower memory complexity but is strictly weaker than k-WL.</a></p>
<div id="refer-anchor-3"></div></li>
<li>[3] <a href="https://arxiv.org/abs/2003.00982" target="_blank" rel="noopener">V. P. Dwivedi et al. Benchmarking graph neural networks (2020). arXiv: 2003.00982.</a><div id="refer-anchor-4"></div></li>
<li>[4] <a href>To be historically accurate, the WL formalism is not new to the machine learning community. The seminal paper of N. Shervashidze and K. M. Borgwardt, Fast subtree kernels on graphs (2009). Proc. NIPS was, to the best of my knowledge, the first to use this construction before the recent resurgence of deep neural networks and precedes the works discussed in this post by nearly a decade.</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/13/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A0%B8%E6%96%B9%E6%B3%95%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/13/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A0%B8%E6%96%B9%E6%B3%95%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/" class="post-title-link" itemprop="url">图神经网络之核方法与正则化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-13 17:27:27" itemprop="dateCreated datePublished" datetime="2021-05-13T17:27:27+08:00">2021-05-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-14 16:38:21" itemprop="dateModified" datetime="2021-05-14T16:38:21+08:00">2021-05-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/27/README/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/27/README/" class="post-title-link" itemprop="url">README</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-27 16:59:57" itemprop="dateCreated datePublished" datetime="2021-03-27T16:59:57+08:00">2021-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-15 11:51:24" itemprop="dateModified" datetime="2021-05-15T11:51:24+08:00">2021-05-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="myblog"><a href="#myblog" class="headerlink" title="myblog"></a>myblog</h1><p>个人读书笔记</p>
<blockquote>
<p>当了那么就的白嫖怪，也是时候为社会做出贡献了。</p>
</blockquote>
<p>并对<a href="https://github.com/miracleyoo/Markdown4Zhihu" target="_blank" rel="noopener">用于知乎的markdown</a>进行了一些修改，更加适合hexo用户。</p>
<blockquote>
<p>zhihu-publisher.py ： 我做了一些简单的修改，让它在文件内也能直接运行，就不需要命令行那么麻烦了。生成出来的文件会在D盘下的Data文件夹内，当然您也可以做出适合自己的修改。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">图神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-22 16:55:49" itemprop="dateCreated datePublished" datetime="2021-03-22T16:55:49+08:00">2021-03-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-13 17:25:35" itemprop="dateModified" datetime="2021-05-13T17:25:35+08:00">2021-05-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://ruin2.github.io/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">github版本在这里，最好用谷歌浏览器</a></p>
<h1 id="【图神经网络】文献阅读笔记"><a href="#【图神经网络】文献阅读笔记" class="headerlink" title="【图神经网络】文献阅读笔记"></a>【图神经网络】文献阅读笔记</h1><p><strong>最近在研究图神经网络相关的内容，所以写下这篇阅读笔记。个人能力有限，所以如果您在阅读的过程中找出了错误请务必指出。另不吝赐教，相互交流学习。本文也会持续更新。<span class="emoji" alias="stuck_out_tongue" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f61b.png?v8">&#x1f61b;</span></strong></p>
<blockquote>
<p><em>个人认为GNN是目前来看，唯一一个能对抗Transformer的框架了。另外，深度学习条件下的GNN的历史虽然差不多和Transformer一样，但对于GNN的研究还差得太多，因此还不够深刻和彻底，这就导致了性能和应用两方面的拉胯。不过也正是GNN的各种研究方面的缺陷才让我们有了“可乘之机”，毕竟相关领域还不是太卷，广阔天地大有可为。正所谓“熟读洋屁三百篇，不会放屁也会吟”，奥利给！兄弟们干了！</em></p>
</blockquote>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>众所周知，自然界的数据绝大部分都是非欧的，这些数据无头无尾，一团乱麻，处理起来非常棘手。</p>
<p>目前的四大主流深度学习算法(算子)：全连接，CNN,RNN,ATT（我们先暂时忽略对抗，自编码以及强化学习）。它们对于图结构的数据的处理能力有限，但问题是深度学习发展到今天，也只有这几种常见的方法。这时候研究者就面临了两种选择：</p>
<ol>
<li>要么让算法去适应非欧的数据（改算法不改数据结构）</li>
<li>要么让非欧的数据去适应算法（改数据结构不改算法）</li>
</ol>
<p>道理是这么个道理，但我的分类方法还是相对粗糙的。学术界的两种主流分类方法是：</p>
<ol>
<li>节点为主的建模方法，边为主的建模方法，节点和边同时建模的方法。</li>
<li>图卷积（CNN）、时序图（RNN）、图注意力（ATT）…</li>
</ol>
<p>无论我们采用何种分类方式，我们总是绕不开两个核心问题：</p>
<ol>
<li>图的拓扑结构应该如何表示？</li>
<li>节点和边的信息应该如何学习？</li>
</ol>
<p>在此，我们将图分成了两部分，即：<span class="emoji" alias="star" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">&#x2b50;</span>结构和信息<span class="emoji" alias="star" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">&#x2b50;</span>。更直白的来说，<font color="red">图=拓扑结构+信息</font>。</p>
<p>而数学中对图的定义则是有序的节点集和边集的集合。我们所采取的这种定义方式只是比较直接的指出了图神经网络研究的主要难点，本质上和数学中的定义是相同的。我们将在后面不断的讨论这两个难点。</p>
<p>另外，单独一个问题的研究都是没有意义的，因为去掉任何一个要素（结构或者信息）都不能构成一个完整的图。所以，我们还需要找到一种连结两个问题的方法。幸运的是，我们目前有两种方法同时处理结构和信息，一个是谱方法（特征值分解），另一个是采样。我个人比较支持采样方法。</p>
<p>好了，既然现在我们知道了图的研究对象是其结构和信息，获取样本的方法是谱分解或者采样，那么这些数据应该如何计算呢？</p>
<p>还记得马克思的那句话吗？<strong>人是其社会关系的总和</strong>。这句话很好的启发了我们，我们可以得到<font color="red">节点是其关系信息的总和</font>，这些关系包括了节点的邻居，邻边和其自身。然后我们将这些关系信息通过某种方式聚合起来，就可以得到该节点的表示了。</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.gif" alt></p>
<p>我们的学习顺序和之前的综述性文章是不太一样的。<br>我们的论文阅读顺序如下所示：</p>
<ol>
<li>图神经网络的基本建模思路（MPNN）</li>
<li>采样方法（GraphSAGE等）</li>
<li>时序图</li>
<li>图卷积</li>
<li>图注意力</li>
</ol>
<h2 id="本文统一的符号"><a href="#本文统一的符号" class="headerlink" title="本文统一的符号"></a>本文统一的符号</h2><blockquote>
<p>先写这么多，会慢慢扩展和补充</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$D=(V,E)$</td>
<td style="text-align:center">图的定义（vertex \&amp; edge）</td>
</tr>
<tr>
<td style="text-align:center">$N,M$</td>
<td style="text-align:center">节点和边的数量</td>
</tr>
<tr>
<td style="text-align:center">$V = { v_1,\cdots,v_n }$</td>
<td style="text-align:center">顶点集</td>
</tr>
<tr>
<td style="text-align:center">$F^V,F^E$</td>
<td style="text-align:center">节点、边的特征（属性）</td>
</tr>
<tr>
<td style="text-align:center">$A$</td>
<td style="text-align:center">邻接矩阵</td>
</tr>
<tr>
<td style="text-align:center">$D(i,i) = \sum<em>j A</em>{ij}$</td>
<td style="text-align:center">对角度矩阵</td>
</tr>
<tr>
<td style="text-align:center">$L=D-A$</td>
<td style="text-align:center">拉普拉斯矩阵</td>
</tr>
<tr>
<td style="text-align:center">$Q\Lambda Q^T=L$</td>
<td style="text-align:center">拉普拉斯矩阵的特征分解</td>
</tr>
<tr>
<td style="text-align:center">$P=D^{-1}A$</td>
<td style="text-align:center">转移矩阵</td>
</tr>
<tr>
<td style="text-align:center">$\mathcal{N}_k(v)$</td>
<td style="text-align:center">节点$v$的$k$阶邻居,没写$k$就是$k=1$</td>
</tr>
<tr>
<td style="text-align:center">$H^l$</td>
<td style="text-align:center">第$l$层的隐藏状态</td>
</tr>
<tr>
<td style="text-align:center">$f_l$</td>
<td style="text-align:center">$H^l$的维度</td>
</tr>
<tr>
<td style="text-align:center">$\sigma(\cdot)$</td>
<td style="text-align:center">非线性激活函数</td>
</tr>
<tr>
<td style="text-align:center">$\Theta$</td>
<td style="text-align:center">可学习的参数</td>
</tr>
<tr>
<td style="text-align:center">$s$</td>
<td style="text-align:center">采样大小</td>
</tr>
<tr>
<td style="text-align:center">$T$</td>
<td style="text-align:center">时间步</td>
</tr>
<tr>
<td style="text-align:center">$U$</td>
<td style="text-align:center">状态更新方程（update），特指隐藏层状态更新</td>
</tr>
<tr>
<td style="text-align:center">$M$</td>
<td style="text-align:center">消息（message），实际就是信息</td>
</tr>
<tr>
<td style="text-align:center">$AGG$</td>
<td style="text-align:center">聚合方程（或聚合器）（aggregater），特指节点或者边上的显示状态更新</td>
</tr>
</tbody>
</table>
</div>
<h2 id="文献阅读笔记法"><a href="#文献阅读笔记法" class="headerlink" title="文献阅读笔记法"></a>文献阅读笔记法</h2><blockquote>
<p>按照这个顺序记笔记</p>
</blockquote>
<p>【<font color="yellow">论文题目</font>】:</p>
<p>【<font color="yellow">概述</font>】:</p>
<p>【<font color="yellow">需要解决的问题</font>】:</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<h2 id="相关术语及概念"><a href="#相关术语及概念" class="headerlink" title="相关术语及概念"></a>相关术语及概念</h2><blockquote>
<p>在进入正式的学习之前，我们需要对该领域的专业术语和概念进行一定的了解，方便同行之间的沟通和交流。不敢保证能达到智取威虎山的效果，但至少能保证不至于张嘴就被击毙。</p>
</blockquote>
<h2 id="A-New-Model-for-Learning-in-Graph-Domains"><a href="#A-New-Model-for-Learning-in-Graph-Domains" class="headerlink" title="A New Model for Learning in Graph Domains"></a>A New Model for Learning in Graph Domains</h2><p>【<font color="yellow">论文题目</font>】：图领域的一种新的学习模型</p>
<p>【<font color="yellow">概述</font>】：在一些场景中，信息是自然的以图的形式所表示的。虽然这些算法能够将图信息表示成一组向量，但通常来说，处理过程会丢失掉图的拓扑结构信息，而且其向量表示也是受算法本身约束的。这片文章提出了一种<font color="red">广义的RNN</font>，能够处理许多种图结构的信息。另外，本文提出了一些重要的图神经网络的术语和概念。</p>
<p>【<font color="yellow">需要解决的问题</font>】：图神经网络建模方法，以及节点和边的表示方法。</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：假设数据都可以表示成图结构的。</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：因为自然界的数据绝大多数都是以图形式出现的，所以我们必须解决图建模的基本算法问题。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：可以用高维形式进行图表示，提升相关任务的准确率。</p>
<p>【<font color="yellow">解决问题的具体方法及实验</font>】：</p>
<ol>
<li><p>论文提出了节点级(node focused)和图级(graph focused)的两种建模方法，区别就是预测的时候，是预测了图的一部分信息还是预测整张图的信息。虽然我个人感觉这种分类方式已经有点过时了，而且以当前的算力来看，似乎又会造成一定程度上的混淆，所以我个人将不再讨论这个分类方式。</p>
</li>
<li><p>广义RNN ： 首先通过随机游走的方式得到一个序列，然后用这个序列上的每一个节点及其邻居节点作为一个子图，再把这个子图中的所有信息通过一定的方式聚合到一起，形成一个新的节点。这样，我们就得到了一个序列化的子图，而且这个序列化的子图可以直接使用RNN计算，因此就成了广义RNN。在这篇论文中，广义RNN实际上就是GNN的原型。</p>
</li>
<li><p>消息的传递、聚合和更新：</p>
<script type="math/tex; mode=display">v_{t+1}=f_{\theta}(v_t,\mathcal{N}(v_t),l_{\mathcal{N}(v_t)} )</script><script type="math/tex; mode=display">o(v_{t+1})=g_{\theta}(v_t,v_{t+1})</script><p> 上式对应于原文中的等式$(7)$。</p>
<p> 第一行实际上就是消息的传递，$l$是节点的标签信息，也就是说，GNN可以是有监督的，也可以是自监督的。</p>
<p> 第二行中相当于节点信息的更新。</p>
<p> 那么聚合体现在哪里呢？实际上原文是没有“聚合”这个概念的，传递和聚合的过程被一同合并在了$f_{\theta}$里面。</p>
<p> 更为具体的概念我们将会在之后的论文中讨论。</p>
</li>
</ol>
<p>用前向传播来计算数值解<br>用聚合信息的方法来计算表示解</p>
<p>【<font color="yellow">结论</font>】：<font color="red">图神经网络=采样+消息传递+消息聚合+状态更新</font>。这四要素构成GNN的基石，缺一不可。</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：这篇论文提出了最原始的采样、传递、聚合、更新的概念（虽然确切的来说，是没有聚合的概念）。让我们知道了GNN中的节点的状态参数应该如何更新。</p>
<p>RNN可以看成是一种GNN的特例，反过来说就是：GNN是广义的RNN。再比如LSTM，其中的门控机制可以看做是聚合方程，而其马尔科夫假设也可以被看做消息传递。但GNN和RNN最大的本质区别就是节点的度。在RNN中，节点的度都是2（除了头尾），而GNN的节点度就变化万千了，因此形成了特殊的拓扑结构，也正因如此，GNN才需要各种采样技术，而RNN却不需要（RNN只需要顺序遍历即可）。虽然原文没有明确的说明，但我们读完论文就能感受到。</p>
<h2 id="The-Graph-Neural-Network-Model"><a href="#The-Graph-Neural-Network-Model" class="headerlink" title="The Graph Neural Network Model"></a>The Graph Neural Network Model</h2><p>【<font color="yellow">论文题目</font>】:</p>
<p>【<font color="yellow">概述</font>】:</p>
<p>【<font color="yellow">需要解决的问题</font>】:</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<h2 id="Introduction-to-GraphNeural-Networks"><a href="#Introduction-to-GraphNeural-Networks" class="headerlink" title="Introduction to GraphNeural Networks"></a>Introduction to GraphNeural Networks</h2><p>在我的印象中这是比较早的几篇综述文章之一。我们仅截取其中一节：第四节，Vanilla Graph Neural Networks。这一节讲道理是我最喜欢的，介绍的</p>
<p>【<font color="yellow">论文题目</font>】:图神经网络介绍之图神经网络初代机</p>
<p>【<font color="yellow">概述</font>】:</p>
<p>【<font color="yellow">需要解决的问题</font>】:</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<h2 id="Neural-Message-Passing-for-Quantum-Chemistry"><a href="#Neural-Message-Passing-for-Quantum-Chemistry" class="headerlink" title="Neural Message Passing for Quantum Chemistry"></a>Neural Message Passing for Quantum Chemistry</h2><p>【<font color="yellow">论文题目</font>】：<font color="cyan">量子化学的消息传递神经网络</font></p>
<p>【<font color="yellow">概述</font>】：化学分子可以看做是图结构，而图结构又有许多相似之处，所以可以用一种监督学习的方法来预测分子的属性，并为新药发现和材料科学做贡献。传统的监督学习方法已经取得了很好的效果，但是作者希望能够用深度学习的方法把成果推广开来。因此他们发明了MPNN框架，并希望在此框架下，各种算法变体能取得进一步的准确度，达到数据集的极限。</p>
<p>【<font color="yellow">需要解决的问题</font>】：设立一个图算法的框架，并在这个框架下寻找更多更准确更快速的，用于预测分子属性的变体模型。</p>
<p>【<font color="yellow">这个问题为什么重要</font>】： 快速解决分子和材料的属性预测。</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<ol>
<li>在同一种图框架下</li>
<li>算法不受同构（例如化学中的手性）的影响</li>
<li>算法的计算效率足够高</li>
</ol>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<ol>
<li>预测时间会下降。用DFT(离散傅里叶变换)做预测需要$10^3$量级的时间，而使用MPNN则可以降低到$10^{-2}$量级。</li>
<li>找到更多的变体模型，这些模型可以完全准确的预测到分子属性，或者是逼近预测的极限。</li>
<li>可以促进新药发现和材料科学。（天坑专业自救指南）</li>
</ol>
<p>【<font color="yellow">解决问题的具体方法及实验</font>】：</p>
<p><strong>在算法方面</strong>：</p>
<ul>
<li><p>节点之间的消息传递：</p>
<script type="math/tex; mode=display">m_v^{t+1} = \sum_{w \in N(v)} M_t(h_v^t,h_w^t,F^{e})</script><p>  上式的含义是，节点$v$的消息传递是汇聚了它自身的信息$h_v^t$以及它邻居节点的消息$h_w^t$和邻边的消息$F^{e}$。但凡能用上的消息基本都用上了。</p>
</li>
</ul>
<ul>
<li><p>消息更新的方式：</p>
<script type="math/tex; mode=display">h_v^{t+1}=U(h_v^t,m_v^{t+1})</script><p>  这里只需要说一下$U$实际上是一个状态更新函数。通常用<code>add</code>,<code>mean</code>,<code>max</code>.但<code>max</code>是不可微的，所以使用的时候要注意。</p>
</li>
<li><p>图表征方法（readout）:</p>
<script type="math/tex; mode=display">y=R(\{h_v^T|v \in G\})</script><p>  $y$是一个向量，它表征了整个图。</p>
</li>
</ul>
<p><strong>在实验数据方面</strong>，本论文在其开发的图建模环境MPNN下，通过错误率和运行时间两方面来证明MPNN框架的可行性和实用性。但是具体的数据我并没有认真研究，因为MPNN的亮点是其图建模的思想（毕竟我太菜了，也找不到比MPNN更早的相关研究了。主要是懒）。</p>
<p>【<font color="yellow">结论</font>】：MPNN确实从准确率和运行时间两方面达到了理想的结果。并且实现了一些图建模的最基本要素，如：消息传递，节点和边的参数更新，图表征（readout，我翻译成图表征，因为readout指的是把一张图表示为一个向量。）</p>
<p>【<font color="yellow">源码解析</font>】：MPNN确实是有一套自己的代码框架，但是我水平有限，没有看懂<span class="emoji" alias="joy" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8">&#x1f602;</span>。不过MPNN的基本思想被研究者们广泛认同，被PYG（pytorch_geometric）继承，然后PYG有被OGB（open graph benchmark）所继承，所以我们来看一下OGB的源码即可<span class="emoji" alias="smile" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span>。（torch\geometric\nn\conv\message_passing.py）</p>
<p><a href="https://zhuanlan.zhihu.com/p/165996331" target="_blank" rel="noopener">关于OGB，可以参考这篇文章</a></p>
<p><a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html" target="_blank" rel="noopener">MPNN的英文文档</a></p>
<p>首先，在OGB中，我们定义MPNN的算法框架：</p>
<script type="math/tex; mode=display">v^{t+1} = \gamma_t(v^t,\square_{u\in \mathcal{N(v)}} \phi(v^t,u^t,F^e))</script><p>其中，$\square$是一个可微的且组合不变的方程，比如<code>sum</code>,<code>mean</code>,<code>max</code>。而$\gamma$和$\phi$则是任意可微方程，如MLP（全连接）。在本文中，$\gamma$就相当于$U$或者$AGG$，$U$和$AGG$的区别仅限于运算的对象，是负责聚合和更新信息的。而$\phi$则相当于$M$，负责收集信息。</p>
<p><code>MessagePassing</code>类内有三个主要方法：</p>
<ul>
<li><code>MessagePassing.propagate(edge_index, size=None, **kwargs)</code>:该方法的输入参数是<code>edge_index</code>，知道了边的编号之后，自然就知道了输入节点$v$和输出节点$u$了。该方法会在内部调用<code>MessagePassing.aggregate</code>和<code>MessagePassing.update</code>。实际上，每次采样的结果就相当于一个子图，而且这个子图还是一个二部图。</li>
<li><code>MessagePassing.message</code>：相当于$\phi_{\mathbf{\Theta}}$。这个方法的输入就是<code>MessagePassing.propagate</code>的输入，</li>
<li><code>MessagePassing.aggregate</code>:相当于$\square_{u\in \mathcal{N(v)}}$，</li>
<li><code>MessagePassing.update</code>:相当于$\gamma_{\mathbf{\Theta}}$这一部分，对每一个节点$v$进行聚合。其输入是<code>MessagePassing.aggregate</code>的输出。</li>
</ul>
<p>【<font color="yellow">个人评价</font>】：<br>MPNN的思想非常简单且可行：节点就是其关系的总和。简单好用易于理解，所以成为了OGB的基础层。</p>
<h1 id="图上的采样方法"><a href="#图上的采样方法" class="headerlink" title="图上的采样方法"></a>图上的采样方法</h1><h2 id="DeepWalk-Online-Learning-of-Social-Representations"><a href="#DeepWalk-Online-Learning-of-Social-Representations" class="headerlink" title="DeepWalk: Online Learning of Social Representations"></a>DeepWalk: Online Learning of Social Representations</h2><p>【<font color="yellow">论文题目</font>】：<font color="cyan">深度随机游走：社团表示的在线学习</font></p>
<p>【<font color="yellow">个人评价</font>】： 真·随机游走。第一个使用深度优先采样的图深度学习方法。</p>
<h2 id="LINE-Large-scale-Information-Network-Embedding"><a href="#LINE-Large-scale-Information-Network-Embedding" class="headerlink" title="LINE: Large-scale Information Network Embedding"></a>LINE: Large-scale Information Network Embedding</h2><p>【<font color="yellow">论文题目</font>】：【LINE】大规模信息网络嵌入</p>
<p>【<font color="yellow">概述</font>】：LINE使用了广度优先采样+深度优先采样</p>
<p>【<font color="yellow">需要解决的问题</font>】：大规模网络节点的向量空间嵌入问题</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.jpg" alt></p>
<p>从直觉的角度来说，两个有着相同朋友圈的人，其兴趣也应该接近，因此这两个人很有可能成为好朋友。从上图中的5，6节点可以看出，这两个节点有着相同的“朋友圈”，所以他们之间的向量应该较为接近。</p>
<p>另外，6，7的嵌入向量也应该接近，因为这两个节点是直接相连（而且关系也比较强，因为边比较粗）</p>
<p>根据以上两个直觉，LINE定义了两种相似度。</p>
<ol>
<li>一阶相似：类似于6，7。如果两个节点之间没有边<strong>直接</strong>相连，这两个节点的一阶相似就是0.</li>
<li>二阶相似：类似于5，6。如果两个节点之间没有相邻的其它节点，那么这两个节点的二阶相似度就是0.</li>
</ol>
<p>【<font color="yellow">这个问题为什么重要</font>】：图数据是离散的。如果能把节点信息向量化，那么我们可以获得<strong>更丰富的表示信息</strong>。这些表示信息对后续任务的帮助巨大。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：这种采样方法相当于NLP的pre-train。对下游任务是有很大帮助的。</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>LINE的目标函数是一个由一阶相似和二阶相似共同拼凑的。</p>
<p>首先是一阶近似的目标函数：</p>
<ol>
<li>定义两个节点之间的概率<script type="math/tex; mode=display">p_{ij} = \frac{1}{1+exp(-v_i,v_j)} \tag{1}</script></li>
<li><p>定义两个节点之间的经验概率。这里需要说明的是，$z<em>{ij}$ 是一个非0即1的数。因为边权的数值是不确定的，有时候很大有时候很小，这会导致梯度的大幅波动，因为方差实在是太大了。所以作者很巧妙的使用了01数值来局部归一化。所以，$Z$ 就代表了边权的数值（只能是整数），$z</em>{ij}$ 要看两个节点间是否有边相连，有就是1，没有就是0.</p>
<script type="math/tex; mode=display">\hat{p}_{ij} = \frac{z_{ij}}{Z} \tag{2}</script><script type="math/tex; mode=display">Z = \sum_{(i,j)\in E}z_{ij}</script></li>
<li><p>$(1)$和$(2)$代表了两个节点之间的相关性，那么我们直接联立两个方程，就可以得到一个误差值，这个误差值就代表了两个节点的一阶相似度（也可以看成是距离）。在原文中，作者使用了KL散度来计算这个距离。</p>
<script type="math/tex; mode=display">d(p,\hat{p}) = KL(p,\hat{p}) = \sum_{(i,j)\in E}z_{ij}\log{p(v_i,v_j)}</script></li>
</ol>
<p>然后是二阶近似的目标函数：</p>
<p>根据之前对二阶近似的定义，如果想计算两个节点之间的二阶近似的目标函数，就是看看第二个节点在第一个节点的朋友圈（目标节点的全部邻居）当中的份量。</p>
<p>这个份量，就可以用<strong>条件概率</strong>来描述。</p>
<ol>
<li>定义条件概率</li>
</ol>
<script type="math/tex; mode=display">p(v_j|v_i) = \frac{exp(v_i \cdot v_j)}{\sum_{\mathcal{v_k \in N(v_i)}}exp(v_k \cdot v_i)} \tag{4}</script><ol>
<li>计算二阶相似度。其中，$\lambda$代表节点的权值，毕竟节点和节点的重要程度也是不一样的。同样的，$d$也是用KL散度代替。然后把$(5)$式化简一下，就能得到$(6)$</li>
</ol>
<script type="math/tex; mode=display">O_2 = \sum_{v_k\in \mathcal{N}(v_i)} \lambda_i d(\hat{p}(\cdot|v_i),p(\cdot|v_i)) \tag{5}</script><script type="math/tex; mode=display">O_2 = \sum_{v_k\in \mathcal{N}(v_i)} z_{ij} \log{p(v_j|v_i)} \tag{6}</script><p>但比较拉胯的是，原文里的训练方法是分别训练，然后把两种近似情况得到的向量拼接在一起。</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：仅从采样方法来看，LINE = deepwalk + BFS。采样的方法变多了，因此能够得到更好的效果。</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>这个算法的问题我个人感觉非常大（先写着，以后慢慢补充）。因为论文为了省时省力（可能是受限于当时的硬件设备），context变量本来是$v_i$的全部邻居节点，但结果却只是$v_j$的嵌入。</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<p>低情商：这个算法有问题</p>
<p>高情商：这个算法仍旧有很大的改进空间。</p>
<p>开个玩笑。实际上LINE可以看做是DEEPWALK的改进，毕竟deepwalk只是用了DFS做采样，而LINE用了BFS。但LINE的问题我个人认为有两处。</p>
<ol>
<li><p>在一阶相似的定义方面：我们观察公式可以发现，$p$是节点向量的内积，而$\hat{p}$是与节点之间的边权相关的量。一阶相似的目标函数是希望内积和边权尽量相同。我们现在用极端条件来验证一下：</p>
<ul>
<li><p>当$v_i=v_j$时，带入公式$(1)$可得：$p = \frac{1}{1+exp(-1)} =0.73$。这个误差大概是0.27</p>
</li>
<li><p>当$v_i \perp v_j$时，带入公式$(1)$可得：$p =\frac{1}{1+exp(0)}= 0.5$,这个误差是0.5</p>
</li>
</ul>
<p>所以，我个人认为用内积来定义相似度是没有问题的，但问题出在相似度的解析方程，至少文中给出的解析方程是有问题的。另外，LINE将相似度与边权划等号，在直觉上是说得通的，但更具体的形式还要继续探索。</p>
</li>
<li>在二阶相似的定义方面：因为在源码中，$v_j$的向量和context(也就是$v_k$，是$v_i$的全部邻域)的向量实际上都是依靠$v_j$做嵌入的，所以当前节点$v_i$的邻域信息，也就是BFS采样，并没有充分利用。如果简化来看，这种方法甚至比deepwalk还简单（LINE在极端条件下实际上就采样了一个点…也就是$v_j$）。</li>
</ol>
<h2 id="node2vec-Scalable-Feature-Learning-for-Networks"><a href="#node2vec-Scalable-Feature-Learning-for-Networks" class="headerlink" title="node2vec: Scalable Feature Learning for Networks"></a>node2vec: Scalable Feature Learning for Networks</h2><p>【<font color="yellow">论文题目</font>】:网络的可扩展特征学习</p>
<p>【<font color="yellow">概述</font>】:如何捕捉邻居节点的多样性特征一直以来都是一个难题。node2vec首次提出了一种启发式的灵活采样方法，该方法结合了DFS和BFS的精髓，并且作者认为这种采样方法能够有效的反映出节点的特性。其中的核心观点就是：</p>
<ol>
<li>邻近的节点应该相似</li>
<li>“朋友圈”相同的节点也应该相似。</li>
</ol>
<p>【<font color="yellow">需要解决的问题</font>】:蕴含更多更丰富信息的采样方法。</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：节点的向量表示在不同的采样策略下，有着不同的表示方法，而且原文中也指出，目前（2016）还没有一种通用的良好表示方法。所以，采样方法在很大程度上决定了节点向量表示学习的能力。而深度学习框架下的采样方法也几乎都是DFS和BFS，一种很自然的想法就是将二者结合。但之前的LINE算法在本质上属于拼接了DFS和BFS，而DeepWalk只有BFS，所以node2vec就搞出了启发式算法来融合DFS和BFS，作者们也认为，灵活的采样方法能够获取更加丰富的表示信息。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：让节点的嵌入向量包含更多更丰富的信息，用以处理下游任务。并且可以更快更好的学习节点的嵌入表示。</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<ol>
<li>skip-gram:<script type="math/tex; mode=display">\max_v \sum_{u\in \mathcal{N}(v)} \log{p}(\mathcal{N}(v)|v) \tag{1}</script>其中，$v$代表节点的向量。这个公式的含义就是，依靠节点$v$来最大可能的预测其周围邻居。相当于NLP中的skip-gram</li>
<li>条件独立:<script type="math/tex; mode=display">\log{p}(\mathcal{N}(v)|v)=\prod_{u\in \mathcal{v}}p(u|v) \tag{2}</script>有了这个假设，子图就可以按节点进行分割了。</li>
<li>特征空间的对称性（我个人觉得，写成排列无关性更好）：<script type="math/tex; mode=display">p(n|v)=\frac{exp(n\cdot v)}{\sum_{u\in \mathcal{v}} exp(u\cdot v)} \tag{3}</script>这个公式是说，起始点$v$和终点$n$之间的概率关系与两者之间节点的排列顺序无关。<br>根据上述三个等式，最终的优化目标可以写为：<script type="math/tex; mode=display">\max_{v} \sum_{u\in \mathcal{N}(v)}[-\log{Z_u} + \sum_{n\in \mathcal{N}(u)}n\cdot u]</script>其中：<script type="math/tex; mode=display">Z_u = \sum_{u\in \mathcal{N}(v)} u\cdot v</script></li>
<li><p>search bias:这是一种启发式的搜索策略。就是在搜索的过程中设置不同的转移概率$\alpha$（可以理解为一种阻力）。<br><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.jpg" alt></p>
<ol>
<li>如果节点$t$和$v$共享同一个邻居$x_1$，那么$\alpha$就是1</li>
<li>$t$和$v$构成的边的$\alpha$是$\frac{1}{p}$</li>
<li><p>其他情况为$\frac{1}{q}$</p>
<p>当然了，真正的$\alpha$还需要和边权$w<em>{vu}$相乘，所以真正的转移概率应该是$\alpha \cdot w</em>{vu}$。<br>另外，每次采样过后，中心节点都会变更，这样才能保证采样的多样性。</p>
</li>
</ol>
</li>
</ol>
<p>【<font color="yellow">实验</font>】：所有的实验结果以OGB为准。</p>
<p>【<font color="yellow">结论</font>】：转移概率在本文中充当了胶水的角色，很好的将BFS和DFS结合在了一起。这种启发式的算法比DeepWalk和LINE这两种最优化算法要好一些。</p>
<p>【<font color="yellow">源码解析</font>】：以后再补。</p>
<p>【<font color="yellow">个人评价</font>】：从论文的实验结果来看，启发式确实是比最优化的采样方法要好。但更重要的信息是，不同的采样方法对结果的影响非常巨大，而且node2vec的超参$p$和$q$仍旧有讨论的空间，但更大的舞台还是设计新的采样方法。</p>
<h2 id="Inductive-Representation-Learning-on-Large-Graphs"><a href="#Inductive-Representation-Learning-on-Large-Graphs" class="headerlink" title="Inductive Representation Learning on Large Graphs"></a>Inductive Representation Learning on Large Graphs</h2><p>【<font color="yellow">论文题目</font>】：<font color="cyan"> 大图的归纳表示学习 </font></p>
<p>【<font color="yellow">概述</font>】：之前的节点嵌入方法都是直接计算一整个较小的图，因此泛化能力差，而且对于新的节点的嵌入显得束手无策。本文采用近邻采样的方法对大图中的节点的嵌入向量进行学习。这种学习方法并非传统的直接学习节点的向量表示，而是去训练聚合器来间接学习节点的向量表示。而且学习目标不仅仅是节点的向量表示，还包括了节点的分布。这种方法速度快，效果好，对未知的节点嵌入也有较高的鲁棒性。</p>
<p>【<font color="yellow">需要解决的问题</font>】：大图中的节点表示学习</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：大图</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：节点的向量表示是下游任务的基础。不解决这个问题很难搞下去。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：解决了节点向量的表示，才能为之后的任务服务。比如推荐系统，只有解决了当前用户节点的表示，才能为新用户的行为进行预测。</p>
<p>【<font color="yellow">解决问题的具体方法及实验</font>】：</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3.jpg" alt="GraphSAGE算法示意图"></p>
<p>如图所示：</p>
<p>(1)表示一个节点的1阶邻居和2阶邻居，这些就是我们所要聚合的信息。</p>
<p>(2)表示对当前节点的信息以及它邻居的信息进行聚合。我们可以很清楚的看到，1阶聚合器和2阶聚合器是不同的，这也很好理解，毕竟你亲戚的亲戚不一定是你的亲戚。所以要区别对待。</p>
<p>(3)则是一个目标函数，要求当前节点去预测他的邻居以及其自身的标签。（有点类似于word2vec的CBOW步骤）</p>
<p>算法的伪代码如下所示：</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.jpg" alt></p>
<p>步骤4聚合了全部的邻居信息，而步骤5则是对当前节点的状态更新。步骤7则是一个常见操作，因为不断的聚合会使节点的模长爆炸，所以需要把隐藏层的模长限定在$[0,1]$之间。这个操作就是求一个向量的单位向量。</p>
<p>最后是目标方程。</p>
<script type="math/tex; mode=display">J = -log(\sigma(z_u^Tz_v)) - Q\mathbb{E}_{v_n \sim P_{n}(v) }log(\sigma(z_u^Tz_{v_n}))</script><p>其中，$\sigma$代表了softmax函数，$z_u$是邻居节点的向量，$z_v$是中心节点的向量。$z_u^Tz_v$代表了两者之间的内积，并用内积来代表相似度。而$P$则代表了负采样，$v_n$就是那个被负采样出来的虚假中心节点，它和$z_u$之间的内积自然会小。所以就用这样一真一假的操作让聚合器去学习。至于Q，那就是负采样的样本个数了。</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：该方法的实现是较为简单的，主要就是将聚合器替换为一种带参的方程，之前提到过的四种常见算子大家有兴趣都可以挨个试一遍。（我太懒了，实际上也根本没有实践过。。。而论文的实际效果又不一定那么好，所以有空还是需要去写一遍代码的。）</p>
<p>【<font color="yellow">个人评价</font>】：从直接学习转变为间接学习，可以说这个idea是可以的，反正我是想不出来。当然了，光有idea还是不够的，还必须work。我觉得GraphSAGE不仅能work还work的很好的主要原因是它的可学习参数加对了地方。因为在之前的聚合器一般都是选用<code>mean</code>,<code>add</code>等一些不带参的方程，但GraphSAGE突破了传统（虽然我也不清楚在此之前是否有人提出过这个问题）。转念一想，什么直接学习间接学习的，在深度学习里，参数的多寡是决定算法上限的最重要标准（毕竟都是拟合）。如果后来人能发现其他增加参数量的地方也一样可以发一篇好文章。</p>
<h2 id="Adaptive-Sampling-Towards-Fast-Graph-Representation-Learning"><a href="#Adaptive-Sampling-Towards-Fast-Graph-Representation-Learning" class="headerlink" title="Adaptive Sampling Towards Fast Graph Representation Learning"></a>Adaptive Sampling Towards Fast Graph Representation Learning</h2><p>【论文题目】：面向图表示学习的自适应采样方法</p>
<p>【需要解决的问题】：</p>
<pre><code>1. 大规模图数据在训练时的采样问题
2. 我个人另外一个理解是：能优化期望的估计值。
</code></pre><p>【这个问题为什么重要】：因为大规模的图数据会导致内存爆炸或者运算量过大的问题<br>【解决之后有什么好处】：<br>【解决问题的具体方法及实验】：<br>【结论】：</p>
<!-- pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html
pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html
pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html
pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html
pip install torch-geometric -->
<h1 id="关于C-amp-S的思考"><a href="#关于C-amp-S的思考" class="headerlink" title="关于C&amp;S的思考"></a>关于C&amp;S的思考</h1><p>原文：<br>(i) a base prediction made with node features that ignores the graph structure (e.g., an<br>MLP or linear model);<br>(ii) a correction step, which propagates uncertainties from the training data<br>across the graph to correct the base prediction; and (iii) a smoothing of the predictions over the</p>
<p>信息传播与标签传播之间的关系：<br>残差，也可以说是误差，到底出在哪里呢？<br>我们思考：</p>
<ol>
<li>比如信息从标签为1的节点传递到标签为2的节点时，会发生什么？<ul>
<li>为此，我们可以将图想象成一种水池结构，节点代表了蓄水池，节点代表水渠。而不同的节点可以用不同的颜色标记。当红色节点的蓄水池顺着水渠流到了蓝色节点的蓄水池，那么蓝色节点的蓄水池会逐渐变绿（红+蓝=绿）。这时候原本蓝色的节点既不是红色也不是蓝色，造成了双重的误差。所以，图神经网络会随着迭代训练而愈发劣化，最后会变成一滩浑水。就比如6层的GCN和2层的GCN效果相差无几。</li>
<li>但如果像标签传播那样，直接规定：无论任何颜色的水，流入蓝色蓄水池之后，池子永远保持蓝色不变。这样的假设确实有点hard，但的确有效。标签传播的hard体现在原文中等式（3）的Y，Y是标签，是不会改变的。</li>
<li>另外，如果是半监督的情况下，标签传播也会遇到红蓝问题。</li>
</ul>
<ul>
<li>一种可能的解决方案：除了标签传播方法以外，我们可以加入一点互信息的方法。</li>
<li>另一种：图的信息临界。当多少红水注入蓝色蓄水池时，会让蓝色蓄水池瞬间变绿？这就是一个临界问题。</li>
<li>还有一种：图神经网络的更新方法是求导，但是每一次求导都不可能完全消除误差。简单的一种做法就是加大同颜色边对节点的权重。</li>
</ul>
</li>
</ol>
<ol>
<li>标签互不相同的两个节点之间的边的信息是一种怎样的存在？</li>
</ol>
<h1 id="以后再分类"><a href="#以后再分类" class="headerlink" title="以后再分类"></a>以后再分类</h1><h2 id="Learning-with-Local-and-Global-Consistency"><a href="#Learning-with-Local-and-Global-Consistency" class="headerlink" title="Learning with Local and Global Consistency"></a>Learning with Local and Global Consistency</h2><p>【<font color="yellow">论文题目</font>】:局部和全局一致性的学习</p>
<p>【<font color="yellow">概述</font>】:这个是<strong>标签传播（LPA : label propagation algorithm）</strong> 的开山之作。文章从全局一致和局部一致两个方面分析了标签传播的算法。其优点是速度快，缺点是不准确。</p>
<p>【<font color="yellow">需要解决的问题</font>】:从已标注数据到未标注数据的（半监督）学习问题。</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：减少标注劳动量，从而获得大量带标签数据。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：未标注数据往往是容易获得的，这些未标注数据如果有了标签，就可以用于后续模型的学习，提升模型的效果。</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：<br>The key to semi-supervised learning problems is the prior assumption of consistency, which<br>means: (1) nearby points are likely to have the same label; and (2) points on the same structure (typically referred to as a cluster or a manifold) are likely to have the same label.</p>
<p>给定一组点集$\mathcal{X}={x<em>1,x_2,\cdots,x+_l,x</em>{l+1},\cdots,x_n}$</p>
<p>以及标签集合$\mathcal{l}={1,\cdots,c}$</p>
<p>那么现在的目标就是从已标注数据到未标注数据的推断问题。</p>
<p>The keynote of our method is to let every point iteratively spread its label<br>information to its neighbors until a global stable state is achieved.</p>
<p>$\mathcal{F}$代表了一个具有非负元素的$n\times c$矩阵，矩阵$F=[F<em>1^T,\cdots,F_n^T] \in \mathcal{F}$代表了数据集$\mathcal{x}$的一个分类，其中的每一个元素$x_i$都有一个$y_i = \argmax</em>{j\le c}F_{ij}$的标签。我们可以认为$F$就是一个向量函数，将数据集$\mathcal{X}$映射到标签集上。$F:\mathcal{X} \rightarrow \mathbb{R}^c$</p>
<p>我们再定义一个$n\times c$的矩阵$Y$，$Y\in \mathcal{F}$,若$x<em>i$具有标签$y_i = j$,则$Y</em>{ij}=1$,其余则为0。显然，$Y$与初始标签是一致的。</p>
<p>算法如下：</p>
<ol>
<li>定义关联矩阵$W$,$W<em>{ij}=exp(-\Vert x_i - x_j \Vert^2 /2\sigma^2)$ 若$i\ne j$否则$W</em>{ii}=0$</li>
<li>构建矩阵$S = D^{-\frac{1}{2}} W D^{-\frac{1}{2}}$,其中，$D$是一个对角矩阵，其$(i,i)$（对角）元素是$W$矩阵第$i$行的和。</li>
<li>迭代$F(t+1)=\alpha SF(t)+(1-\alpha)Y$至收敛，$\alpha$是一个介于0，1之间的超参。</li>
<li>记$F^{<em>}$为${F(t)}$的极限，每一个点$x<em>i$都被标记为$y_i = \argmax</em>{j\le c}F^</em>_{ij}$</li>
</ol>
<p>我们首先在数据集$\mathcal{X}$上定义成对关系$W$，其对角线元素为零。那么我们就可以认为图$G=(V,E)$中的顶点集$V$是定义在数据集$\mathcal{X}$上的，边集$E$的权重是定义在关联矩阵$W$上。在第二步骤中，$W$的标准对角化是为了后续迭代的收敛。前两步骤和谱聚类方法完全相同。在第三个步骤中，每一次的迭代都会让顶点接收其邻居的信息，同时也保持了其自身的特征（第二项）。超参$\alpha$则代表了邻居信息与中心节点信息之间的相关程度。除此之外，信息是均匀传播的因为$S$是一个对称矩阵。最后，每一个未被标注的节点的标签会被标注为迭代过程中接收信息最多的那一类的标签。</p>
<p>关于收敛性的证明：</p>
<p>不妨设$F(0)=Y$.</p>
<p>迭代过程的等式可以记为：$F(t+1)=\alpha SF(t)+(1-\alpha)Y$</p>
<p>那么从0到$n$累加可得：</p>
<script type="math/tex; mode=display">F(t)=(\alpha S)^{t-1}Y+(1-\alpha)\sum_{i=0}^{t-1}(\alpha S)^{i}Y \tag{1}</script><p>由于$0&lt;\alpha &lt;1$,且$S$的特征值在$[-1,1]$之间（级数收敛）（另外，$S$与随机矩阵$P=D^{-1}W=D^{-\frac{1}{2}} W D^{-\frac{1}{2}}$相似）</p>
<script type="math/tex; mode=display">\lim_{t\rightarrow \infty} (\alpha S)^{t-1}=0 \quad and \quad \lim_{t\rightarrow \infty} \sum_{i=0}^{t-1}(\alpha S)^{i}=(I-\alpha S)^{-1} \tag{2}</script><p>前者是极限，后者是无穷级数求和。</p>
<p>所以：</p>
<script type="math/tex; mode=display">F^* = \lim_{t\rightarrow \infty} F(t) = (1-\alpha)(I-\alpha S)^{-1}Y</script><p>而对于分类问题，则可以得到：</p>
<script type="math/tex; mode=display">F^* = \lim_{t\rightarrow \infty} F(t) = (I-\alpha S)^{-1}Y \tag{3}</script><p>(（3）没弄明白)</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">预处理语言模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-03-11 11:29:55 / 修改时间：19:15:47" itemprop="dateCreated datePublished" datetime="2021-03-11T11:29:55+08:00">2021-03-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>为了研究NLG和NLU，我们必须要了解主流的预处理方法，以便更好的服务下游任务。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/a.jpg" alt></p>
<h1 id="预处理语言模型：理论-amp-架构-amp-技巧"><a href="#预处理语言模型：理论-amp-架构-amp-技巧" class="headerlink" title="预处理语言模型：理论&amp;架构&amp;技巧"></a>预处理语言模型：理论&amp;架构&amp;技巧</h1><p>2013年，word2vec横空出世。在那个深度学习还未出现，caffe尚未被开发的年代，这个最早的自然语言预处理模型给人工智能带来了一些惊喜，比如【国王-男人=女王-女人】的词类比。当时的人们还不知道这项技术有何用处，受制于当时的硬件条件，在那个岁月静好，网络层很少的年代，word2vec如同种子一般深埋大地。直到14年后，预处理语言模型终于成长为参天大树，并在那个名为BERT的树干之上开花结果，令人眼花缭乱。</p>
<h2 id="独热码"><a href="#独热码" class="headerlink" title="独热码"></a>独热码</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/onehot.png" alt="one hot"><br>缺点：矩阵太大，而且极为稀疏</p>
<h2 id="共现矩阵"><a href="#共现矩阵" class="headerlink" title="共现矩阵"></a>共现矩阵</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/4.png" alt="共现矩阵"><br>缺点：矩阵太大，而且极为稀疏</p>
<h2 id="【2003年】-NNLM-基于前馈神经网络的-N-元神经语言模型"><a href="#【2003年】-NNLM-基于前馈神经网络的-N-元神经语言模型" class="headerlink" title="【2003年】 NNLM:基于前馈神经网络的 N 元神经语言模型"></a>【2003年】 NNLM:基于前馈神经网络的 N 元神经语言模型</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/5.jpg" alt="共现矩阵"></p>
<h2 id="【2013年】-word2vec"><a href="#【2013年】-word2vec" class="headerlink" title="【2013年】 word2vec"></a>【2013年】 word2vec</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/1.jpg" alt="word2vec架构"><br>作为被大家公认的第一个预处理语言模型（）</p>
<h3 id="八卦时间"><a href="#八卦时间" class="headerlink" title="八卦时间"></a>八卦时间</h3><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/11.png" alt="人物关系图"></p>
<p>word2vec的主要贡献并不是其结构，因为skip gram（预测+聚类） 和 CBOW（采样+完形填空）在传统方法中也有被用到（比如n元语法），也不是层次化的softmax（Hierarchical Softmax）和 负采样（negative sampling），毕竟这些写不成公式的方法都属于trick。<br>主要贡献是【词类比】。</p>
<p>首先来解释word2vec中的各项技术：</p>
<ol>
<li>CBOW：continuous bag of words 连续词袋模型。可以理解为：用一个固定大小的窗口对语料库进行采样，并对采样后的样本做一个“完形填空”，用窗口中的周围词去预测中心词。</li>
<li>skip gram：得到了预测到的中心词之后，再反过来用中心词去预测周围的词。从直觉上来说，同一个窗口中的词汇应该在某种程度上更加近似，所以skip gram就有点聚类的味道了。</li>
<li>Hsoftmax: 可以先看一下softmax的函数图像 <img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/0.jpg" alt="softmax函数图像"> 如果直接预测，会分母太大，直接爆炸，因为少说也是大几千的token量。但如果分开来看，每一句都给一个softmax，那就不会产生爆炸，有点像激活函数和BN。</li>
<li>负采样：就是预测标签中的一个子集。因为整个标签集太大了，即使是现在的硬件水平做起来也很慢。</li>
</ol>
<p>那为什么词类比成了word2vec的主要贡献了呢？个人认为是它达到了一个【情理之中，意料之外】的效果。</p>
<p>到目前为止，预训练模型都充满了令人怀念的古早味。那时的NLP三四层是比较正常的，超过十层都属于“巨型结构”。</p>
<h2 id="【2015】Semi-supervised-Sequence-Learning"><a href="#【2015】Semi-supervised-Sequence-Learning" class="headerlink" title="【2015】Semi-supervised Sequence Learning"></a>【2015】Semi-supervised Sequence Learning</h2><p>可以说是预训练语言模型的开山之作，明确了“pretrain”的概念，提出了预训练模型的“fine-tuning”概念。虽然这篇文章放在今天看来，它的思想已经让我们感到理所当然，它的效果也没有那么地让人感觉惊艳，但是放在那个历史时刻里，它就像一盏明灯，为人们照明了一个方向。</p>
<p>论文的主要内容如下：</p>
<ol>
<li>利用自回归（AR）对一个句子进行逐词预测。</li>
<li>利用自编码方法（AE）对句子进行映射再重构。</li>
</ol>
<p>他们明确提出了，这两种算法可以为接下来的监督学习算法提供 “pretraining step”，换句话说这两种无监督训练得到的参数可以作为接下来有监督学习的模型的起始点，他们发现这样做了以后，可以使后续模型更稳定，泛化得更好，并且经过更少的训练，就能在很多分类任务上得到很不错的结果。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/8.png" alt> </p>
<p>众所周知，RNN模型虽然对时序数据建模很强大，但是因为训练时需要 “back-propagation through time”， 所以训练过程是比较困难的。Dai 和 Le 提出的预训练的方法，可以帮助RNN更好的收敛和泛化，而且在特定业务上不需要额外的标注数据，只需要收集成本很低的无标注的文本。而且这些文本与你的特定业务越相关，效果就会越好，他们认为如此一来，这种做法就支持使用大量的无监督数据来帮助监督任务提高效果，在大量无监督数据上预训练以后只要在少量监督数据上fine-tuning就能获得良好的效果，所以他们给这篇论文取名为 “半监督序列学习”。</p>
<h2 id="【2016年】context2vec-Learning-Generic-Context-Embedding-with-Bidirectional-LSTM"><a href="#【2016年】context2vec-Learning-Generic-Context-Embedding-with-Bidirectional-LSTM" class="headerlink" title="【2016年】context2vec: Learning Generic Context Embedding with Bidirectional LSTM"></a>【2016年】context2vec: Learning Generic Context Embedding with Bidirectional LSTM</h2><p>双向LSTM + mask</p>
<p>这篇文章提出的idea是学习文本中包含上下文信息的embeddings。由于词在不同上下文可以有歧义，相同的指代词也经常在不同上下文中指代不同的实体。所以NLP任务中很重要的就是考虑每个词在其上下文中所应该呈现的向量表达方式。从摘要看，这篇文章的主要贡献，是它使用了双向的LSTM可以从一个比较大的文本语料中，有效地学到了包含上下文信息的embeddings，在很多词义消岐，完形填空的任务上都取得了不低于state-of-the-art的效果。同时他们提到，之前的研究有把上下文的独立embedding收集起来，或是进行简单的平均，而没有一个比较好的机制来优化基于上下文的向量表达。所以，他们提出了context2vec ，一个能够通过双向LSTM学习广泛上下文embedding的非监督的模型。</p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/9.png" alt="softmax函数图像"></p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/10.png" alt="softmax函数图像"></p>
<h3 id="从这一年之后，NLP进入了大力出奇迹的时代"><a href="#从这一年之后，NLP进入了大力出奇迹的时代" class="headerlink" title="从这一年之后，NLP进入了大力出奇迹的时代"></a>从这一年之后，NLP进入了大力出奇迹的时代</h3><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/3.jpg" alt></p>
<h2 id="【2017年】ELMO"><a href="#【2017年】ELMO" class="headerlink" title="【2017年】ELMO"></a>【2017年】ELMO</h2><p>更大的双向LSTM。</p>
<p>具体而言，ELMO的底层输入推荐使用已经学好的静态词向量比如Glove等，向上接两层的双向LSTM作为特征提取器，最终以语言模型作为训练任务进行学习。学好之后，每一个词会得到三个向量（底层，第一层拼接LSTM，第二层拼接LSTM），ELMO告诉你只要下游任务用到WordEmbedding的时候，就用产生的三个向量进行加权平均，其中的权重需要在新任务中进行学习。而这种方式也称为Feature-based Pre Training。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/12.png" alt="ELMO指标"><br>相应的评价指标也都做到了在当时的SOTA，并比其他的高出5-25个百分点。这么好的效果和清晰地思路也使得该论文获得了NAACL2018最佳论文。不过，现在看来ELMO也有缺点，具体而言：</p>
<ol>
<li>LSTM抽取特征的能力远弱于Transformer </li>
<li>拼接方式双向融合特征融合能力偏弱。</li>
</ol>
<h2 id="【2018】GPT-amp-BERT"><a href="#【2018】GPT-amp-BERT" class="headerlink" title="【2018】GPT &amp; BERT"></a>【2018】GPT &amp; BERT</h2><p>GPT最主要的贡献就是证明了tranformer结构比RNN更好。</p>
<p>除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式更为契合，我们一般将这种方式成为基于“Fine-tuning”的二阶段训练。<br>GPT：ImprovingLanguage Understanding by Generative Pre-Training 其实就是Transformer的decoder，一种采用了masked self-attention的训练方式。GPT总结而言有以下几点：</p>
<ol>
<li>semi-supervised learning：无监督pre-train+下游有监督fine-tune</li>
<li>multi-task learning：损失函数为预训练阶段languagemodel目标函数+ λ * 有监督softmax损失</li>
<li>12个decoder：masked-attention（12个multi-head+768维）+ point-wise FFN（3072维）+ Adam + warm-up + GELU</li>
</ol>
<p>BERT刷榜</p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/13.png" alt="BERT"></p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/14.png" alt="BERT刷榜"></p>
<h3 id="RoBERTa，屠榜神器"><a href="#RoBERTa，屠榜神器" class="headerlink" title="RoBERTa，屠榜神器"></a>RoBERTa，屠榜神器</h3><ol>
<li>更大的训练集:16GB-&gt; 160GB</li>
<li>更长的训练时间，6W美金训练一次。</li>
<li>静态Mask -&gt; 动态Mask</li>
<li>去除了NSP</li>
</ol>
<h3 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h3><ol>
<li>词嵌入向量参数的因式分解</li>
<li>跨层参数共享 （feed-forward network、attention、all）</li>
<li>NSP 预训练任务-&gt; Sentence-order prediction (SOP)</li>
<li>去掉dropout、LAMB优化器、更大的batch-size</li>
<li>N-gram mask</li>
</ol>
<h3 id="ERINE，更加疯狂的mask训练"><a href="#ERINE，更加疯狂的mask训练" class="headerlink" title="ERINE，更加疯狂的mask训练"></a>ERINE，更加疯狂的mask训练</h3><ol>
<li>Basic-Level Masking：和BERT的方式一致，简单的随机mask单独的单词</li>
<li>Phrase-Level Masking：对命名实体进行全词Mask，包括人名、地名、机构名等</li>
<li>Phrase-Level Masking：对短语进行全词的Mask，如 a series of等</li>
<li>引入多源数据语料：百科类，新闻资讯类、论坛对话类数据并引入DLM（Dialogue Language Model）来训练模型。</li>
</ol>
<h2 id="XLNET"><a href="#XLNET" class="headerlink" title="XLNET"></a>XLNET</h2><p>本文最后介绍的就是xlnet了。<br>因为BERT的训练中有mask占位符存在。如果一个原本有4000词的文本，在训练的时候就会变成4001词（至少）。这就会造成三个问题：</p>
<ol>
<li>训练与使用不一致，因为在使用阶段是没有mask占位符的。</li>
<li>多出来的mask占位符会影响整体性能。</li>
<li>mask的训练假设是，被mask掉的词是统计独立，但显然不是。</li>
<li>因为BERT的mask占位符在训练阶段有15%，这也就是说，BERT的训练效率只有15%。</li>
</ol>
<p>xlnet希望解决上述问题。<br>根据组合数学，一个由n个词汇组成的句子会产生n！个语言模型。xlnet利用了一个巧妙的方式在训练阶段学习到了这n！个语言模型。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/15.jpg" alt></p>
<!-- ## T-TA
我们知道Transformer的核心运算是Attention(Q,K,V)，在BERT里边Q,K,V都是同一个，也就是Self Attention。而在MLM中，我们既然要建模p(xi|x∖{xi})，那么第i个输出肯定是不能包含第i个token的信息的，为此，第一步要做出的改动是：去掉Q里边的token输入，也就是说第一层的Attention的Q不能包含token信息，只能包含位置向量。这是因为我们是通过Q把K,V的信息聚合起来的，如果Q本身就有token信息，那么就会造成信息泄漏了。然后，我们要防止K,V的信息泄漏，这需要修改Attention Mask，把对角线部分的Attention（也就是自身的）给Mask掉，如图所示。

![](预处理语言模型/16.png)

![](预处理语言模型/17.png) -->

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/29/%E5%8E%BB%E9%9B%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/29/%E5%8E%BB%E9%9B%BE/" class="post-title-link" itemprop="url">去雾</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-01-29 19:42:39 / 修改时间：20:33:17" itemprop="dateCreated datePublished" datetime="2021-01-29T19:42:39+08:00">2021-01-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Single-Image-Haze-Removal-Using-Dark-Channel-Prior"><a href="#Single-Image-Haze-Removal-Using-Dark-Channel-Prior" class="headerlink" title="Single Image Haze Removal Using Dark Channel Prior"></a>Single Image Haze Removal Using Dark Channel Prior</h1><p>In this paper , we proposed a simple but effecient image prior - dark channel<br>prior to remove haze from a single input image.The dark channel is a kind of<br>statistics of haze free out door images.It is based on a key observation -<br>most local patches in haze-free outdoor images contains some pixels which<br>have very low intensities in at least one color channel.Using this prior<br>with the haze imaging model,we can directly estimate the thickness of the haze and recover a high quality haze-free image.Results on a variety of<br>outdoor haze images demonstrate the power of proposed prior.More over , a<br>deepth map can also be obtained as a by-product of haze-removal.</p>
<p>In this paper,we proposed a simple but effecient image prior - dark channel<br>prior to remove haze from a single input image.The dark channel is a kind of<br>statistacs of haze free outdoor images.It is based on a key observation -<br>most local patches in haze-free outdoor images contains some pixels which<br>have very low intensities in at least one color channel.Using this prior<br>with the haze imaging model,we can directly estimate the thickness of the<br>haze and recover a high quality haze-free image.Results on a variety of<br>outdoor haze images demostrate the power of proposed prior.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/10/GNN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/10/GNN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">GNN学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-01-10 13:32:51" itemprop="dateCreated datePublished" datetime="2021-01-10T13:32:51+08:00">2021-01-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-27 19:54:47" itemprop="dateModified" datetime="2021-03-27T19:54:47+08:00">2021-03-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h1><p>本篇笔记实际上是GSN的草稿。</p>
<h1 id="图论的基本概念"><a href="#图论的基本概念" class="headerlink" title="图论的基本概念"></a>图论的基本概念</h1><p>边与两端点称为<strong>关联的(incident)</strong></p>
<p>与同一条边关联的两端点或者与同一个顶点关联的两条边称为<strong>相邻的(adjacent)</strong></p>
<p>两端点相同的边称为<strong>环(loop)</strong></p>
<p>有公共起点并没有公共终点的边称为<strong>平行边(paralled edges)</strong></p>
<p>无环并且无平行边的图称为简单图。</p>
<p>设G为无向图，$x \in V(G)$的顶点度(vertex degree)定义为G中与x关联边的数目，记为$d_G(x)$</p>
<p>1.4 导出子图和支撑子图</p>
<p>1.5 路和连通<br>图$D$(或$G$)中连接顶点$x$和y且长度为k的链W，记为</p>
<p>p32最后一段：割边和割点</p>
<p>4.3 LIMITATIONS<br>Though experimental results showed that GNN is a powerful architecture for modeling struc-<br>tural data, there are still several limitations of the vanilla GNN.<br>• First, it is computationally inefficient to update the hidden states of nodes iteratively to get<br>the fixed point. The model needs T steps of computation to approximate the fixed point.<br>If relaxing the assumption of the fixed point, we can design a multi-layer GNN to get a<br>stable representation of the node and its neighborhood.<br>• Second, vanilla GNN uses the same parameters in the iteration while most popular neural<br>networks use different parameters in different layers, which serves as a hierarchical feature<br>extraction method. Moreover, the update of node hidden states is a sequential process<br>which can benefit from the RNN kernels like GRU and LSTM.<br>• Third, there are also some informative features on the edges which cannot be effectively<br>modeled in the vanilla GNN. For example, the edges in the knowledge graph have the<br>type of relations and the message propagation through different edges should be differ-<br>ent according to their types. Besides, how to learn the hidden states of edges is also an<br>important problem.<br>22 4. VANILLAGRAPH NEURALNETWORKS<br>• Last, if T is pretty large, it is unsuitable to use the fixed points if we focus on the repre-<br>sentation of nodes instead of graphs because the distribution of representation in the fixed<br>point will be much more smooth in value and less informative for distinguishing each<br>node.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/" class="post-title-link" itemprop="url">文本生成</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-12-23 09:40:47" itemprop="dateCreated datePublished" datetime="2020-12-23T09:40:47+08:00">2020-12-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-28 17:49:08" itemprop="dateModified" datetime="2020-12-28T17:49:08+08:00">2020-12-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="写在最前面的逻辑导图"><a href="#写在最前面的逻辑导图" class="headerlink" title="写在最前面的逻辑导图"></a>写在最前面的逻辑导图</h1><p>如果看晕了就可以回过头看一下这个，整理思路。</p>
<h2 id="主线剧情"><a href="#主线剧情" class="headerlink" title="主线剧情"></a>主线剧情</h2><ol>
<li>句子可以用一个联合概率分布来描述</li>
<li>但是联合的不好算，我们用贝叶斯全概率公式改写成条件概率分布</li>
<li>条件概率分布是链式的，可以用递归的方法计算</li>
<li>自然而然的得到seq2seq结构</li>
<li>针对seq2seq的固有问题，提出attention解决方案</li>
<li>绕了一圈又突然发现attention就可以直接求解联合分布</li>
<li>深度学习的哲学1：不好算的东西都交给神经网络去拟合，我们用attention算法来拟合联合概率分布。</li>
<li>深度学习的哲学2：大力出奇迹，如果一层attention不能拟合联合分布，那就叠100层。</li>
<li>深度学习的哲学3：能用显卡解决的问题，就不要用算法。几十层attention堆叠的BERT就这么被搞出来了。而且BERT的原理十分简单（相对于传统的机器学习），但是效果又出奇的好，是NLP的里程碑。</li>
<li>既然有了求解联合分布的BERT，那么文本生成任务实际上就可以看成是全概率公式的逆向运用，用条件概率（也就是不完整的输入）去求联合分布（完整的文本）</li>
<li>整 上 花 活：当我们不再满足于单个句子的联合分布的计算的时候，比如对话或者长文本生成，这时候，针对更长的文本的联合分布的计算，我们就需要用mask方法了。虽然这个方法的名字叫做【掩码】，但实际上它的作用相当于胶水。</li>
</ol>
<h1 id="什么是文本生成？"><a href="#什么是文本生成？" class="headerlink" title="什么是文本生成？"></a>什么是文本生成？</h1><p>文本生成在计算机领域指的是用算法生成可读的文本(让代码说人话)，所谓的人话就是自然语言，这东西极其复杂，因此与自然语言相关的任务大都是非常棘手的。</p>
<h2 id="文本生成的局限性"><a href="#文本生成的局限性" class="headerlink" title="文本生成的局限性"></a>文本生成的局限性</h2><ol>
<li>模型的自然语言理解和语义分析。</li>
<li>长程依赖和全局一致</li>
<li>融入知识图谱</li>
</ol>
<p>以上这三条摘抄自一片综述性文章，都是NLP中的困难问题，当然也是研究的重点方向。</p>
<h1 id="经典的Seq2Seq"><a href="#经典的Seq2Seq" class="headerlink" title="经典的Seq2Seq"></a>经典的Seq2Seq</h1><div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/3.png" width="XXX" height="XXX">
* </div>

<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>Seq2Seq模型是一种基于RNN的序列学习模型，其主要的工作原理是求解条件概率。(不要太在意下标，问题不大)</p>
<script type="math/tex; mode=display">p(x_0) = p(x_0|x_1,x_2,x_3,...,x_n)</script><p>例如给定一个句子：</p>
<blockquote>
<p>今天晚上去吃麻辣【】</p>
</blockquote>
<p>我们已经给定了“今天晚上去吃麻辣”这几个字符，也就是对应的$x_1$至$x_n$。那么现在，我们的任务就是求解【】内的内容。所以，今天晚上究竟是吃麻辣小龙虾还是麻辣香锅，这个问题就是神经网络所需要解决的重点了。</p>
<p>对应实际的问题，我们可以将公式写成如下形式：</p>
<script type="math/tex; mode=display">sentence = \{x_0,x_1,x_2,x_3,...,x_n\}</script><p>其中，sentence是由$n$个单词组成的有序集合。只有当这些单词有序的时候，整个句子才会产生意义。虽然我们不知道自然语言的结构形式，但至少我们可以从概率的角度先进行考虑：</p>
<script type="math/tex; mode=display">P(sentence) = P(x_0,x_1,x_2,x_3,...,x_n)</script><p>但问题是上式右边是一个联合分布，求解困难。当然，如果加入一点马尔科夫的思想，这个联合分布讲道理还是可以计算出来的，也确实有这种算法，那就是n元语法：</p>
<blockquote>
<p>n元语法((n-gram grammar)建立在马尔可夫模型上的一种概率语法.它通过对自然语言的符号串中n个符号同时出现概率的统计数据来推断句子的结构关系.当n=2时，称为二元语法，当n = 3时，称为三元语法.</p>
</blockquote>
<p>我们可以退而求其次，继续将公式改写成如下形式：</p>
<script type="math/tex; mode=display">P(sentence) = p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>这样，我们就得到了一个贝叶斯视角下的句子建模了。上式是计算可行的，其中$n$并非趋近于无穷，因为一个句子总是有结束的时候。甚至当我们不开心的时候可以指定$n=1$，这就对应了一个字的回复。</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/5.png" width="XXX" height="XXX">
</div>

<p>那么应该如何计算呢？<br>我们可以递归的计算：先求第一个字的概率，然后将答案作为输入，输入神经网络中再计算第二个字的概率。。。这就是递归神经网络（RNN）了。</p>
<p>随着硬件计算能力的进步，现在的RNN一般都是结合n元语法的思想，每次的输入都是n个字符，然后去求第n+1个字符。</p>
<p>这时候，我们就可以自然而然的得出seq2seq结构了。</p>
<h2 id="seq2seq的结构"><a href="#seq2seq的结构" class="headerlink" title="seq2seq的结构"></a>seq2seq的结构</h2><div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/2.png" width="XXX" height="XXX">
</div>
encoder-decoder框架如上图所示，中间的那个“语义编码c”实际上相当于链接encoder-decoder的桥梁。我们很自然而然的觉得问题不大，但实际上这其中却蕴藏着一定的不合理：在训练阶段，模型有着明确的目标。但在测试阶段，模型就没了目标。就好比读书的时候老师会告诉你要学什么，但是工作之后就基本再也没有人管你了，唯一关心你（工作进度）的人就是产品经理了。

这个现象（笔者也是最近才听说）被称为Teacher-Foring，确实很形象。当模型没有了训练阶段的压力之后就会放飞自我。例如tensorflow官网的一个教程，用LSTM生成文本，以下是第4000轮的结果。尽管有些短文本已经能读的足够通顺，但是整体上还是拉胯。
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/安娜.png" width="XXX" height="XXX">
</div>

<p>究其原因，最主要问题是RNN的长程依赖问题，形象的说就是“记不住”和“记得太死”。当然后来的LSTM和GRU都在一定程度上缓解了这个问题，但还是治标不治本。这里就需要提到一个概念：Exposure Bias</p>
<blockquote>
<p>Exposure Bias 是在RNN（递归神经网络）中的一种偏差,即 RNN 在 训练(training) 时接受的标签是真实的值(ground truth input)，但测试 (testing) 时却接受自己前一个单元的输出(output)作为本单元的输入(input)，这两个setting不一致会导致误差累积error accumulate,误差累积是因为，你在测试的时候，如果前面单元的输出已经是错的，那么你把这个错的输出作为下一单元的输入，那么理所当然就是“一错再错”，造成错误的累积。</p>
</blockquote>
<p>Exposure Bias给我的感觉实际上是这样的：</p>
<p>工程师：马冬【梅】</p>
<p>RNN:马什么梅？</p>
<p>工程师：马【冬】梅</p>
<p>RNN：什么冬梅？</p>
<p>工程师：最后一遍，马冬梅，记住了吗？！</p>
<p>RNN：记住了记住了。</p>
<p>工程师：马什么梅？</p>
<p>RNN：马化腾</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/马冬梅.png" width="XXX" height="XXX">
</div>



<p>那有的小朋友就会问了，为啥呢？</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/1.jpg" width="XXX" height="XXX">
</div>



<p>为了回答小朋友的这个问题，我们可以设想，如果现在机器预测出了一个【小】字，那你以为最终结果一定会是【小龙虾】吗？当然不一定，一旦变成【小汉堡】，整个模型的level就拉胯了。</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/老八.png" width="50" height="XXX">
</div>

<p>其根本原因是递归问题都存在长程依赖和局部最优问题。</p>
<ol>
<li>长程依赖：RNN是递归求解的序列模型，因此当序列（本文我们就单指文本序列）足够长的时候，最开始的几个字就可能会被遗忘，因此不构成训练的依赖。就比如你还记得这篇文章的最开始是那几个字吗？所以，再回头看安娜·卡列尼娜的那个文本生成模型，长句基本都是很拉胯。</li>
<li>局部最优问题：我们以【麻辣小龙虾】为例，对于【麻辣小】这三个字来说，【龙虾】是他的最优解，因为【麻辣小】+【龙虾】构成了我们爱吃的麻辣小龙虾。但是对于【小】这个字来说，有可能【汉堡】是他的最优解，毕竟曾经的小汉堡几乎全网尽人皆知。。。但是【麻辣小汉堡】就有点意义不明。这个例子就是说，如果模型每一次都选择它所认为的“最优解”，最终有可能每一步的选择都是“正确”的，但得到的结果却是错误的。</li>
</ol>
<p>虽然我们可以用Beam-Search来解决搜索问题，但beam-search只能说是缓解了局部最优的问题（因为自然语言并不是一个最优化的问题，但目前来看我们很难找到其他的解决方案，所以我们就需要先解决局部最优化的问题）。顺着这个思路，我们也可以加长RNN的长度和深度，毕竟大力出奇迹。</p>
<blockquote>
<p>其实，写到这里的时候我就突然想起来我国科学家当年试东风1号导弹，（忘记了什么原因）导致航程不够，无法在靶点爆炸。当时的大部分方案都是加入更多的燃料，但这又会增加弹体重量，那不就飞不动了吗。唯独一个年轻人的方案是抽出燃料，理论上也确实可行（大佬就是大佬）。最后还真的成功了。</p>
</blockquote>
<p>所以，考虑到beam-search和大力出奇迹的方案都是增加计算量，我们可以反向思考，比如用mask的方法盖住几个词。这个方法苏神已经帮我们做了，可以去看他的<a href="https://kexue.fm/archives/7259" target="_blank" rel="noopener">博客</a>。</p>
<h2 id="我们到底在干什么？"><a href="#我们到底在干什么？" class="headerlink" title="我们到底在干什么？"></a>我们到底在干什么？</h2><p>说了这么多，我们到底在解决什么问题？</p>
<p>继续观察这个公式：</p>
<script type="math/tex; mode=display">P(sentence) = p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>只看右边：</p>
<script type="math/tex; mode=display">p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>每一个$p(\cdot)$都代表着一个字，如果将这些字看做节点，实际上我们就是在寻找一个“最优”的路径把他们都链接起来。当然了，如果你回过头去看这张图：</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/2.png" width="XXX" height="XXX">
</div>

<p>就会发现，“桥梁”只有一条。</p>
<p>众所周知，一条路只能链接两个城市。如果写成公式，那就是：</p>
<script type="math/tex; mode=display">p(x_0)p(z)p(x_1|x_0)p(z)p(x_2|x_0,x_1)\cdots p(z)p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>$p(z)$就是神经网络所要拟合的参数之一(对应于上图的“语义编码c”)，它告诉了句子应该以什么样的概率转移为另一个概率。</p>
<p>单一的$p(z)$显然是不充分的，【马】这个字显然是不能以相同的概率转移为马冬梅或者马化腾或者马云。</p>
<p>所以，为了造更多的“桥”：</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/4.png" width="XXX" height="XXX">
</div>

<p><a href="https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216" target="_blank" rel="noopener">参考博客</a></p>
<p>注意力机制就应运而生了。</p>
<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><script type="math/tex; mode=display">self\, attention = Softmax(\frac{QK^T}{\sqrt{d}})V</script><p>其中的QKV同一个东西的三种投影。所以，如果不看softmax一项，整个公式实际上是在求非归一化的联合概率分布。</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/att.png" width="XXX" height="XXX">
</div>

<p>假设上图就是self attention的计算结果，每一行每一列的和都不是1，所以没有归一化。但从感觉上来看，这东西长的可真的像联合分布的计算结果啊！我们随便上网找一张联合分布的图片:</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/jdf.png" width="XXX" height="XXX">
</div>

<p>简直神似啊！</p>
<p>仔细一想…好像又绕回了最初的原点。<br>我们来分析一下：</p>
<ol>
<li>早期的NLP探索者们用n元语法模型去求解联合分布，但是受制于当时的硬件条件，所以n元语法模型的探索也都止步在个位数。</li>
<li>后来出现了word2vec，滑动窗口实际上相当于n元语法中的n，但窗口大小依旧没有突破个位数。</li>
<li>神经网络大行其道，RNN递归求解条件概率，直逼联合分布。</li>
</ol>
<p>但是随着seq2seq问题的研究（主要是当年研究自动翻译的那些学者，著名的attention公式就是从Transformer的研究中诞生的），大家突然发现，直接用attention去求联合分布不就OJBK了吗？？？</p>
<p>然后就出现了不讲武德的BERT，虽然算法层面没有太多的进步，但是从深度学习的哲学水平上来看，这波BERT在大气层。（一部分算法工程师的梦想是用数学的方法去解决复杂的问题，但是另外一批算法工程师直接对复杂问题重拳出击）</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/6.png" width="300" height="XXX">
</div>

<!-- 简单的改动，极致的享受 -->
<h2 id="mask方法：建造跨海大桥"><a href="#mask方法：建造跨海大桥" class="headerlink" title="mask方法：建造跨海大桥"></a>mask方法：建造跨海大桥</h2><p>我们知道，自然语言是有其结构信息的，而问题在于我们对自然语言的结构知之甚少。从小学开始我们就要一直学习语法，定语从句，倒装句之类；将语法结构应用于自然语言处理也确实是NLP的最初之路。但这种路子很快就被证明是走不通的，因为我们不知道如何描述结构。反观BERT的成功，再怎么成功，其数学基础也是建立在统计学之上的，至少我们知道联合概率分布是如何计算的。现在的问题是，我们既不知道结构如何描述，也不知道结构如何计算。</p>
<font color="red">所以，mask是一种通过表象去计算自然语言结构的主要方式。mask虽然被称为掩码，但其主要的功能是【补全】，就好像英语考试中的完形填空，在补全句子信息的过程之中也学习到了句子的整体结构，做的多了，自然而然的也就形成了“语感”。这个语感，实际上就是脑子里对自然语言结构的建模。</font>

<p>在之前的叙述中，我们所关注的都是单个句子的联合分布。秉承深度学习的哲学，我们可以用mask的方式对多个句子重拳出击，甚至说如果你开心(有显卡)，你也可以同时对多篇文章一起重拳出击。</p>
<p>具体做法就是把多个句子当成一个句子，每一个句子的句尾加入一个特殊的标记，比如$[cut]$，然后多个句子顺序拼接，直接输入神经网络进行计算。</p>
<!-- # 更多的思考
<font color=red>当然，我们也可以从互信息的角度去思考问题。</font>



为了更加形象的讲解，我们可以想象如下的一个图形：

![](文本生成/h.png) -->
<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><p>gan的主要问题是：</p>
<ol>
<li>训练困难，有可能因为梯度爆炸而导致模式崩塌。（这个训练困难的问题我不确定还是否存在，因为看到的论文是在18年发表的，而18年的时候又提出了f-gan，其中的共轭算法已经基本避免了这个问题）</li>
<li>多样性不足：具体表现为GAN文本生成模型总是会生成一些短小的句子，从而可以获得更高的分数。（但这个问题我感觉也已经基本解决了，那就是依靠mask方法做长文本输入。）</li>
</ol>
<h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>强化学习目前给我的感觉仍旧是牛刀杀鸡。我们来简单的讨论一下强化学习中的两个概念：收益和动作空间。（这两个问题也许是强化学习在自然语言处理方面效果不显著的主要原因）</p>
<p>首先，收益相当于深度学习中的损失函数，因为在数学上，前者是求最大值，后者是求最小值，没有本质上的区别。对我来说，也许不同的概念会帮助人们产生不同的理解，但这种名称上的改变并没有给算法带来本质上的不同：因此我就可以在损失函数前面加上一个符号，然后将这个负损失定义为收益。</p>
<p>其次，动作空间。这个东西就有点像几十年前人们精心设计的特征一样。如果是对于一些简单问题，那当然是非常有效；但对于自然语言这种极其复杂的问题来说，动作空间就显得有点“狭小且破碎”，因为人工定义的动作空间目前还不能完全描述自然语言。反向思考，如果动作空间能够几乎完整的描述自然语言的特性，那我为什么不去手工写正则表达式呢？</p>
<h1 id="文本生成的关键在于MASK"><a href="#文本生成的关键在于MASK" class="headerlink" title="文本生成的关键在于MASK"></a>文本生成的关键在于MASK</h1><h2 id="用mask来取代动作空间"><a href="#用mask来取代动作空间" class="headerlink" title="用mask来取代动作空间"></a>用mask来取代动作空间</h2><h1 id="着手建模"><a href="#着手建模" class="headerlink" title="着手建模"></a>着手建模</h1><p>依旧是基于公式：</p>
<script type="math/tex; mode=display">p(x_0) = p(x_0|x_1,x_2,x_3,...,x_n)</script><p>个人认为，这个是最好的文本建模方法了，公式中既包含了结构信息（贝叶斯全概率公式）又能够体现语义信息（条件概率），是非常好的生成模型。</p>
<p>但是好东西也是不容易获得的，条件概率的计算量可以达到无穷，力大砖飞的BERT系列甚至就可以看做一个非常强悍的条件概率计算模型。如何降低条件概率的计算复杂度问题仍旧是一个非常值得研究的问题。</p>
<p>好了，言归正传。首先给定一个$sentence$作为输入</p>
<script type="math/tex; mode=display">P(sentence) = p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><h1 id="文献时间"><a href="#文献时间" class="headerlink" title="文献时间"></a>文献时间</h1><h2 id="第一篇"><a href="#第一篇" class="headerlink" title="第一篇"></a>第一篇</h2><p><a href>An Auto-Encoder Matching Model for Learning Utterance-Level<br>Semantic Dependency in Dialogue Generation</a></p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/2.png" alt="原文loss"></p>
<p>编码器和解码器都是LSTM，作者希望在隐变量空间中将上文信息与下文信息对齐，如大图中的中间那一层所示。</p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/1.png" alt="原文loss"></p>
<p>对齐的方式是L2范数。其实改成别的散度也是可以的。</p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/3.png" alt="原文loss"></p>
<p>生成效果如图所示。</p>
<h2 id="第二篇"><a href="#第二篇" class="headerlink" title="第二篇"></a>第二篇</h2><p><a href>CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning</a></p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/4.jpg" alt="模型任务"></p>
<p>这篇文章在文本生成的任务中引入了人类的常识。</p>
<font color="red">其作用相当于引入了知识图谱</font>，这就相当于在求联合分布的时候给定了一些标签，标签相当于条件，在某种程度上相当于求解条件分布。<font color="blue">虽然文章的任务是做图片描述，但有一些地方还是值得我们参考的。</font>
给定的输入为若干个单词，目标是生成一句通顺的人话。

作者认为完成这个任务的条件主要有两个：
1. 词与词之间的关系
2. 合理的语法

作者首先构造了一个“概念的集合”，这个集合中的概念都是一些基本的名词和动词：
$$\{c_1,c_2,...\} \in \mathcal{X}$$

以及一个句子集合$\mathcal{Y}$，里面装的都是人话。模型的目的就是去学习$\mathcal{X} \rightarrow \mathcal{Y}$的映射。

那么这个映射应该怎么去学习呢？

***<font color="red">对比学习 </font>***

首先我们构造训练集$\mathcal{T}$，$\mathcal{T}$是$\mathcal{Y}$的一个子集，但是我们需要给$\mathcal{T}$加上一个限制条件:<font color="red">所有的$\mathcal{T}$ 都必须包含$\mathcal{X}$中的若干个元素</font>

<p>举例来说（本节第一张图片所示），训练集可以长成这个样子：<br>x1 : {苹果，袋子，放}<br>t1 ：{一个女孩儿把苹果放在她的袋子里}<br>x2 ：{苹果，树，摘}<br>t2 ：{一个男人从树上摘了一些苹果}<br>x3 : {苹果，篮子，洗}<br>t3 ：{一个男孩从篮子里拿了几个苹果去洗}</p>
<p>好了，现在我们有了三个训练样本。里面包含了实体，以及隐藏在句子中的实体关系。在训练阶段，假设输入的概念集合为:{梨，袋子，放}，【梨】这个词在训练集中从来没有出现过，那么就可以认为<font color="red">【鸭梨】=【苹果】，在{袋子，放}的条件下</font></p>
<!-- 当然了，上述方法有点类似于对比学习，但我个人认为对比学习的本质不是找相同，而是找不同。

反正体现在哪里？个人认为，反正体现在训练集当中。新的实体，比如【鸭梨】，从来没有在训练集中出现过。现在要证明【鸭梨】是一个实体，那么反证法就是【鸭梨】不是一个实体。

但是与【鸭梨】相关的单词是在训练集中出现过的，那这时候如果想证明 -->
<h2 id="第三篇"><a href="#第三篇" class="headerlink" title="第三篇"></a>第三篇</h2><p><a href>Adversarial Ranking for Language Generation</a><br><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/5.png" alt><br>图中,G是生成器生成的文本，H是人话。RANKER是一个打分器，给所有的输入样本打分，本质上是一个discriminator.打分方法本质上还是对比学习的方法，请参考第二篇。</p>
<p>具体的做法是，针对相同的$x \in \mathcal{X}$,挑选出一些包含x的人话H，然后用这些x生成样本G，将${H,G}$输入到打分器中排序，并根据排序来设计loss.<br><!-- 这片文章是利用NLP中的负采样方法来训练GAN。针对GAN的训练困难问题，负采样确实是一种好方法。虽然文中将这种方法称为Rankings
 --></p>
<h2 id="第四篇"><a href="#第四篇" class="headerlink" title="第四篇"></a>第四篇</h2><p><a href>Generating Text through Adversarial Training using Skip-Thought Vectors</a><br>文本是离散的，所以不适合用GAN来训练。虽然我们可以用嵌入的方式得到词向量，但在输出端还是需要把词向量转化成离散的字，这样才能得到一句文本。就比如我们假设”你好”=1.0,但是”1.01”是什么，谁也不知道。</p>
<p>所以本文作者打算用稠密的句向量来做。所谓的稠密就是连续，与离散相对的概念。<br>在介绍这篇文章之前，我们要先看看什么是Skip-Thought。<br><a href="https://zhuanlan.zhihu.com/p/100271181" target="_blank" rel="noopener">一种传统的句表示学习方法——Skip-Thought Vectors</a></p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/6.jpg" alt><br>然后，原文作者将文本转化为稠密的句向量输入到GAN中做训练，输出的稠密向量用Skip-Thought 的解码器解码一下就是句子了。</p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/7.jpg" alt></p>
<h2 id="第五篇"><a href="#第五篇" class="headerlink" title="第五篇"></a>第五篇</h2><p><a href>Learning Neural Templates for Text Generation</a></p>
<p>这篇文章虽然是说用神经网络学习模板，但实际上已经是在践行强化学习了。如果将模板想象成动作空间，那就容易理解一些了。</p>
<h2 id="第六篇"><a href="#第六篇" class="headerlink" title="第六篇"></a>第六篇</h2><p><a href>Long Text Generation via Adversarial Training with Leaked Information</a></p>
<p>文章作者认为，训练GAN的时候，目标文本序列作为控制信号是处于最后一个阶段的，而生成器所生成的隐变量序列中是包含结构信息和语义信息的，而控制信号作为离散的序列只是在最后阶段与隐变量交互信息。什么意思，就是说稠密的隐变量只有在argmax运算之后才能与文本计算距离，而argmax却大量的损耗了隐变量所蕴含的信息。<br>那么作者做了一件什么什么事情呢？那就是把控制序列的最后一个信号，也就是文本的最后一个字作为额外的输入信息，以及判别器的编码，输入生成器中，。<br>一般的GAN，生成器和判别器之间只有在BP阶段才有信息交互。</p>
<p>模型整体结构如下图所示。<br><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/8.jpg" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/29/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/29/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">生成模型学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-11-29 22:18:31 / 修改时间：22:23:20" itemprop="dateCreated datePublished" datetime="2020-11-29T22:18:31+08:00">2020-11-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/14/%E5%9F%BA%E7%A1%80%E6%8B%93%E6%89%91%E5%AD%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/14/%E5%9F%BA%E7%A1%80%E6%8B%93%E6%89%91%E5%AD%A6/" class="post-title-link" itemprop="url">基础拓扑学</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-14 10:11:17" itemprop="dateCreated datePublished" datetime="2020-10-14T10:11:17+08:00">2020-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-14 20:22:40" itemprop="dateModified" datetime="2020-11-14T20:22:40+08:00">2020-11-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hausdorff空间定义：如果对于拓扑空间X中任意两个不同的点$x_1$和$x_2$，分别存在$x_1$和$x_2$的邻域$U_1$和$U_2$使得这两个邻域无交，则称X为一个Hausdorff空间.</p>
<p>定理：Hausdorff空间中的任何有限集都是闭的。</p>
<p>拓扑学核心任务：寻找拓扑不变性质</p>
<p>拓扑的定义：<br>设X是一个非空集合。X的一个子集族$\tau$称为X的一个拓扑，如果它满足：</p>
<ol>
<li>X,$\varnothing$都包含在$\tau$中。</li>
<li>$\tau$中任意多个成员的并集都在$\tau$中。</li>
<li>$\tau$中有限多个成员的交集都在$\tau$中。</li>
</ol>
<p>集合X和拓扑$\tau$一并称为拓扑空间。</p>
<p>诱导拓扑：若X为拓扑空间，Y为X的子集，Y上的子空间拓扑或诱导拓扑是以X的开集与Y的交集作为这个拓扑的开集而定义的。</p>
<p>定理：一个子集为闭集，当且仅当它包含了自己全部的极限点。</p>
<p>定理：A的闭包是包含A的最小闭集，换句话说，是包含A的一切闭集之交。</p>
<p>拓扑基：设集合X上有了一个拓扑，$\beta$为这个拓扑的一组开集，使得每个开集可以写成$\beta$中成员的并集，则$\beta$叫做这个拓扑的一组拓扑基。</p>
<p>同胚：是指一个连续单一满映射，它的逆映射也连续。</p>
<p>定理：设X为拓扑空间，$\mathcal{F}$为X的一组开集，他们的并集是整个X。这样的一组开集叫做X的 <strong>开覆盖</strong>。若$\mathcal{F}’$ 是$\mathcal{F}$的一个子集，并且若$\bigcup \mathcal{F}’=X$，则$\mathcal{F}’$叫做$\mathcal{F}$的一个子覆盖。</p>
<p>定义：拓扑空间X紧致，假如X的任何开覆盖包含有限子覆盖。</p>
<p> 群”是一个“ 有结构的集合”</p>
<p> 紧致空间的无穷子集必有极限点。（如果在紧致空间内取出无穷多个点，那么这些点的分布必然在某处显得很拥挤，这就是极限点）</p>
<p>道路：一个连续映射$\gamma{:[0,1]\rightarrow} X$</p>
<p>定义：空间被称为道路连通的，假如它的任意两点可以用一条道路连结。$\gamma{(0)}$和$\gamma{(1)}$分别叫做道路的起点和终点。注意，若$\gamma^{-1}$定义做：</p>
<script type="math/tex; mode=display">\gamma^{-1}{(t)} = \gamma{(1-t)} ,0 \le t \le 1</script><p>则$\gamma^{-1}$是连结$\gamma{(0)}$和$\gamma{(1)}$的一条道路。</p>
<p>（基本群是利用空间中的道路连通构建出来的）</p>
<p>定理：道路连通空间是连通的。</p>
<p>粘合拓扑：设X为拓扑空间，$\mathcal{P}$为X的一族互不相交的非空子集，使得$\bigcup{\mathcal{P}}=X$ ，这样的一个族$\mathcal{P}$叫做X的一个划分。按照下述方式制造一个新空间Y，叫做粘合空间：Y是$\mathcal{P}$的成员，并且若$\pi : x\rightarrow y$将X的每点送到$\mathcal{P}$所属的成员中，而Y的拓扑是使$\pi$为连续的最大拓扑。于是，Y的子集O为开集，当且仅当$\pi^{-1}{O}$在X为开集。我们可以把Y看做是从空间X出发，把属于$\mathcal{P}$的每一子集粘合成为一点而形成的空间。</p>
<p>定义：G是一个拓扑群，假如它既是一个Hausdorff空间，又是一个群，并且这两个结构在下述意义之下是相容的：群的乘积$m:G\times G \rightarrow G$,与群的求逆运算$i:G\rightarrow G$都是连续映射。</p>
<p>定义：设X是一个拓扑空间，$A\subset X$.如果点$x\in X$的每一个邻域U中都有A中异于x的点，即$U \bigcap (A-{x}) \neq \varnothing$,则称点x为A的一个<strong>凝聚点</strong>或<strong>极限点</strong>。集合A的所有凝聚点构成的集合称为A的导集，记做$d(A)$，</p>
<p>定义：设X是一个拓扑空间，$A\subset X$.集合A与A的导集$d(A)$的并$A\bigcup d(A)$称为集合A的闭包，记做$\overline{A}$</p>
<p>定义：设X是一个拓扑空间，$A\subset X$.如果A是点$x\in X$的一个邻域，即存在X中的一个开集V使得$x\in X \subset A$，则称点x是A的一个内点。集合A的所有内点所构成的集合称为A的内部，记做$\mathring{A}$</p>
<p>每一个球形邻域都是开集，从而任意多个球形邻域的并也是开集；另一方面，假如U是度量空间X中的一个开集，则对于每一个$x\in U$有一个球形邻域$B(x,\epsilon) \subset U$,因此$U = \bigcup_{x\in U}B(x,\epsilon)$.也就是说，一个集合是某度量空间中的一个开集，当且仅当它是这个度量空间中的若干个球形邻域的并。因此我们可以说，度量空间的拓扑是由它的所有的球形邻域的集族求并这一运算“产生”出来的。</p>
<p>定义：设$(X,\mathcal{T})$是一个拓扑空间，$\mathcal{B}$是$\mathcal{T}$的一个子族。如果$\mathcal{T}$中的每一个元素（即拓扑空间X中的每一个开集）是$\mathcal{B}$中某些元素的并，即对于每一个$U\in \mathcal{T}$,存在$\mathcal{B}<em>1 \subset \mathcal{B}$，使得$U=\bigcup</em>{B\in \mathcal{B_1}}B$,则称$\mathcal{B}$是拓扑$\mathcal{T}$的一个基，或称$\mathcal{B}$是拓扑空间X的一个基。</p>
<p>“子空间”实际上是从大拓扑空间中“切割”出来的一部分。这里有一个问题，概言之就是：一个拓扑空间什么时候是另一个拓扑空间的子空间？换言之，一个拓扑空间在什么条件下能够“镶嵌”到另一个拓扑空间中去？</p>
<p>定义：设X和Y是两个拓扑空间，$f:X\rightarrow Y$，映射$f$称为一个<strong>嵌入</strong>，如果它是一个单射，并且是从X到它的像集的$f(X)$的一个同胚。如果存在$f:X\rightarrow Y$，则我们称<strong>拓扑空间X可嵌入拓扑空间Y</strong>。</p>
<p>拓扑空间的某种性质，如果为一个拓扑空间所具有也必然为它在任何一个连续映射下的像所具有，则称这个性质是一个在<strong>连续映射下保持不变的性质</strong>。</p>
<h1 id="点集拓扑-GTM27"><a href="#点集拓扑-GTM27" class="headerlink" title="点集拓扑 GTM27"></a>点集拓扑 GTM27</h1><p>disjoint : $A \bigcap B = 0$</p>
<p>the absolute complement of a set A is written $\sim A$</p>
<p>the relative complement of A is written as $X \sim A = X\bigcap \sim A$</p>
<p>if R is a relation and A is a set,then R[A] ,the set of all R-relatives all points of A.</p>
<p>the members of the topology of $\mathcal{T}$ are called <strong>open</strong> relative to $\mathcal{T} $</p>
<p>normal space(p128) : a space is normal iff disjoint closed sets have disjoint neighbourhoods.And another statement is suggested that <strong>a family of neighbourhoods of a set is a base of neighbourhood system of the set iff every neighbour of the set contains a member of the family</strong></p>
<p>regular space(p129):<strong>iff for each point x and each neibourhood U of x , there is a closed neighbourhood V of x ,such that V $\subset$ U</strong>,that is a closed neighbourhoods of each point is a base for the neighbourhood system of the point.</p>
<p>Suppose that one-point sets are closed in X.Then X is said to be 【regular】if for each pair consisting of a point x and a closed set B disjoint from x, there exist disjoint open sets containing x and B, respectively.<br>The space X is said to be 【normal】if for each pair A,B of disjoint closed sets of X,there exist disjoint open sets containing A and B, respectively.</p>
<p>第二可数公理：一个拓扑空间如果有一个可数基，则称这个空间满足第二可数公理。</p>
<p>第一可数公理：一个拓扑空间如果在它的每一点处都有一个可数邻域基，则称这个可数空间是第一可数的。</p>
<p>p129，结尾的一段很重要</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">李忠宇</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李忠宇</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
