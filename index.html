<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="AIfuns">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="AIfuns">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="李忠宇">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>AIfuns</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AIfuns</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">为中华之富强而读书</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/27/README/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/27/README/" class="post-title-link" itemprop="url">README</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-27 16:59:57" itemprop="dateCreated datePublished" datetime="2021-03-27T16:59:57+08:00">2021-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-28 11:48:36" itemprop="dateModified" datetime="2021-03-28T11:48:36+08:00">2021-03-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="posts"><a href="#posts" class="headerlink" title="_posts"></a>_posts</h1><p>个人读书笔记</p>
<blockquote>
<p>当了那么就的白嫖怪，也是时候为社会做出贡献了。</p>
</blockquote>
<p>并对<a href="https://github.com/miracleyoo/Markdown4Zhihu" target="_blank" rel="noopener">用于知乎的markdown</a>进行了一些修改，更加适合hexo用户。</p>
<blockquote>
<p>zhihu-publisher.py ： 我做了一些简单的修改，让它在文件内也能直接运行，就不需要命令行那么麻烦了。生成出来的文件会在D盘下的Data文件夹内，当然您也可以做出适合自己的修改。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">图神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-22 16:55:49" itemprop="dateCreated datePublished" datetime="2021-03-22T16:55:49+08:00">2021-03-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-04-05 20:40:09" itemprop="dateModified" datetime="2021-04-05T20:40:09+08:00">2021-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://ruin2.github.io/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">github版本在这里，最好用谷歌浏览器</a></p>
<h1 id="【图神经网络】文献阅读笔记"><a href="#【图神经网络】文献阅读笔记" class="headerlink" title="【图神经网络】文献阅读笔记"></a>【图神经网络】文献阅读笔记</h1><p><strong>最近在研究图神经网络相关的内容，所以写下这篇阅读笔记。个人能力有限，所以如果您在阅读的过程中找出了错误请务必指出。另不吝赐教，相互交流学习。本文也会持续更新。<span class="emoji" alias="stuck_out_tongue" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f61b.png?v8">&#x1f61b;</span></strong></p>
<blockquote>
<p><em>个人认为GNN是目前来看，唯一一个能对抗Transformer的框架了。另外，深度学习条件下的GNN的历史虽然差不多和Transformer一样，但对于GNN的研究还差得太多，因此还不够深刻和彻底，这就导致了性能和应用两方面的拉胯。不过也正是GNN的各种研究方面的缺陷才让我们有了“可乘之机”，毕竟相关领域还不是太卷，广阔天地大有可为。正所谓“熟读洋屁三百篇，不会放屁也会吟”，奥利给！兄弟们干了！</em></p>
</blockquote>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>众所周知，自然界的数据绝大部分都是非欧的，这些数据无头无尾，一团乱麻，处理起来非常棘手。</p>
<p>目前的四大主流深度学习算法(算子)：全连接，CNN,RNN,ATT（我们先暂时忽略对抗，自编码以及强化学习）。它们对于图结构的数据的处理能力有限，但问题是深度学习发展到今天，也只有这几种常见的方法。这时候研究者就面临了两种选择：</p>
<ol>
<li>要么让算法去适应非欧的数据（改算法不改数据结构）</li>
<li>要么让非欧的数据去适应算法（改数据结构不改算法）</li>
</ol>
<p>道理是这么个道理，但我的分类方法还是相对粗糙的。学术界的两种主流分类方法是：</p>
<ol>
<li>节点为主的建模方法，边为主的建模方法，节点和边同时建模的方法。</li>
<li>图卷积（CNN）、时序图（RNN）、图注意力（ATT）…</li>
</ol>
<p>无论我们采用何种分类方式，我们总是绕不开两个核心问题：</p>
<ol>
<li>图的拓扑结构应该如何表示？</li>
<li>节点和边的信息应该如何学习？</li>
</ol>
<p>在此，我们将图分成了两部分，即：<span class="emoji" alias="star" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">&#x2b50;</span>结构和信息<span class="emoji" alias="star" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">&#x2b50;</span>。更直白的来说，<font color="red">图=拓扑结构+信息</font>。</p>
<p>而数学中对图的定义则是有序的节点集和边集的集合。我们所采取的这种定义方式只是比较直接的指出了图神经网络研究的主要难点，本质上和数学中的定义是相同的。我们将在后面不断的讨论这两个难点。</p>
<p>另外，单独一个问题的研究都是没有意义的，因为去掉任何一个要素（结构或者信息）都不能构成一个完整的图。所以，我们还需要找到一种连结两个问题的方法。幸运的是，我们目前有两种方法同时处理结构和信息，一个是谱方法（特征值分解），另一个是采样。我个人比较支持采样方法。</p>
<p>好了，既然现在我们知道了图的研究对象是其结构和信息，获取样本的方法是谱分解或者采样，那么这些数据应该如何计算呢？</p>
<p>还记得马克思的那句话吗？<strong>人是其社会关系的总和</strong>。这句话很好的启发了我们，我们可以得到<font color="red">节点是其关系信息的总和</font>，这些关系包括了节点的邻居，邻边和其自身。然后我们将这些关系信息通过某种方式聚合起来，就可以得到该节点的表示了。</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.gif" alt></p>
<p>我们的学习顺序和之前的综述性文章是不太一样的。<br>我们的论文阅读顺序如下所示：</p>
<ol>
<li>图神经网络的基本建模思路（MPNN）</li>
<li>采样方法（GraphSAGE等）</li>
<li>时序图</li>
<li>图卷积</li>
<li>图注意力</li>
</ol>
<h2 id="本文统一的符号"><a href="#本文统一的符号" class="headerlink" title="本文统一的符号"></a>本文统一的符号</h2><blockquote>
<p>先写这么多，会慢慢扩展和补充</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$D=(V,E)$</td>
<td style="text-align:center">图的定义（vertex \&amp; edge）</td>
</tr>
<tr>
<td style="text-align:center">$N,M$</td>
<td style="text-align:center">节点和边的数量</td>
</tr>
<tr>
<td style="text-align:center">$V = {v_1,\cdots,v_n}$</td>
<td style="text-align:center">顶点集</td>
</tr>
<tr>
<td style="text-align:center">$F^V,F^E$</td>
<td style="text-align:center">节点、边的特征（属性）</td>
</tr>
<tr>
<td style="text-align:center">$A$</td>
<td style="text-align:center">邻接矩阵</td>
</tr>
<tr>
<td style="text-align:center">$D(i,i)=\sum<em>{j} A</em>{ij}$</td>
<td style="text-align:center">对角度矩阵</td>
</tr>
<tr>
<td style="text-align:center">$L=D-A$</td>
<td style="text-align:center">拉普拉斯矩阵</td>
</tr>
<tr>
<td style="text-align:center">$Q\Lambda Q^T=L$</td>
<td style="text-align:center">拉普拉斯矩阵的特征分解</td>
</tr>
<tr>
<td style="text-align:center">$P=D^{-1}A$</td>
<td style="text-align:center">转移矩阵</td>
</tr>
<tr>
<td style="text-align:center">$\mathcal{N}_k(v)$</td>
<td style="text-align:center">节点$v$的$k$阶邻居,没写$k$就是$k=1$</td>
</tr>
<tr>
<td style="text-align:center">$H^l$</td>
<td style="text-align:center">第$l$层的隐藏状态</td>
</tr>
<tr>
<td style="text-align:center">$f_l$</td>
<td style="text-align:center">$H^l$的维度</td>
</tr>
<tr>
<td style="text-align:center">$\sigma(\cdot)$</td>
<td style="text-align:center">非线性激活函数</td>
</tr>
<tr>
<td style="text-align:center">$\Theta$</td>
<td style="text-align:center">可学习的参数</td>
</tr>
<tr>
<td style="text-align:center">$s$</td>
<td style="text-align:center">采样大小</td>
</tr>
<tr>
<td style="text-align:center">$T$</td>
<td style="text-align:center">时间步</td>
</tr>
<tr>
<td style="text-align:center">$U$</td>
<td style="text-align:center">状态更新方程（update），特指隐藏层状态更新</td>
</tr>
<tr>
<td style="text-align:center">$M$</td>
<td style="text-align:center">消息（message），实际就是信息</td>
</tr>
<tr>
<td style="text-align:center">$AGG$</td>
<td style="text-align:center">聚合方程（或聚合器）（aggregater），特指节点或者边上的显示状态更新</td>
</tr>
</tbody>
</table>
</div>
<h2 id="文献阅读笔记法"><a href="#文献阅读笔记法" class="headerlink" title="文献阅读笔记法"></a>文献阅读笔记法</h2><blockquote>
<p>按照这个顺序记笔记</p>
</blockquote>
<p>【<font color="yellow">论文题目</font>】:</p>
<p>【<font color="yellow">概述</font>】:</p>
<p>【<font color="yellow">需要解决的问题</font>】:</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<h2 id="相关术语及概念"><a href="#相关术语及概念" class="headerlink" title="相关术语及概念"></a>相关术语及概念</h2><blockquote>
<p>在进入正式的学习之前，我们需要对该领域的专业术语和概念进行一定的了解，方便同行之间的沟通和交流。不敢保证能达到智取威虎山的效果，但至少能保证不至于张嘴就被击毙。</p>
</blockquote>
<h2 id="A-New-Model-for-Learning-in-Graph-Domains"><a href="#A-New-Model-for-Learning-in-Graph-Domains" class="headerlink" title="A New Model for Learning in Graph Domains"></a>A New Model for Learning in Graph Domains</h2><p>【<font color="yellow">论文题目</font>】：图领域的一种新的学习模型</p>
<p>【<font color="yellow">概述</font>】：在一些场景中，信息是自然的以图的形式所表示的。虽然这些算法能够将图信息表示成一组向量，但通常来说，处理过程会丢失掉图的拓扑结构信息，而且其向量表示也是受算法本身约束的。这片文章提出了一种<font color="red">广义的RNN</font>，能够处理许多种图结构的信息。另外，本文提出了一些重要的图神经网络的术语和概念。</p>
<p>【<font color="yellow">需要解决的问题</font>】：图神经网络建模方法，以及节点和边的表示方法。</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：假设数据都可以表示成图结构的。</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：因为自然界的数据绝大多数都是以图形式出现的，所以我们必须解决图建模的基本算法问题。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：可以用高维形式进行图表示，提升相关任务的准确率。</p>
<p>【<font color="yellow">解决问题的具体方法及实验</font>】：</p>
<ol>
<li><p>论文提出了节点级(node focused)和图级(graph focused)的两种建模方法，区别就是预测的时候，是预测了图的一部分信息还是预测整张图的信息。虽然我个人感觉这种分类方式已经有点过时了，而且以当前的算力来看，似乎又会造成一定程度上的混淆，所以我个人将不再讨论这个分类方式。</p>
</li>
<li><p>广义RNN ： 首先通过随机游走的方式得到一个序列，然后用这个序列上的每一个节点及其邻居节点作为一个子图，再把这个子图中的所有信息通过一定的方式聚合到一起，形成一个新的节点。这样，我们就得到了一个序列化的子图，而且这个序列化的子图可以直接使用RNN计算，因此就成了广义RNN。在这篇论文中，广义RNN实际上就是GNN的原型。</p>
</li>
<li><p>消息的传递、聚合和更新：</p>
<script type="math/tex; mode=display">v_{t+1}=f_{\theta}(v_t,\mathcal{N}(v_t),l_{\mathcal{N}(v_t)} )</script><script type="math/tex; mode=display">o(v_{t+1})=g_{\theta}(v_t,v_{t+1})</script><p> 上式对应于原文中的等式$(7)$。</p>
<p> 第一行实际上就是消息的传递，$l$是节点的标签信息，也就是说，GNN可以是有监督的，也可以是自监督的。</p>
<p> 第二行中相当于节点信息的更新。</p>
<p> 那么聚合体现在哪里呢？实际上原文是没有“聚合”这个概念的，传递和聚合的过程被一同合并在了$f_{\theta}$里面。</p>
<p> 更为具体的概念我们将会在之后的论文中讨论。</p>
</li>
</ol>
<p>用前向传播来计算数值解<br>用聚合信息的方法来计算表示解</p>
<p>【<font color="yellow">结论</font>】：<font color="red">图神经网络=采样+消息传递+消息聚合+状态更新</font>。这四要素构成GNN的基石，缺一不可。</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：这篇论文提出了最原始的采样、传递、聚合、更新的概念（虽然确切的来说，是没有聚合的概念）。让我们知道了GNN中的节点的状态参数应该如何更新。</p>
<p>RNN可以看成是一种GNN的特例，反过来说就是：GNN是广义的RNN。再比如LSTM，其中的门控机制可以看做是聚合方程，而其马尔科夫假设也可以被看做消息传递。但GNN和RNN最大的本质区别就是节点的度。在RNN中，节点的度都是2（除了头尾），而GNN的节点度就变化万千了，因此形成了特殊的拓扑结构，也正因如此，GNN才需要各种采样技术，而RNN却不需要（RNN只需要顺序遍历即可）。虽然原文没有明确的说明，但我们读完论文就能感受到。</p>
<h2 id="The-Graph-Neural-Network-Model"><a href="#The-Graph-Neural-Network-Model" class="headerlink" title="The Graph Neural Network Model"></a>The Graph Neural Network Model</h2><p>【<font color="yellow">论文题目</font>】:</p>
<p>【<font color="yellow">概述</font>】:</p>
<p>【<font color="yellow">需要解决的问题</font>】:</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<h2 id="Introduction-to-GraphNeural-Networks"><a href="#Introduction-to-GraphNeural-Networks" class="headerlink" title="Introduction to GraphNeural Networks"></a>Introduction to GraphNeural Networks</h2><p>在我的印象中这是比较早的几篇综述文章之一。我们仅截取其中一节：第四节，Vanilla Graph Neural Networks。这一节讲道理是我最喜欢的，介绍的</p>
<p>【<font color="yellow">论文题目</font>】:图神经网络介绍之图神经网络初代机</p>
<p>【<font color="yellow">概述</font>】:</p>
<p>【<font color="yellow">需要解决的问题</font>】:</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<h2 id="Neural-Message-Passing-for-Quantum-Chemistry"><a href="#Neural-Message-Passing-for-Quantum-Chemistry" class="headerlink" title="Neural Message Passing for Quantum Chemistry"></a>Neural Message Passing for Quantum Chemistry</h2><p>【<font color="yellow">论文题目</font>】：<font color="cyan">量子化学的消息传递神经网络</font></p>
<p>【<font color="yellow">概述</font>】：化学分子可以看做是图结构，而图结构又有许多相似之处，所以可以用一种监督学习的方法来预测分子的属性，并为新药发现和材料科学做贡献。传统的监督学习方法已经取得了很好的效果，但是作者希望能够用深度学习的方法把成果推广开来。因此他们发明了MPNN框架，并希望在此框架下，各种算法变体能取得进一步的准确度，达到数据集的极限。</p>
<p>【<font color="yellow">需要解决的问题</font>】：设立一个图算法的框架，并在这个框架下寻找更多更准确更快速的，用于预测分子属性的变体模型。</p>
<p>【<font color="yellow">这个问题为什么重要</font>】： 快速解决分子和材料的属性预测。</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<ol>
<li>在同一种图框架下</li>
<li>算法不受同构（例如化学中的手性）的影响</li>
<li>算法的计算效率足够高</li>
</ol>
<p>【<font color="yellow">解决之后有什么好处</font>】：</p>
<ol>
<li>预测时间会下降。用DFT(离散傅里叶变换)做预测需要$10^3$量级的时间，而使用MPNN则可以降低到$10^{-2}$量级。</li>
<li>找到更多的变体模型，这些模型可以完全准确的预测到分子属性，或者是逼近预测的极限。</li>
<li>可以促进新药发现和材料科学。（天坑专业自救指南）</li>
</ol>
<p>【<font color="yellow">解决问题的具体方法及实验</font>】：</p>
<p><strong>在算法方面</strong>：</p>
<ul>
<li>节点之间的消息传递：</li>
</ul>
<pre><code>$m_v^{t+1} = \sum_{w \in N(v)} M_t(h_v^t,h_w^t,F^{e})$

上式的含义是，节点$v$的消息传递是汇聚了它自身的信息$h_v^t$以及它邻居节点的消息$h_w^t$和邻边的消息$F^{e}$。但凡能用上的消息基本都用上了。
</code></pre><ul>
<li><p>消息更新的方式：</p>
<p>  $h_v^{t+1}=U(h_v^t,m_v^{t+1})$</p>
<p>  这里只需要说一下$U$实际上是一个状态更新函数。通常用<code>add</code>,<code>mean</code>,<code>max</code>.但<code>max</code>是不可微的，所以使用的时候要注意。</p>
</li>
<li><p>图表征方法（readout）:</p>
<p>  $y=R({h_v^T|v \in G})$</p>
<p>  $y$是一个向量，它表征了整个图。</p>
</li>
</ul>
<p><strong>在实验数据方面</strong>，本论文在其开发的图建模环境MPNN下，通过错误率和运行时间两方面来证明MPNN框架的可行性和实用性。但是具体的数据我并没有认真研究，因为MPNN的亮点是其图建模的思想（毕竟我太菜了，也找不到比MPNN更早的相关研究了。主要是懒）。</p>
<p>【<font color="yellow">结论</font>】：MPNN确实从准确率和运行时间两方面达到了理想的结果。并且实现了一些图建模的最基本要素，如：消息传递，节点和边的参数更新，图表征（readout，我翻译成图表征，因为readout指的是把一张图表示为一个向量。）</p>
<p>【<font color="yellow">源码解析</font>】：MPNN确实是有一套自己的代码框架，但是我水平有限，没有看懂<span class="emoji" alias="joy" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8">&#x1f602;</span>。不过MPNN的基本思想被研究者们广泛认同，被PYG（pytorch_geometric）继承，然后PYG有被OGB（open graph benchmark）所继承，所以我们来看一下OGB的源码即可<span class="emoji" alias="smile" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span>。（torch\geometric\nn\conv\message_passing.py）</p>
<p><a href="https://zhuanlan.zhihu.com/p/165996331" target="_blank" rel="noopener">关于OGB，可以参考这篇文章</a></p>
<p><a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html" target="_blank" rel="noopener">MPNN的英文文档</a></p>
<p>首先，在OGB中，我们定义MPNN的算法框架：</p>
<script type="math/tex; mode=display">v^{t+1} = \gamma_t(v^t,\square_{u\in \mathcal{N(v)}} \phi(v^t,u^t,F^e))</script><p>其中，$\square$是一个可微的且组合不变的方程，比如<code>sum</code>,<code>mean</code>,<code>max</code>。而$\gamma$和$\phi$则是任意可微方程，如MLP（全连接）。在本文中，$\gamma$就相当于$U$或者$AGG$，$U$和$AGG$的区别仅限于运算的对象，是负责聚合和更新信息的。而$\phi$则相当于$M$，负责收集信息。</p>
<p><code>MessagePassing</code>类内有三个主要方法：</p>
<ul>
<li><code>MessagePassing.propagate(edge_index, size=None, **kwargs)</code>:该方法的输入参数是<code>edge_index</code>，知道了边的编号之后，自然就知道了输入节点$v$和输出节点$u$了。该方法会在内部调用<code>MessagePassing.aggregate</code>和<code>MessagePassing.update</code>。实际上，每次采样的结果就相当于一个子图，而且这个子图还是一个二部图。</li>
<li><code>MessagePassing.message</code>：相当于$\phi_{\mathbf{\Theta}}$。这个方法的输入就是<code>MessagePassing.propagate</code>的输入，</li>
<li><code>MessagePassing.aggregate</code>:相当于$\square_{u\in \mathcal{N(v)}}$，</li>
<li><code>MessagePassing.update</code>:相当于$\gamma_{\mathbf{\Theta}}$这一部分，对每一个节点$v$进行聚合。其输入是<code>MessagePassing.aggregate</code>的输出。</li>
</ul>
<p>【<font color="yellow">个人评价</font>】：<br>MPNN的思想非常简单且可行：节点就是其关系的总和。简单好用易于理解，所以成为了OGB的基础层。</p>
<h1 id="图上的采样方法"><a href="#图上的采样方法" class="headerlink" title="图上的采样方法"></a>图上的采样方法</h1><h2 id="DeepWalk-Online-Learning-of-Social-Representations"><a href="#DeepWalk-Online-Learning-of-Social-Representations" class="headerlink" title="DeepWalk: Online Learning of Social Representations"></a>DeepWalk: Online Learning of Social Representations</h2><p>【<font color="yellow">论文题目</font>】：<font color="cyan">深度随机游走：社团表示的在线学习</font></p>
<p>【<font color="yellow">个人评价</font>】： 真·随机游走。第一个使用深度优先采样的图深度学习方法。</p>
<h2 id="LINE-Large-scale-Information-Network-Embedding"><a href="#LINE-Large-scale-Information-Network-Embedding" class="headerlink" title="LINE: Large-scale Information Network Embedding"></a>LINE: Large-scale Information Network Embedding</h2><p>【<font color="yellow">论文题目</font>】：【LINE】大规模信息网络嵌入</p>
<p>【<font color="yellow">概述</font>】：LINE使用了广度优先采样+深度优先采样</p>
<p>【<font color="yellow">需要解决的问题</font>】：大规模网络节点的向量空间嵌入问题</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.jpg" alt></p>
<p>从直觉的角度来说，两个有着相同朋友圈的人，其兴趣也应该接近，因此这两个人很有可能成为好朋友。从上图中的5，6节点可以看出，这两个节点有着相同的“朋友圈”，所以他们之间的向量应该较为接近。</p>
<p>另外，6，7的嵌入向量也应该接近，因为这两个节点是直接相连（而且关系也比较强，因为边比较粗）</p>
<p>根据以上两个直觉，LINE定义了两种相似度。</p>
<ol>
<li>一阶相似：类似于6，7。如果两个节点之间没有边<strong>直接</strong>相连，这两个节点的一阶相似就是0.</li>
<li>二阶相似：类似于5，6。如果两个节点之间没有相邻的其它节点，那么这两个节点的二阶相似度就是0.</li>
</ol>
<p>【<font color="yellow">这个问题为什么重要</font>】：图数据是离散的。如果能把节点信息向量化，那么我们可以获得<strong>更丰富的表示信息</strong>。这些表示信息对后续任务的帮助巨大。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：这种采样方法相当于NLP的pre-train。对下游任务是有很大帮助的。</p>
<p>【<font color="yellow">解决问题的具体方法</font>】：</p>
<p>LINE的目标函数是一个由一阶相似和二阶相似共同拼凑的。</p>
<p>首先是一阶近似的目标函数：</p>
<ol>
<li><p>定义两个节点之间的概率</p>
<script type="math/tex; mode=display">p_{ij} = \frac{1}{1+exp(-v_i,v_j)} \tag{1}</script></li>
<li><p>定义两个节点之间的经验概率。这里需要说明的是，$z<em>{ij}$是一个非0即1的数。因为边权的数值是不确定的，有时候很大有时候很小，这会导致梯度的大幅波动，因为方差实在是太大了。所以作者很巧妙的使用了01数值来局部归一化。所以，$Z$就代表了边权的数值（只能是整数），$z</em>{ij}$要看两个节点间是否有边相连，有就是1，没有就是0.</p>
<script type="math/tex; mode=display">\hat{p}_{ij} = \frac{z_{ij}}{Z} \tag{2}</script><script type="math/tex; mode=display">Z = \sum_{(i,j)\in E}z_{ij}</script></li>
<li><p>$(1)$和$(2)$代表了两个节点之间的相关性，那么我们直接联立两个方程，就可以得到一个误差值，这个误差值就代表了两个节点的一阶相似度（也可以看成是距离）。在原文中，作者使用了KL散度来计算这个距离。</p>
<script type="math/tex; mode=display">d(p,\hat{p}) = KL(p,\hat{p}) = \sum_{(i,j)\in E}z_{ij}\log{p(v_i,v_j)}</script></li>
</ol>
<p>然后是二阶近似的目标函数：</p>
<p>根据之前对二阶近似的定义，如果想计算两个节点之间的二阶近似的目标函数，就是看看第二个节点在第一个节点的朋友圈（目标节点的全部邻居）当中的份量。</p>
<p>这个份量，就可以用<strong>条件概率</strong>来描述。</p>
<ol>
<li>定义条件概率</li>
</ol>
<script type="math/tex; mode=display">p(v_j|v_i) = \frac{exp(v_i \cdot v_j)}{\sum_{\mathcal{v_k \in N(v_i)}}exp(v_k \cdot v_i)} \tag{4}</script><ol>
<li>计算二阶相似度。其中，$\lambda$代表节点的权值，毕竟节点和节点的重要程度也是不一样的。同样的，$d$也是用KL散度代替。然后把$(5)$式化简一下，就能得到$(6)$</li>
</ol>
<script type="math/tex; mode=display">O_2 = \sum_{v_k\in \mathcal{N}(v_i)} \lambda_i d(\hat{p}(\cdot|v_i),p(\cdot|v_i)) \tag{5}</script><script type="math/tex; mode=display">O_2 = \sum_{v_k\in \mathcal{N}(v_i)} z_{ij} \log{p(v_j|v_i)} \tag{6}</script><p>但比较拉胯的是，原文里的训练方法是分别训练，然后把两种近似情况得到的向量拼接在一起。</p>
<p>【<font color="yellow">实验</font>】：</p>
<p>【<font color="yellow">结论</font>】：仅从采样方法来看，LINE = deepwalk + BFS。采样的方法变多了，因此能够得到更好的效果。</p>
<p>【<font color="yellow">源码解析</font>】：</p>
<p>这个算法的问题我个人感觉非常大（先写着，以后慢慢补充）。因为论文为了省时省力（可能是受限于当时的硬件设备），context变量本来是$v_i$的全部邻居节点，但结果却只是$v_j$的嵌入。</p>
<p>【<font color="yellow">个人评价</font>】：</p>
<p>低情商：这个算法有问题</p>
<p>高情商：这个算法仍旧有很大的改进空间。</p>
<p>开个玩笑。实际上LINE可以看做是DEEPWALK的改进，毕竟deepwalk只是用了DFS做采样，而LINE用了BFS。但LINE的问题我个人认为有两处。</p>
<ol>
<li><p>在一阶相似的定义方面：我们观察公式可以发现，$p$是节点向量的内积，而$\hat{p}$是与节点之间的边权相关的量。一阶相似的目标函数是希望内积和边权尽量相同。我们现在用极端条件来验证一下：</p>
<ul>
<li><p>当$v_i=v_j$时，带入公式$(1)$可得：$p = \frac{1}{1+exp(-1)} =0.73$。这个误差大概是0.27</p>
</li>
<li><p>当$v_i \perp v_j$时，带入公式$(1)$可得：$p =\frac{1}{1+exp(0)}= 0.5$,这个误差是0.5</p>
</li>
</ul>
<p>所以，我个人认为用内积来定义相似度是没有问题的，但问题出在相似度的解析方程，至少文中给出的解析方程是有问题的。另外，LINE将相似度与边权划等号，在直觉上是说得通的，但更具体的形式还要继续探索。</p>
</li>
<li>在二阶相似的定义方面：因为在源码中，$v_j$的向量和context(也就是$v_k$，是$v_i$的全部邻域)的向量实际上都是依靠$v_j$做嵌入的，所以当前节点$v_i$的邻域信息，也就是BFS采样，并没有充分利用。如果简化来看，这种方法甚至比deepwalk还简单（LINE在极端条件下实际上就采样了一个点…也就是$v_j$）。</li>
</ol>
<h2 id="Inductive-Representation-Learning-on-Large-Graphs"><a href="#Inductive-Representation-Learning-on-Large-Graphs" class="headerlink" title="Inductive Representation Learning on Large Graphs"></a>Inductive Representation Learning on Large Graphs</h2><p>【<font color="yellow">论文题目</font>】：<font color="cyan"> 大图的归纳表示学习 </font></p>
<p>【<font color="yellow">概述</font>】：之前的节点嵌入方法都是直接计算一整个较小的图，因此泛化能力差，而且对于新的节点的嵌入显得束手无策。本文采用近邻采样的方法对大图中的节点的嵌入向量进行学习。这种学习方法并非传统的直接学习节点的向量表示，而是去训练聚合器来间接学习节点的向量表示。而且学习目标不仅仅是节点的向量表示，还包括了节点的分布。这种方法速度快，效果好，对未知的节点嵌入也有较高的鲁棒性。</p>
<p>【<font color="yellow">需要解决的问题</font>】：大图中的节点表示学习</p>
<p>【<font color="yellow">解决问题的前提条件或者是假设</font>】：大图</p>
<p>【<font color="yellow">这个问题为什么重要</font>】：节点的向量表示是下游任务的基础。不解决这个问题很难搞下去。</p>
<p>【<font color="yellow">解决之后有什么好处</font>】：解决了节点向量的表示，才能为之后的任务服务。比如推荐系统，只有解决了当前用户节点的表示，才能为新用户的行为进行预测。</p>
<p>【<font color="yellow">解决问题的具体方法及实验</font>】：</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3.jpg" alt="GraphSAGE算法示意图"></p>
<p>如图所示：</p>
<p>(1)表示一个节点的1阶邻居和2阶邻居，这些就是我们所要聚合的信息。</p>
<p>(2)表示对当前节点的信息以及它邻居的信息进行聚合。我们可以很清楚的看到，1阶聚合器和2阶聚合器是不同的，这也很好理解，毕竟你亲戚的亲戚不一定是你的亲戚。所以要区别对待。</p>
<p>(3)则是一个目标函数，要求当前节点去预测他的邻居以及其自身的标签。（有点类似于word2vec的CBOW步骤）</p>
<p>算法的伪代码如下所示：</p>
<p><img src="/2021/03/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.jpg" alt></p>
<p>步骤4聚合了全部的邻居信息，而步骤5则是对当前节点的状态更新。步骤7则是一个常见操作，因为不断的聚合会使节点的模长爆炸，所以需要把隐藏层的模长限定在$[0,1]$之间。这个操作就是求一个向量的单位向量。</p>
<p>最后是目标方程。</p>
<script type="math/tex; mode=display">J = -log(\sigma(z_u^Tz_v)) - Q\mathbb{E}_{v_n \sim P_{n}(v) }log(\sigma(z_u^Tz_{v_n}))</script><p>其中，$\sigma$代表了softmax函数，$z_u$是邻居节点的向量，$z_v$是中心节点的向量。$z_u^Tz_v$代表了两者之间的内积，并用内积来代表相似度。而$P$则代表了负采样，$v_n$就是那个被负采样出来的虚假中心节点，它和$z_u$之间的内积自然会小。所以就用这样一真一假的操作让聚合器去学习。至于Q，那就是负采样的样本个数了。</p>
<p>【<font color="yellow">结论</font>】：</p>
<p>【<font color="yellow">源码解析</font>】：该方法的实现是较为简单的，主要就是将聚合器替换为一种带参的方程，之前提到过的四种常见算子大家有兴趣都可以挨个试一遍。（我太懒了，实际上也根本没有实践过。。。而论文的实际效果又不一定那么好，所以有空还是需要去写一遍代码的。）</p>
<p>【<font color="yellow">个人评价</font>】：从直接学习转变为间接学习，可以说这个idea是可以的，反正我是想不出来。当然了，光有idea还是不够的，还必须work。我觉得GraphSAGE不仅能work还work的很好的主要原因是它的可学习参数加对了地方。因为在之前的聚合器一般都是选用<code>mean</code>,<code>add</code>等一些不带参的方程，但GraphSAGE突破了传统（虽然我也不清楚在此之前是否有人提出过这个问题）。转念一想，什么直接学习间接学习的，在深度学习里，参数的多寡是决定算法上限的最重要标准（毕竟都是拟合）。如果后来人能发现其他增加参数量的地方也一样可以发一篇好文章。</p>
<h2 id="Adaptive-Sampling-Towards-Fast-Graph-Representation-Learning"><a href="#Adaptive-Sampling-Towards-Fast-Graph-Representation-Learning" class="headerlink" title="Adaptive Sampling Towards Fast Graph Representation Learning"></a>Adaptive Sampling Towards Fast Graph Representation Learning</h2><p>【论文题目】：面向图表示学习的自适应采样方法</p>
<p>【需要解决的问题】：</p>
<pre><code>1. 大规模图数据在训练时的采样问题
2. 我个人另外一个理解是：能优化期望的估计值。
</code></pre><p>【这个问题为什么重要】：因为大规模的图数据会导致内存爆炸或者运算量过大的问题<br>【解决之后有什么好处】：<br>【解决问题的具体方法及实验】：<br>【结论】：</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">预处理语言模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-03-11 11:29:55 / 修改时间：19:15:47" itemprop="dateCreated datePublished" datetime="2021-03-11T11:29:55+08:00">2021-03-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>为了研究NLG和NLU，我们必须要了解主流的预处理方法，以便更好的服务下游任务。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/a.jpg" alt></p>
<h1 id="预处理语言模型：理论-amp-架构-amp-技巧"><a href="#预处理语言模型：理论-amp-架构-amp-技巧" class="headerlink" title="预处理语言模型：理论&amp;架构&amp;技巧"></a>预处理语言模型：理论&amp;架构&amp;技巧</h1><p>2013年，word2vec横空出世。在那个深度学习还未出现，caffe尚未被开发的年代，这个最早的自然语言预处理模型给人工智能带来了一些惊喜，比如【国王-男人=女王-女人】的词类比。当时的人们还不知道这项技术有何用处，受制于当时的硬件条件，在那个岁月静好，网络层很少的年代，word2vec如同种子一般深埋大地。直到14年后，预处理语言模型终于成长为参天大树，并在那个名为BERT的树干之上开花结果，令人眼花缭乱。</p>
<h2 id="独热码"><a href="#独热码" class="headerlink" title="独热码"></a>独热码</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/onehot.png" alt="one hot"><br>缺点：矩阵太大，而且极为稀疏</p>
<h2 id="共现矩阵"><a href="#共现矩阵" class="headerlink" title="共现矩阵"></a>共现矩阵</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/4.png" alt="共现矩阵"><br>缺点：矩阵太大，而且极为稀疏</p>
<h2 id="【2003年】-NNLM-基于前馈神经网络的-N-元神经语言模型"><a href="#【2003年】-NNLM-基于前馈神经网络的-N-元神经语言模型" class="headerlink" title="【2003年】 NNLM:基于前馈神经网络的 N 元神经语言模型"></a>【2003年】 NNLM:基于前馈神经网络的 N 元神经语言模型</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/5.jpg" alt="共现矩阵"></p>
<h2 id="【2013年】-word2vec"><a href="#【2013年】-word2vec" class="headerlink" title="【2013年】 word2vec"></a>【2013年】 word2vec</h2><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/1.jpg" alt="word2vec架构"><br>作为被大家公认的第一个预处理语言模型（）</p>
<h3 id="八卦时间"><a href="#八卦时间" class="headerlink" title="八卦时间"></a>八卦时间</h3><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/11.png" alt="人物关系图"></p>
<p>word2vec的主要贡献并不是其结构，因为skip gram（预测+聚类） 和 CBOW（采样+完形填空）在传统方法中也有被用到（比如n元语法），也不是层次化的softmax（Hierarchical Softmax）和 负采样（negative sampling），毕竟这些写不成公式的方法都属于trick。<br>主要贡献是【词类比】。</p>
<p>首先来解释word2vec中的各项技术：</p>
<ol>
<li>CBOW：continuous bag of words 连续词袋模型。可以理解为：用一个固定大小的窗口对语料库进行采样，并对采样后的样本做一个“完形填空”，用窗口中的周围词去预测中心词。</li>
<li>skip gram：得到了预测到的中心词之后，再反过来用中心词去预测周围的词。从直觉上来说，同一个窗口中的词汇应该在某种程度上更加近似，所以skip gram就有点聚类的味道了。</li>
<li>Hsoftmax: 可以先看一下softmax的函数图像 <img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/0.jpg" alt="softmax函数图像"> 如果直接预测，会分母太大，直接爆炸，因为少说也是大几千的token量。但如果分开来看，每一句都给一个softmax，那就不会产生爆炸，有点像激活函数和BN。</li>
<li>负采样：就是预测标签中的一个子集。因为整个标签集太大了，即使是现在的硬件水平做起来也很慢。</li>
</ol>
<p>那为什么词类比成了word2vec的主要贡献了呢？个人认为是它达到了一个【情理之中，意料之外】的效果。</p>
<p>到目前为止，预训练模型都充满了令人怀念的古早味。那时的NLP三四层是比较正常的，超过十层都属于“巨型结构”。</p>
<h2 id="【2015】Semi-supervised-Sequence-Learning"><a href="#【2015】Semi-supervised-Sequence-Learning" class="headerlink" title="【2015】Semi-supervised Sequence Learning"></a>【2015】Semi-supervised Sequence Learning</h2><p>可以说是预训练语言模型的开山之作，明确了“pretrain”的概念，提出了预训练模型的“fine-tuning”概念。虽然这篇文章放在今天看来，它的思想已经让我们感到理所当然，它的效果也没有那么地让人感觉惊艳，但是放在那个历史时刻里，它就像一盏明灯，为人们照明了一个方向。</p>
<p>论文的主要内容如下：</p>
<ol>
<li>利用自回归（AR）对一个句子进行逐词预测。</li>
<li>利用自编码方法（AE）对句子进行映射再重构。</li>
</ol>
<p>他们明确提出了，这两种算法可以为接下来的监督学习算法提供 “pretraining step”，换句话说这两种无监督训练得到的参数可以作为接下来有监督学习的模型的起始点，他们发现这样做了以后，可以使后续模型更稳定，泛化得更好，并且经过更少的训练，就能在很多分类任务上得到很不错的结果。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/8.png" alt> </p>
<p>众所周知，RNN模型虽然对时序数据建模很强大，但是因为训练时需要 “back-propagation through time”， 所以训练过程是比较困难的。Dai 和 Le 提出的预训练的方法，可以帮助RNN更好的收敛和泛化，而且在特定业务上不需要额外的标注数据，只需要收集成本很低的无标注的文本。而且这些文本与你的特定业务越相关，效果就会越好，他们认为如此一来，这种做法就支持使用大量的无监督数据来帮助监督任务提高效果，在大量无监督数据上预训练以后只要在少量监督数据上fine-tuning就能获得良好的效果，所以他们给这篇论文取名为 “半监督序列学习”。</p>
<h2 id="【2016年】context2vec-Learning-Generic-Context-Embedding-with-Bidirectional-LSTM"><a href="#【2016年】context2vec-Learning-Generic-Context-Embedding-with-Bidirectional-LSTM" class="headerlink" title="【2016年】context2vec: Learning Generic Context Embedding with Bidirectional LSTM"></a>【2016年】context2vec: Learning Generic Context Embedding with Bidirectional LSTM</h2><p>双向LSTM + mask</p>
<p>这篇文章提出的idea是学习文本中包含上下文信息的embeddings。由于词在不同上下文可以有歧义，相同的指代词也经常在不同上下文中指代不同的实体。所以NLP任务中很重要的就是考虑每个词在其上下文中所应该呈现的向量表达方式。从摘要看，这篇文章的主要贡献，是它使用了双向的LSTM可以从一个比较大的文本语料中，有效地学到了包含上下文信息的embeddings，在很多词义消岐，完形填空的任务上都取得了不低于state-of-the-art的效果。同时他们提到，之前的研究有把上下文的独立embedding收集起来，或是进行简单的平均，而没有一个比较好的机制来优化基于上下文的向量表达。所以，他们提出了context2vec ，一个能够通过双向LSTM学习广泛上下文embedding的非监督的模型。</p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/9.png" alt="softmax函数图像"></p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/10.png" alt="softmax函数图像"></p>
<h3 id="从这一年之后，NLP进入了大力出奇迹的时代"><a href="#从这一年之后，NLP进入了大力出奇迹的时代" class="headerlink" title="从这一年之后，NLP进入了大力出奇迹的时代"></a>从这一年之后，NLP进入了大力出奇迹的时代</h3><p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/3.jpg" alt></p>
<h2 id="【2017年】ELMO"><a href="#【2017年】ELMO" class="headerlink" title="【2017年】ELMO"></a>【2017年】ELMO</h2><p>更大的双向LSTM。</p>
<p>具体而言，ELMO的底层输入推荐使用已经学好的静态词向量比如Glove等，向上接两层的双向LSTM作为特征提取器，最终以语言模型作为训练任务进行学习。学好之后，每一个词会得到三个向量（底层，第一层拼接LSTM，第二层拼接LSTM），ELMO告诉你只要下游任务用到WordEmbedding的时候，就用产生的三个向量进行加权平均，其中的权重需要在新任务中进行学习。而这种方式也称为Feature-based Pre Training。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/12.png" alt="ELMO指标"><br>相应的评价指标也都做到了在当时的SOTA，并比其他的高出5-25个百分点。这么好的效果和清晰地思路也使得该论文获得了NAACL2018最佳论文。不过，现在看来ELMO也有缺点，具体而言：</p>
<ol>
<li>LSTM抽取特征的能力远弱于Transformer </li>
<li>拼接方式双向融合特征融合能力偏弱。</li>
</ol>
<h2 id="【2018】GPT-amp-BERT"><a href="#【2018】GPT-amp-BERT" class="headerlink" title="【2018】GPT &amp; BERT"></a>【2018】GPT &amp; BERT</h2><p>GPT最主要的贡献就是证明了tranformer结构比RNN更好。</p>
<p>除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式更为契合，我们一般将这种方式成为基于“Fine-tuning”的二阶段训练。<br>GPT：ImprovingLanguage Understanding by Generative Pre-Training 其实就是Transformer的decoder，一种采用了masked self-attention的训练方式。GPT总结而言有以下几点：</p>
<ol>
<li>semi-supervised learning：无监督pre-train+下游有监督fine-tune</li>
<li>multi-task learning：损失函数为预训练阶段languagemodel目标函数+ λ * 有监督softmax损失</li>
<li>12个decoder：masked-attention（12个multi-head+768维）+ point-wise FFN（3072维）+ Adam + warm-up + GELU</li>
</ol>
<p>BERT刷榜</p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/13.png" alt="BERT"></p>
<p><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/14.png" alt="BERT刷榜"></p>
<h3 id="RoBERTa，屠榜神器"><a href="#RoBERTa，屠榜神器" class="headerlink" title="RoBERTa，屠榜神器"></a>RoBERTa，屠榜神器</h3><ol>
<li>更大的训练集:16GB-&gt; 160GB</li>
<li>更长的训练时间，6W美金训练一次。</li>
<li>静态Mask -&gt; 动态Mask</li>
<li>去除了NSP</li>
</ol>
<h3 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h3><ol>
<li>词嵌入向量参数的因式分解</li>
<li>跨层参数共享 （feed-forward network、attention、all）</li>
<li>NSP 预训练任务-&gt; Sentence-order prediction (SOP)</li>
<li>去掉dropout、LAMB优化器、更大的batch-size</li>
<li>N-gram mask</li>
</ol>
<h3 id="ERINE，更加疯狂的mask训练"><a href="#ERINE，更加疯狂的mask训练" class="headerlink" title="ERINE，更加疯狂的mask训练"></a>ERINE，更加疯狂的mask训练</h3><ol>
<li>Basic-Level Masking：和BERT的方式一致，简单的随机mask单独的单词</li>
<li>Phrase-Level Masking：对命名实体进行全词Mask，包括人名、地名、机构名等</li>
<li>Phrase-Level Masking：对短语进行全词的Mask，如 a series of等</li>
<li>引入多源数据语料：百科类，新闻资讯类、论坛对话类数据并引入DLM（Dialogue Language Model）来训练模型。</li>
</ol>
<h2 id="XLNET"><a href="#XLNET" class="headerlink" title="XLNET"></a>XLNET</h2><p>本文最后介绍的就是xlnet了。<br>因为BERT的训练中有mask占位符存在。如果一个原本有4000词的文本，在训练的时候就会变成4001词（至少）。这就会造成三个问题：</p>
<ol>
<li>训练与使用不一致，因为在使用阶段是没有mask占位符的。</li>
<li>多出来的mask占位符会影响整体性能。</li>
<li>mask的训练假设是，被mask掉的词是统计独立，但显然不是。</li>
<li>因为BERT的mask占位符在训练阶段有15%，这也就是说，BERT的训练效率只有15%。</li>
</ol>
<p>xlnet希望解决上述问题。<br>根据组合数学，一个由n个词汇组成的句子会产生n！个语言模型。xlnet利用了一个巧妙的方式在训练阶段学习到了这n！个语言模型。<br><img src="/2021/03/11/%E9%A2%84%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/15.jpg" alt></p>
<!-- ## T-TA
我们知道Transformer的核心运算是Attention(Q,K,V)，在BERT里边Q,K,V都是同一个，也就是Self Attention。而在MLM中，我们既然要建模p(xi|x∖{xi})，那么第i个输出肯定是不能包含第i个token的信息的，为此，第一步要做出的改动是：去掉Q里边的token输入，也就是说第一层的Attention的Q不能包含token信息，只能包含位置向量。这是因为我们是通过Q把K,V的信息聚合起来的，如果Q本身就有token信息，那么就会造成信息泄漏了。然后，我们要防止K,V的信息泄漏，这需要修改Attention Mask，把对角线部分的Attention（也就是自身的）给Mask掉，如图所示。

![](预处理语言模型/16.png)

![](预处理语言模型/17.png) -->

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/29/%E5%8E%BB%E9%9B%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/29/%E5%8E%BB%E9%9B%BE/" class="post-title-link" itemprop="url">去雾</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-01-29 19:42:39 / 修改时间：20:33:17" itemprop="dateCreated datePublished" datetime="2021-01-29T19:42:39+08:00">2021-01-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Single-Image-Haze-Removal-Using-Dark-Channel-Prior"><a href="#Single-Image-Haze-Removal-Using-Dark-Channel-Prior" class="headerlink" title="Single Image Haze Removal Using Dark Channel Prior"></a>Single Image Haze Removal Using Dark Channel Prior</h1><p>In this paper , we proposed a simple but effecient image prior - dark channel<br>prior to remove haze from a single input image.The dark channel is a kind of<br>statistics of haze free out door images.It is based on a key observation -<br>most local patches in haze-free outdoor images contains some pixels which<br>have very low intensities in at least one color channel.Using this prior<br>with the haze imaging model,we can directly estimate the thickness of the haze and recover a high quality haze-free image.Results on a variety of<br>outdoor haze images demonstrate the power of proposed prior.More over , a<br>deepth map can also be obtained as a by-product of haze-removal.</p>
<p>In this paper,we proposed a simple but effecient image prior - dark channel<br>prior to remove haze from a single input image.The dark channel is a kind of<br>statistacs of haze free outdoor images.It is based on a key observation -<br>most local patches in haze-free outdoor images contains some pixels which<br>have very low intensities in at least one color channel.Using this prior<br>with the haze imaging model,we can directly estimate the thickness of the<br>haze and recover a high quality haze-free image.Results on a variety of<br>outdoor haze images demostrate the power of proposed prior.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/10/GNN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/10/GNN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">GNN学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-01-10 13:32:51" itemprop="dateCreated datePublished" datetime="2021-01-10T13:32:51+08:00">2021-01-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-27 19:54:47" itemprop="dateModified" datetime="2021-03-27T19:54:47+08:00">2021-03-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h1><p>本篇笔记实际上是GSN的草稿。</p>
<h1 id="图论的基本概念"><a href="#图论的基本概念" class="headerlink" title="图论的基本概念"></a>图论的基本概念</h1><p>边与两端点称为<strong>关联的(incident)</strong></p>
<p>与同一条边关联的两端点或者与同一个顶点关联的两条边称为<strong>相邻的(adjacent)</strong></p>
<p>两端点相同的边称为<strong>环(loop)</strong></p>
<p>有公共起点并没有公共终点的边称为<strong>平行边(paralled edges)</strong></p>
<p>无环并且无平行边的图称为简单图。</p>
<p>设G为无向图，$x \in V(G)$的顶点度(vertex degree)定义为G中与x关联边的数目，记为$d_G(x)$</p>
<p>1.4 导出子图和支撑子图</p>
<p>1.5 路和连通<br>图$D$(或$G$)中连接顶点$x$和y且长度为k的链W，记为</p>
<p>p32最后一段：割边和割点</p>
<p>4.3 LIMITATIONS<br>Though experimental results showed that GNN is a powerful architecture for modeling struc-<br>tural data, there are still several limitations of the vanilla GNN.<br>• First, it is computationally inefficient to update the hidden states of nodes iteratively to get<br>the fixed point. The model needs T steps of computation to approximate the fixed point.<br>If relaxing the assumption of the fixed point, we can design a multi-layer GNN to get a<br>stable representation of the node and its neighborhood.<br>• Second, vanilla GNN uses the same parameters in the iteration while most popular neural<br>networks use different parameters in different layers, which serves as a hierarchical feature<br>extraction method. Moreover, the update of node hidden states is a sequential process<br>which can benefit from the RNN kernels like GRU and LSTM.<br>• Third, there are also some informative features on the edges which cannot be effectively<br>modeled in the vanilla GNN. For example, the edges in the knowledge graph have the<br>type of relations and the message propagation through different edges should be differ-<br>ent according to their types. Besides, how to learn the hidden states of edges is also an<br>important problem.<br>22 4. VANILLAGRAPH NEURALNETWORKS<br>• Last, if T is pretty large, it is unsuitable to use the fixed points if we focus on the repre-<br>sentation of nodes instead of graphs because the distribution of representation in the fixed<br>point will be much more smooth in value and less informative for distinguishing each<br>node.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/" class="post-title-link" itemprop="url">文本生成</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-12-23 09:40:47" itemprop="dateCreated datePublished" datetime="2020-12-23T09:40:47+08:00">2020-12-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-28 17:49:08" itemprop="dateModified" datetime="2020-12-28T17:49:08+08:00">2020-12-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="写在最前面的逻辑导图"><a href="#写在最前面的逻辑导图" class="headerlink" title="写在最前面的逻辑导图"></a>写在最前面的逻辑导图</h1><p>如果看晕了就可以回过头看一下这个，整理思路。</p>
<h2 id="主线剧情"><a href="#主线剧情" class="headerlink" title="主线剧情"></a>主线剧情</h2><ol>
<li>句子可以用一个联合概率分布来描述</li>
<li>但是联合的不好算，我们用贝叶斯全概率公式改写成条件概率分布</li>
<li>条件概率分布是链式的，可以用递归的方法计算</li>
<li>自然而然的得到seq2seq结构</li>
<li>针对seq2seq的固有问题，提出attention解决方案</li>
<li>绕了一圈又突然发现attention就可以直接求解联合分布</li>
<li>深度学习的哲学1：不好算的东西都交给神经网络去拟合，我们用attention算法来拟合联合概率分布。</li>
<li>深度学习的哲学2：大力出奇迹，如果一层attention不能拟合联合分布，那就叠100层。</li>
<li>深度学习的哲学3：能用显卡解决的问题，就不要用算法。几十层attention堆叠的BERT就这么被搞出来了。而且BERT的原理十分简单（相对于传统的机器学习），但是效果又出奇的好，是NLP的里程碑。</li>
<li>既然有了求解联合分布的BERT，那么文本生成任务实际上就可以看成是全概率公式的逆向运用，用条件概率（也就是不完整的输入）去求联合分布（完整的文本）</li>
<li>整 上 花 活：当我们不再满足于单个句子的联合分布的计算的时候，比如对话或者长文本生成，这时候，针对更长的文本的联合分布的计算，我们就需要用mask方法了。虽然这个方法的名字叫做【掩码】，但实际上它的作用相当于胶水。</li>
</ol>
<h1 id="什么是文本生成？"><a href="#什么是文本生成？" class="headerlink" title="什么是文本生成？"></a>什么是文本生成？</h1><p>文本生成在计算机领域指的是用算法生成可读的文本(让代码说人话)，所谓的人话就是自然语言，这东西极其复杂，因此与自然语言相关的任务大都是非常棘手的。</p>
<h2 id="文本生成的局限性"><a href="#文本生成的局限性" class="headerlink" title="文本生成的局限性"></a>文本生成的局限性</h2><ol>
<li>模型的自然语言理解和语义分析。</li>
<li>长程依赖和全局一致</li>
<li>融入知识图谱</li>
</ol>
<p>以上这三条摘抄自一片综述性文章，都是NLP中的困难问题，当然也是研究的重点方向。</p>
<h1 id="经典的Seq2Seq"><a href="#经典的Seq2Seq" class="headerlink" title="经典的Seq2Seq"></a>经典的Seq2Seq</h1><div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/3.png" width="XXX" height="XXX">
* </div>

<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>Seq2Seq模型是一种基于RNN的序列学习模型，其主要的工作原理是求解条件概率。(不要太在意下标，问题不大)</p>
<script type="math/tex; mode=display">p(x_0) = p(x_0|x_1,x_2,x_3,...,x_n)</script><p>例如给定一个句子：</p>
<blockquote>
<p>今天晚上去吃麻辣【】</p>
</blockquote>
<p>我们已经给定了“今天晚上去吃麻辣”这几个字符，也就是对应的$x_1$至$x_n$。那么现在，我们的任务就是求解【】内的内容。所以，今天晚上究竟是吃麻辣小龙虾还是麻辣香锅，这个问题就是神经网络所需要解决的重点了。</p>
<p>对应实际的问题，我们可以将公式写成如下形式：</p>
<script type="math/tex; mode=display">sentence = \{x_0,x_1,x_2,x_3,...,x_n\}</script><p>其中，sentence是由$n$个单词组成的有序集合。只有当这些单词有序的时候，整个句子才会产生意义。虽然我们不知道自然语言的结构形式，但至少我们可以从概率的角度先进行考虑：</p>
<script type="math/tex; mode=display">P(sentence) = P(x_0,x_1,x_2,x_3,...,x_n)</script><p>但问题是上式右边是一个联合分布，求解困难。当然，如果加入一点马尔科夫的思想，这个联合分布讲道理还是可以计算出来的，也确实有这种算法，那就是n元语法：</p>
<blockquote>
<p>n元语法((n-gram grammar)建立在马尔可夫模型上的一种概率语法.它通过对自然语言的符号串中n个符号同时出现概率的统计数据来推断句子的结构关系.当n=2时，称为二元语法，当n = 3时，称为三元语法.</p>
</blockquote>
<p>我们可以退而求其次，继续将公式改写成如下形式：</p>
<script type="math/tex; mode=display">P(sentence) = p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>这样，我们就得到了一个贝叶斯视角下的句子建模了。上式是计算可行的，其中$n$并非趋近于无穷，因为一个句子总是有结束的时候。甚至当我们不开心的时候可以指定$n=1$，这就对应了一个字的回复。</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/5.png" width="XXX" height="XXX">
</div>

<p>那么应该如何计算呢？<br>我们可以递归的计算：先求第一个字的概率，然后将答案作为输入，输入神经网络中再计算第二个字的概率。。。这就是递归神经网络（RNN）了。</p>
<p>随着硬件计算能力的进步，现在的RNN一般都是结合n元语法的思想，每次的输入都是n个字符，然后去求第n+1个字符。</p>
<p>这时候，我们就可以自然而然的得出seq2seq结构了。</p>
<h2 id="seq2seq的结构"><a href="#seq2seq的结构" class="headerlink" title="seq2seq的结构"></a>seq2seq的结构</h2><div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/2.png" width="XXX" height="XXX">
</div>
encoder-decoder框架如上图所示，中间的那个“语义编码c”实际上相当于链接encoder-decoder的桥梁。我们很自然而然的觉得问题不大，但实际上这其中却蕴藏着一定的不合理：在训练阶段，模型有着明确的目标。但在测试阶段，模型就没了目标。就好比读书的时候老师会告诉你要学什么，但是工作之后就基本再也没有人管你了，唯一关心你（工作进度）的人就是产品经理了。

这个现象（笔者也是最近才听说）被称为Teacher-Foring，确实很形象。当模型没有了训练阶段的压力之后就会放飞自我。例如tensorflow官网的一个教程，用LSTM生成文本，以下是第4000轮的结果。尽管有些短文本已经能读的足够通顺，但是整体上还是拉胯。
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/安娜.png" width="XXX" height="XXX">
</div>

<p>究其原因，最主要问题是RNN的长程依赖问题，形象的说就是“记不住”和“记得太死”。当然后来的LSTM和GRU都在一定程度上缓解了这个问题，但还是治标不治本。这里就需要提到一个概念：Exposure Bias</p>
<blockquote>
<p>Exposure Bias 是在RNN（递归神经网络）中的一种偏差,即 RNN 在 训练(training) 时接受的标签是真实的值(ground truth input)，但测试 (testing) 时却接受自己前一个单元的输出(output)作为本单元的输入(input)，这两个setting不一致会导致误差累积error accumulate,误差累积是因为，你在测试的时候，如果前面单元的输出已经是错的，那么你把这个错的输出作为下一单元的输入，那么理所当然就是“一错再错”，造成错误的累积。</p>
</blockquote>
<p>Exposure Bias给我的感觉实际上是这样的：</p>
<p>工程师：马冬【梅】</p>
<p>RNN:马什么梅？</p>
<p>工程师：马【冬】梅</p>
<p>RNN：什么冬梅？</p>
<p>工程师：最后一遍，马冬梅，记住了吗？！</p>
<p>RNN：记住了记住了。</p>
<p>工程师：马什么梅？</p>
<p>RNN：马化腾</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/马冬梅.png" width="XXX" height="XXX">
</div>



<p>那有的小朋友就会问了，为啥呢？</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/1.jpg" width="XXX" height="XXX">
</div>



<p>为了回答小朋友的这个问题，我们可以设想，如果现在机器预测出了一个【小】字，那你以为最终结果一定会是【小龙虾】吗？当然不一定，一旦变成【小汉堡】，整个模型的level就拉胯了。</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/老八.png" width="50" height="XXX">
</div>

<p>其根本原因是递归问题都存在长程依赖和局部最优问题。</p>
<ol>
<li>长程依赖：RNN是递归求解的序列模型，因此当序列（本文我们就单指文本序列）足够长的时候，最开始的几个字就可能会被遗忘，因此不构成训练的依赖。就比如你还记得这篇文章的最开始是那几个字吗？所以，再回头看安娜·卡列尼娜的那个文本生成模型，长句基本都是很拉胯。</li>
<li>局部最优问题：我们以【麻辣小龙虾】为例，对于【麻辣小】这三个字来说，【龙虾】是他的最优解，因为【麻辣小】+【龙虾】构成了我们爱吃的麻辣小龙虾。但是对于【小】这个字来说，有可能【汉堡】是他的最优解，毕竟曾经的小汉堡几乎全网尽人皆知。。。但是【麻辣小汉堡】就有点意义不明。这个例子就是说，如果模型每一次都选择它所认为的“最优解”，最终有可能每一步的选择都是“正确”的，但得到的结果却是错误的。</li>
</ol>
<p>虽然我们可以用Beam-Search来解决搜索问题，但beam-search只能说是缓解了局部最优的问题（因为自然语言并不是一个最优化的问题，但目前来看我们很难找到其他的解决方案，所以我们就需要先解决局部最优化的问题）。顺着这个思路，我们也可以加长RNN的长度和深度，毕竟大力出奇迹。</p>
<blockquote>
<p>其实，写到这里的时候我就突然想起来我国科学家当年试东风1号导弹，（忘记了什么原因）导致航程不够，无法在靶点爆炸。当时的大部分方案都是加入更多的燃料，但这又会增加弹体重量，那不就飞不动了吗。唯独一个年轻人的方案是抽出燃料，理论上也确实可行（大佬就是大佬）。最后还真的成功了。</p>
</blockquote>
<p>所以，考虑到beam-search和大力出奇迹的方案都是增加计算量，我们可以反向思考，比如用mask的方法盖住几个词。这个方法苏神已经帮我们做了，可以去看他的<a href="https://kexue.fm/archives/7259" target="_blank" rel="noopener">博客</a>。</p>
<h2 id="我们到底在干什么？"><a href="#我们到底在干什么？" class="headerlink" title="我们到底在干什么？"></a>我们到底在干什么？</h2><p>说了这么多，我们到底在解决什么问题？</p>
<p>继续观察这个公式：</p>
<script type="math/tex; mode=display">P(sentence) = p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>只看右边：</p>
<script type="math/tex; mode=display">p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>每一个$p(\cdot)$都代表着一个字，如果将这些字看做节点，实际上我们就是在寻找一个“最优”的路径把他们都链接起来。当然了，如果你回过头去看这张图：</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/2.png" width="XXX" height="XXX">
</div>

<p>就会发现，“桥梁”只有一条。</p>
<p>众所周知，一条路只能链接两个城市。如果写成公式，那就是：</p>
<script type="math/tex; mode=display">p(x_0)p(z)p(x_1|x_0)p(z)p(x_2|x_0,x_1)\cdots p(z)p(x_n|x_0,x_1,\cdots,x_{n-1})</script><p>$p(z)$就是神经网络所要拟合的参数之一(对应于上图的“语义编码c”)，它告诉了句子应该以什么样的概率转移为另一个概率。</p>
<p>单一的$p(z)$显然是不充分的，【马】这个字显然是不能以相同的概率转移为马冬梅或者马化腾或者马云。</p>
<p>所以，为了造更多的“桥”：</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/4.png" width="XXX" height="XXX">
</div>

<p><a href="https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216" target="_blank" rel="noopener">参考博客</a></p>
<p>注意力机制就应运而生了。</p>
<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><script type="math/tex; mode=display">self\, attention = Softmax(\frac{QK^T}{\sqrt{d}})V</script><p>其中的QKV同一个东西的三种投影。所以，如果不看softmax一项，整个公式实际上是在求非归一化的联合概率分布。</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/att.png" width="XXX" height="XXX">
</div>

<p>假设上图就是self attention的计算结果，每一行每一列的和都不是1，所以没有归一化。但从感觉上来看，这东西长的可真的像联合分布的计算结果啊！我们随便上网找一张联合分布的图片:</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/jdf.png" width="XXX" height="XXX">
</div>

<p>简直神似啊！</p>
<p>仔细一想…好像又绕回了最初的原点。<br>我们来分析一下：</p>
<ol>
<li>早期的NLP探索者们用n元语法模型去求解联合分布，但是受制于当时的硬件条件，所以n元语法模型的探索也都止步在个位数。</li>
<li>后来出现了word2vec，滑动窗口实际上相当于n元语法中的n，但窗口大小依旧没有突破个位数。</li>
<li>神经网络大行其道，RNN递归求解条件概率，直逼联合分布。</li>
</ol>
<p>但是随着seq2seq问题的研究（主要是当年研究自动翻译的那些学者，著名的attention公式就是从Transformer的研究中诞生的），大家突然发现，直接用attention去求联合分布不就OJBK了吗？？？</p>
<p>然后就出现了不讲武德的BERT，虽然算法层面没有太多的进步，但是从深度学习的哲学水平上来看，这波BERT在大气层。（一部分算法工程师的梦想是用数学的方法去解决复杂的问题，但是另外一批算法工程师直接对复杂问题重拳出击）</p>
<div align="center">
<img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/6.png" width="300" height="XXX">
</div>

<!-- 简单的改动，极致的享受 -->
<h2 id="mask方法：建造跨海大桥"><a href="#mask方法：建造跨海大桥" class="headerlink" title="mask方法：建造跨海大桥"></a>mask方法：建造跨海大桥</h2><p>我们知道，自然语言是有其结构信息的，而问题在于我们对自然语言的结构知之甚少。从小学开始我们就要一直学习语法，定语从句，倒装句之类；将语法结构应用于自然语言处理也确实是NLP的最初之路。但这种路子很快就被证明是走不通的，因为我们不知道如何描述结构。反观BERT的成功，再怎么成功，其数学基础也是建立在统计学之上的，至少我们知道联合概率分布是如何计算的。现在的问题是，我们既不知道结构如何描述，也不知道结构如何计算。</p>
<font color="red">所以，mask是一种通过表象去计算自然语言结构的主要方式。mask虽然被称为掩码，但其主要的功能是【补全】，就好像英语考试中的完形填空，在补全句子信息的过程之中也学习到了句子的整体结构，做的多了，自然而然的也就形成了“语感”。这个语感，实际上就是脑子里对自然语言结构的建模。</font>

<p>在之前的叙述中，我们所关注的都是单个句子的联合分布。秉承深度学习的哲学，我们可以用mask的方式对多个句子重拳出击，甚至说如果你开心(有显卡)，你也可以同时对多篇文章一起重拳出击。</p>
<p>具体做法就是把多个句子当成一个句子，每一个句子的句尾加入一个特殊的标记，比如$[cut]$，然后多个句子顺序拼接，直接输入神经网络进行计算。</p>
<!-- # 更多的思考
<font color=red>当然，我们也可以从互信息的角度去思考问题。</font>



为了更加形象的讲解，我们可以想象如下的一个图形：

![](文本生成/h.png) -->
<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><p>gan的主要问题是：</p>
<ol>
<li>训练困难，有可能因为梯度爆炸而导致模式崩塌。（这个训练困难的问题我不确定还是否存在，因为看到的论文是在18年发表的，而18年的时候又提出了f-gan，其中的共轭算法已经基本避免了这个问题）</li>
<li>多样性不足：具体表现为GAN文本生成模型总是会生成一些短小的句子，从而可以获得更高的分数。（但这个问题我感觉也已经基本解决了，那就是依靠mask方法做长文本输入。）</li>
</ol>
<h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>强化学习目前给我的感觉仍旧是牛刀杀鸡。我们来简单的讨论一下强化学习中的两个概念：收益和动作空间。（这两个问题也许是强化学习在自然语言处理方面效果不显著的主要原因）</p>
<p>首先，收益相当于深度学习中的损失函数，因为在数学上，前者是求最大值，后者是求最小值，没有本质上的区别。对我来说，也许不同的概念会帮助人们产生不同的理解，但这种名称上的改变并没有给算法带来本质上的不同：因此我就可以在损失函数前面加上一个符号，然后将这个负损失定义为收益。</p>
<p>其次，动作空间。这个东西就有点像几十年前人们精心设计的特征一样。如果是对于一些简单问题，那当然是非常有效；但对于自然语言这种极其复杂的问题来说，动作空间就显得有点“狭小且破碎”，因为人工定义的动作空间目前还不能完全描述自然语言。反向思考，如果动作空间能够几乎完整的描述自然语言的特性，那我为什么不去手工写正则表达式呢？</p>
<h1 id="文本生成的关键在于MASK"><a href="#文本生成的关键在于MASK" class="headerlink" title="文本生成的关键在于MASK"></a>文本生成的关键在于MASK</h1><h2 id="用mask来取代动作空间"><a href="#用mask来取代动作空间" class="headerlink" title="用mask来取代动作空间"></a>用mask来取代动作空间</h2><h1 id="着手建模"><a href="#着手建模" class="headerlink" title="着手建模"></a>着手建模</h1><p>依旧是基于公式：</p>
<script type="math/tex; mode=display">p(x_0) = p(x_0|x_1,x_2,x_3,...,x_n)</script><p>个人认为，这个是最好的文本建模方法了，公式中既包含了结构信息（贝叶斯全概率公式）又能够体现语义信息（条件概率），是非常好的生成模型。</p>
<p>但是好东西也是不容易获得的，条件概率的计算量可以达到无穷，力大砖飞的BERT系列甚至就可以看做一个非常强悍的条件概率计算模型。如何降低条件概率的计算复杂度问题仍旧是一个非常值得研究的问题。</p>
<p>好了，言归正传。首先给定一个$sentence$作为输入</p>
<script type="math/tex; mode=display">P(sentence) = p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)\cdots p(x_n|x_0,x_1,\cdots,x_{n-1})</script><h1 id="文献时间"><a href="#文献时间" class="headerlink" title="文献时间"></a>文献时间</h1><h2 id="第一篇"><a href="#第一篇" class="headerlink" title="第一篇"></a>第一篇</h2><p><a href>An Auto-Encoder Matching Model for Learning Utterance-Level<br>Semantic Dependency in Dialogue Generation</a></p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/2.png" alt="原文loss"></p>
<p>编码器和解码器都是LSTM，作者希望在隐变量空间中将上文信息与下文信息对齐，如大图中的中间那一层所示。</p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/1.png" alt="原文loss"></p>
<p>对齐的方式是L2范数。其实改成别的散度也是可以的。</p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/3.png" alt="原文loss"></p>
<p>生成效果如图所示。</p>
<h2 id="第二篇"><a href="#第二篇" class="headerlink" title="第二篇"></a>第二篇</h2><p><a href>CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning</a></p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/4.jpg" alt="模型任务"></p>
<p>这篇文章在文本生成的任务中引入了人类的常识。</p>
<font color="red">其作用相当于引入了知识图谱</font>，这就相当于在求联合分布的时候给定了一些标签，标签相当于条件，在某种程度上相当于求解条件分布。<font color="blue">虽然文章的任务是做图片描述，但有一些地方还是值得我们参考的。</font>
给定的输入为若干个单词，目标是生成一句通顺的人话。

作者认为完成这个任务的条件主要有两个：
1. 词与词之间的关系
2. 合理的语法

作者首先构造了一个“概念的集合”，这个集合中的概念都是一些基本的名词和动词：
$$\{c_1,c_2,...\} \in \mathcal{X}$$

以及一个句子集合$\mathcal{Y}$，里面装的都是人话。模型的目的就是去学习$\mathcal{X} \rightarrow \mathcal{Y}$的映射。

那么这个映射应该怎么去学习呢？

***<font color="red">对比学习 </font>***

首先我们构造训练集$\mathcal{T}$，$\mathcal{T}$是$\mathcal{Y}$的一个子集，但是我们需要给$\mathcal{T}$加上一个限制条件:<font color="red">所有的$\mathcal{T}$ 都必须包含$\mathcal{X}$中的若干个元素</font>

<p>举例来说（本节第一张图片所示），训练集可以长成这个样子：<br>x1 : {苹果，袋子，放}<br>t1 ：{一个女孩儿把苹果放在她的袋子里}<br>x2 ：{苹果，树，摘}<br>t2 ：{一个男人从树上摘了一些苹果}<br>x3 : {苹果，篮子，洗}<br>t3 ：{一个男孩从篮子里拿了几个苹果去洗}</p>
<p>好了，现在我们有了三个训练样本。里面包含了实体，以及隐藏在句子中的实体关系。在训练阶段，假设输入的概念集合为:{梨，袋子，放}，【梨】这个词在训练集中从来没有出现过，那么就可以认为<font color="red">【鸭梨】=【苹果】，在{袋子，放}的条件下</font></p>
<!-- 当然了，上述方法有点类似于对比学习，但我个人认为对比学习的本质不是找相同，而是找不同。

反正体现在哪里？个人认为，反正体现在训练集当中。新的实体，比如【鸭梨】，从来没有在训练集中出现过。现在要证明【鸭梨】是一个实体，那么反证法就是【鸭梨】不是一个实体。

但是与【鸭梨】相关的单词是在训练集中出现过的，那这时候如果想证明 -->
<h2 id="第三篇"><a href="#第三篇" class="headerlink" title="第三篇"></a>第三篇</h2><p><a href>Adversarial Ranking for Language Generation</a><br><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/5.png" alt><br>图中,G是生成器生成的文本，H是人话。RANKER是一个打分器，给所有的输入样本打分，本质上是一个discriminator.打分方法本质上还是对比学习的方法，请参考第二篇。</p>
<p>具体的做法是，针对相同的$x \in \mathcal{X}$,挑选出一些包含x的人话H，然后用这些x生成样本G，将${H,G}$输入到打分器中排序，并根据排序来设计loss.<br><!-- 这片文章是利用NLP中的负采样方法来训练GAN。针对GAN的训练困难问题，负采样确实是一种好方法。虽然文中将这种方法称为Rankings
 --></p>
<h2 id="第四篇"><a href="#第四篇" class="headerlink" title="第四篇"></a>第四篇</h2><p><a href>Generating Text through Adversarial Training using Skip-Thought Vectors</a><br>文本是离散的，所以不适合用GAN来训练。虽然我们可以用嵌入的方式得到词向量，但在输出端还是需要把词向量转化成离散的字，这样才能得到一句文本。就比如我们假设”你好”=1.0,但是”1.01”是什么，谁也不知道。</p>
<p>所以本文作者打算用稠密的句向量来做。所谓的稠密就是连续，与离散相对的概念。<br>在介绍这篇文章之前，我们要先看看什么是Skip-Thought。<br><a href="https://zhuanlan.zhihu.com/p/100271181" target="_blank" rel="noopener">一种传统的句表示学习方法——Skip-Thought Vectors</a></p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/6.jpg" alt><br>然后，原文作者将文本转化为稠密的句向量输入到GAN中做训练，输出的稠密向量用Skip-Thought 的解码器解码一下就是句子了。</p>
<p><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/7.jpg" alt></p>
<h2 id="第五篇"><a href="#第五篇" class="headerlink" title="第五篇"></a>第五篇</h2><p><a href>Learning Neural Templates for Text Generation</a></p>
<p>这篇文章虽然是说用神经网络学习模板，但实际上已经是在践行强化学习了。如果将模板想象成动作空间，那就容易理解一些了。</p>
<h2 id="第六篇"><a href="#第六篇" class="headerlink" title="第六篇"></a>第六篇</h2><p><a href>Long Text Generation via Adversarial Training with Leaked Information</a></p>
<p>文章作者认为，训练GAN的时候，目标文本序列作为控制信号是处于最后一个阶段的，而生成器所生成的隐变量序列中是包含结构信息和语义信息的，而控制信号作为离散的序列只是在最后阶段与隐变量交互信息。什么意思，就是说稠密的隐变量只有在argmax运算之后才能与文本计算距离，而argmax却大量的损耗了隐变量所蕴含的信息。<br>那么作者做了一件什么什么事情呢？那就是把控制序列的最后一个信号，也就是文本的最后一个字作为额外的输入信息，以及判别器的编码，输入生成器中，。<br>一般的GAN，生成器和判别器之间只有在BP阶段才有信息交互。</p>
<p>模型整体结构如下图所示。<br><img src="/2020/12/23/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/论文/8.jpg" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/29/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/29/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">生成模型学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-11-29 22:18:31 / 修改时间：22:23:20" itemprop="dateCreated datePublished" datetime="2020-11-29T22:18:31+08:00">2020-11-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/14/%E5%9F%BA%E7%A1%80%E6%8B%93%E6%89%91%E5%AD%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/14/%E5%9F%BA%E7%A1%80%E6%8B%93%E6%89%91%E5%AD%A6/" class="post-title-link" itemprop="url">基础拓扑学</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-14 10:11:17" itemprop="dateCreated datePublished" datetime="2020-10-14T10:11:17+08:00">2020-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-14 20:22:40" itemprop="dateModified" datetime="2020-11-14T20:22:40+08:00">2020-11-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hausdorff空间定义：如果对于拓扑空间X中任意两个不同的点$x_1$和$x_2$，分别存在$x_1$和$x_2$的邻域$U_1$和$U_2$使得这两个邻域无交，则称X为一个Hausdorff空间.</p>
<p>定理：Hausdorff空间中的任何有限集都是闭的。</p>
<p>拓扑学核心任务：寻找拓扑不变性质</p>
<p>拓扑的定义：<br>设X是一个非空集合。X的一个子集族$\tau$称为X的一个拓扑，如果它满足：</p>
<ol>
<li>X,$\varnothing$都包含在$\tau$中。</li>
<li>$\tau$中任意多个成员的并集都在$\tau$中。</li>
<li>$\tau$中有限多个成员的交集都在$\tau$中。</li>
</ol>
<p>集合X和拓扑$\tau$一并称为拓扑空间。</p>
<p>诱导拓扑：若X为拓扑空间，Y为X的子集，Y上的子空间拓扑或诱导拓扑是以X的开集与Y的交集作为这个拓扑的开集而定义的。</p>
<p>定理：一个子集为闭集，当且仅当它包含了自己全部的极限点。</p>
<p>定理：A的闭包是包含A的最小闭集，换句话说，是包含A的一切闭集之交。</p>
<p>拓扑基：设集合X上有了一个拓扑，$\beta$为这个拓扑的一组开集，使得每个开集可以写成$\beta$中成员的并集，则$\beta$叫做这个拓扑的一组拓扑基。</p>
<p>同胚：是指一个连续单一满映射，它的逆映射也连续。</p>
<p>定理：设X为拓扑空间，$\mathcal{F}$为X的一组开集，他们的并集是整个X。这样的一组开集叫做X的 <strong>开覆盖</strong>。若$\mathcal{F}’$ 是$\mathcal{F}$的一个子集，并且若$\bigcup \mathcal{F}’=X$，则$\mathcal{F}’$叫做$\mathcal{F}$的一个子覆盖。</p>
<p>定义：拓扑空间X紧致，假如X的任何开覆盖包含有限子覆盖。</p>
<p> 群”是一个“ 有结构的集合”</p>
<p> 紧致空间的无穷子集必有极限点。（如果在紧致空间内取出无穷多个点，那么这些点的分布必然在某处显得很拥挤，这就是极限点）</p>
<p>道路：一个连续映射$\gamma{:[0,1]\rightarrow} X$</p>
<p>定义：空间被称为道路连通的，假如它的任意两点可以用一条道路连结。$\gamma{(0)}$和$\gamma{(1)}$分别叫做道路的起点和终点。注意，若$\gamma^{-1}$定义做：</p>
<script type="math/tex; mode=display">\gamma^{-1}{(t)} = \gamma{(1-t)} ,0 \le t \le 1</script><p>则$\gamma^{-1}$是连结$\gamma{(0)}$和$\gamma{(1)}$的一条道路。</p>
<p>（基本群是利用空间中的道路连通构建出来的）</p>
<p>定理：道路连通空间是连通的。</p>
<p>粘合拓扑：设X为拓扑空间，$\mathcal{P}$为X的一族互不相交的非空子集，使得$\bigcup{\mathcal{P}}=X$ ，这样的一个族$\mathcal{P}$叫做X的一个划分。按照下述方式制造一个新空间Y，叫做粘合空间：Y是$\mathcal{P}$的成员，并且若$\pi : x\rightarrow y$将X的每点送到$\mathcal{P}$所属的成员中，而Y的拓扑是使$\pi$为连续的最大拓扑。于是，Y的子集O为开集，当且仅当$\pi^{-1}{O}$在X为开集。我们可以把Y看做是从空间X出发，把属于$\mathcal{P}$的每一子集粘合成为一点而形成的空间。</p>
<p>定义：G是一个拓扑群，假如它既是一个Hausdorff空间，又是一个群，并且这两个结构在下述意义之下是相容的：群的乘积$m:G\times G \rightarrow G$,与群的求逆运算$i:G\rightarrow G$都是连续映射。</p>
<p>定义：设X是一个拓扑空间，$A\subset X$.如果点$x\in X$的每一个邻域U中都有A中异于x的点，即$U \bigcap (A-{x}) \neq \varnothing$,则称点x为A的一个<strong>凝聚点</strong>或<strong>极限点</strong>。集合A的所有凝聚点构成的集合称为A的导集，记做$d(A)$，</p>
<p>定义：设X是一个拓扑空间，$A\subset X$.集合A与A的导集$d(A)$的并$A\bigcup d(A)$称为集合A的闭包，记做$\overline{A}$</p>
<p>定义：设X是一个拓扑空间，$A\subset X$.如果A是点$x\in X$的一个邻域，即存在X中的一个开集V使得$x\in X \subset A$，则称点x是A的一个内点。集合A的所有内点所构成的集合称为A的内部，记做$\mathring{A}$</p>
<p>每一个球形邻域都是开集，从而任意多个球形邻域的并也是开集；另一方面，假如U是度量空间X中的一个开集，则对于每一个$x\in U$有一个球形邻域$B(x,\epsilon) \subset U$,因此$U = \bigcup_{x\in U}B(x,\epsilon)$.也就是说，一个集合是某度量空间中的一个开集，当且仅当它是这个度量空间中的若干个球形邻域的并。因此我们可以说，度量空间的拓扑是由它的所有的球形邻域的集族求并这一运算“产生”出来的。</p>
<p>定义：设$(X,\mathcal{T})$是一个拓扑空间，$\mathcal{B}$是$\mathcal{T}$的一个子族。如果$\mathcal{T}$中的每一个元素（即拓扑空间X中的每一个开集）是$\mathcal{B}$中某些元素的并，即对于每一个$U\in \mathcal{T}$,存在$\mathcal{B}<em>1 \subset \mathcal{B}$，使得$U=\bigcup</em>{B\in \mathcal{B_1}}B$,则称$\mathcal{B}$是拓扑$\mathcal{T}$的一个基，或称$\mathcal{B}$是拓扑空间X的一个基。</p>
<p>“子空间”实际上是从大拓扑空间中“切割”出来的一部分。这里有一个问题，概言之就是：一个拓扑空间什么时候是另一个拓扑空间的子空间？换言之，一个拓扑空间在什么条件下能够“镶嵌”到另一个拓扑空间中去？</p>
<p>定义：设X和Y是两个拓扑空间，$f:X\rightarrow Y$，映射$f$称为一个<strong>嵌入</strong>，如果它是一个单射，并且是从X到它的像集的$f(X)$的一个同胚。如果存在$f:X\rightarrow Y$，则我们称<strong>拓扑空间X可嵌入拓扑空间Y</strong>。</p>
<p>拓扑空间的某种性质，如果为一个拓扑空间所具有也必然为它在任何一个连续映射下的像所具有，则称这个性质是一个在<strong>连续映射下保持不变的性质</strong>。</p>
<h1 id="点集拓扑-GTM27"><a href="#点集拓扑-GTM27" class="headerlink" title="点集拓扑 GTM27"></a>点集拓扑 GTM27</h1><p>disjoint : $A \bigcap B = 0$</p>
<p>the absolute complement of a set A is written $\sim A$</p>
<p>the relative complement of A is written as $X \sim A = X\bigcap \sim A$</p>
<p>if R is a relation and A is a set,then R[A] ,the set of all R-relatives all points of A.</p>
<p>the members of the topology of $\mathcal{T}$ are called <strong>open</strong> relative to $\mathcal{T} $</p>
<p>normal space(p128) : a space is normal iff disjoint closed sets have disjoint neighbourhoods.And another statement is suggested that <strong>a family of neighbourhoods of a set is a base of neighbourhood system of the set iff every neighbour of the set contains a member of the family</strong></p>
<p>regular space(p129):<strong>iff for each point x and each neibourhood U of x , there is a closed neighbourhood V of x ,such that V $\subset$ U</strong>,that is a closed neighbourhoods of each point is a base for the neighbourhood system of the point.</p>
<p>Suppose that one-point sets are closed in X.Then X is said to be 【regular】if for each pair consisting of a point x and a closed set B disjoint from x, there exist disjoint open sets containing x and B, respectively.<br>The space X is said to be 【normal】if for each pair A,B of disjoint closed sets of X,there exist disjoint open sets containing A and B, respectively.</p>
<p>第二可数公理：一个拓扑空间如果有一个可数基，则称这个空间满足第二可数公理。</p>
<p>第一可数公理：一个拓扑空间如果在它的每一点处都有一个可数邻域基，则称这个可数空间是第一可数的。</p>
<p>p129，结尾的一段很重要</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/" class="post-title-link" itemprop="url">论文写作</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-29 20:08:06" itemprop="dateCreated datePublished" datetime="2020-06-29T20:08:06+08:00">2020-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-03 12:47:11" itemprop="dateModified" datetime="2020-09-03T12:47:11+08:00">2020-09-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Lectrue-1"><a href="#Lectrue-1" class="headerlink" title="Lectrue 1"></a>Lectrue 1</h1><h1 id="Structure-and-Clarity"><a href="#Structure-and-Clarity" class="headerlink" title="Structure and Clarity"></a>Structure and Clarity</h1><h2 id="Know-your-readers-who-want-to"><a href="#Know-your-readers-who-want-to" class="headerlink" title="Know your readers who want to"></a>Know your readers who want to</h2><ul>
<li>know the result of the paper to determine to read on - <font color="purpo">Abstract</font></li>
<li>know the intuitively the key idea behind this paper,how it is derived,what are the significance and implications - <font color="purpo">Introduction</font></li>
<li>know all the gory details <font color="purpo">whole paper</font></li>
</ul>
<h2 id="Difference-between-writing-papers-in-English-and-Chinese"><a href="#Difference-between-writing-papers-in-English-and-Chinese" class="headerlink" title="Difference between writing papers in English and Chinese"></a>Difference between writing papers in English and Chinese</h2><h3 id="Cultural"><a href="#Cultural" class="headerlink" title="Cultural"></a>Cultural</h3><ul>
<li>Confidence versus modesty:中国认为谦逊是好的表达方式，而外国人则认为证据才是最有力的。</li>
<li>“Responsibility” of delivery(how much the reader can understand you). 中国人认为，背景知识应该是读者所预先掌握的；而外国人则相反。</li>
<li>Language:<font color="red">Direct translations often do not work</font>,so,can i write paper in Chinese and then translate it into English?Theacher do not recommond you to do so,cause that is not a good practice.Ofent come to a bad translation.</li>
</ul>
<p>No personal expression in your paper! </p>
<h1 id="Lectrue-2"><a href="#Lectrue-2" class="headerlink" title="Lectrue 2"></a>Lectrue 2</h1><h1 id="Main-Contents-of-a-Scientific-Paper"><a href="#Main-Contents-of-a-Scientific-Paper" class="headerlink" title="Main Contents of a Scientific Paper"></a>Main Contents of a Scientific Paper</h1><p>first of all , title. </p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h2><ul>
<li>Best representation of the paper : contributions and unique features.</li>
<li>Generally not a sentence </li>
<li>Importance of right word order</li>
<li>No waste of words</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>Summary in a few hundred words</li>
<li>Very important to attract readers</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>Motivations and problem descriptions:what is motivation to write the paper and what is the main problem we discussing</li>
<li>Academic and/or industrial impact</li>
<li>Literature reviews <ul>
<li>Show the SOTA</li>
<li>Cite references available to general public</li>
<li>Avoid offensive remarks</li>
<li>Use the most recent results </li>
<li>Orgnization of the paper (optional)</li>
</ul>
</li>
</ul>
<!-- and then,we need to review the exsisting work , what are the methodology,what are the result..we need to show the SOTA... -->
<h2 id="Problem-Descriptions-of-Formulation"><a href="#Problem-Descriptions-of-Formulation" class="headerlink" title="Problem Descriptions of Formulation"></a>Problem Descriptions of Formulation</h2><ul>
<li>System configuration , environment,etc</li>
<li>Definitions of all notation for variables,symbols,etc.</li>
<li>Mathematical models and formulations.</li>
</ul>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><ul>
<li>Main ideas to solve the problem</li>
<li>Theorems and proofs</li>
<li>Descriptions of the approach or algorithm</li>
</ul>
<h2 id="Results-of-applications-experiment-or-testing"><a href="#Results-of-applications-experiment-or-testing" class="headerlink" title="Results of applications , experiment or testing"></a>Results of applications , experiment or testing</h2><ul>
<li>Description of the practical or experiment system</li>
<li>Description of the data and the associated system</li>
<li>Descriptions of the computer system</li>
<li>Presentation of the results:tables,figures,etc.</li>
<li>Applications and benefits</li>
</ul>
<h2 id="Conclution"><a href="#Conclution" class="headerlink" title="Conclution"></a>Conclution</h2><p>conclution is not the repeat of the abstract.</p>
<ul>
<li>Summary of your findings (not a repeat of abstract)</li>
<li>Concluding remarks</li>
</ul>
<h2 id="Acknowledgment"><a href="#Acknowledgment" class="headerlink" title="Acknowledgment"></a>Acknowledgment</h2><ul>
<li>Acknowledge the funding agencies</li>
<li>Thank individuals</li>
</ul>
<h2 id="References-Very-important"><a href="#References-Very-important" class="headerlink" title="References(Very important!!!)"></a>References(Very important!!!)</h2><ul>
<li>List references in approproate fromat</li>
<li>Have recent references if possible</li>
<li>Give complete information</li>
<li>List appropriate numbers of references</li>
<li>Do not list anything that is not refereed to</li>
<li>Do not list too many home made references</li>
</ul>
<p>List references in appropriate format and try to meet the requirements of the journal</p>
<p>format must be consistently</p>
<h1 id="论文的三段式结构"><a href="#论文的三段式结构" class="headerlink" title="论文的三段式结构"></a>论文的三段式结构</h1><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/1.jpg" alt></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>老师将论文大致分为三个部分。通常来说，引言段和结论段应该以议论文的形式来进行书写，而中间段可以以写说明文的方式进行书写。所以引言段和结论段通常来说需要较强的逻辑性，而中间段则需要写清晰的实验过程。</p>
<ul>
<li>引言段<br>需要得出的结论是，这篇文章的研究目标是合理的，论据应该是从各种文献中收集来的相关资料。<ul>
<li><font color="blue">从论述结构的角度来看</font>，引言段是从一般到特殊（倒三角），从一个较为普遍，大家都比较接受的问题或者观念入手，逐步引导读者，逐步深入，最终说服读者接受一个特定的研究目标。</li>
</ul>
</li>
<li>中间段<br>Objective , method , result</li>
<li>结论段<br>目的是为了说明这篇文章是有价值的，所使用的论据<font color="red">主要</font>是自己的（即中间段）的研究结果，同时也包括一些从文献中搜集来的相关资料。<ul>
<li><font color="blue">从论述结构的角度来看</font>，结论段是从特定到普遍（正三角），从一个特定的研究结果开始，引导读者接受这个结果所带来的普遍的意义。</li>
</ul>
</li>
</ul>
<p>写论文越早越好，最好早于做实验和采集数据，因为写作引言段是不需要有任何实验数据的。而且，一个与Introduction相对应的单词叫做outroduction，是conclution的同义词。这就好像，如果把读者比作猎物，那么Introduction就像是洞口，是吸引猎物上钩的诱饵；那么中间段就是洞内，吸引猎物继续前进，看一看；最后，结论段将猎物带出洞。</p>
<h2 id="引言段"><a href="#引言段" class="headerlink" title="引言段"></a>引言段</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/2.jpg" alt><br>引言应落脚于研究目标，所以在引言结尾一边都连接着对应研究目标的完整描述。在本模板中，将引言和研究目标作为一个整体来考虑，可以更好的分析他们之间的逻辑关系。在这个模板中，可以分为相互对应的两大段：</p>
<ul>
<li>在第一段，确定一个研究机会。从一个大问题开始，到提出一个研究空白、研究问题结束。</li>
<li>在第二段，提出一个解决方案，并占领这个研究机会。即：清晰的描述研究目标，填补在第一段中的研究空白</li>
</ul>
<h2 id="中间段"><a href="#中间段" class="headerlink" title="中间段"></a>中间段</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/3.jpg" alt><br>如何确立一个好的研究课题以及怎样才算是一个好的研究课题：</p>
<ol>
<li>填补研究空白：<br>致力于发现现有文献中的研究空白或者漏洞，而我们提出一个课题去弥补这个漏洞。一般来说，空白都是领域内的老问题，虽然被普遍认可，但是填补的难度很高，具有挑战性。因此，主要强调课题中的创新部分，它可能是一个新的解决方案或者是从其他领域中借鉴过来的新的想法。同时，还因为它是一个老问题，还需要了解前人都做过怎样的尝试，我们需要论述，我们的研究与前人的研究有着怎样的不同。或者我们的研究怎样才能与前人的研究成果相依托相配合。</li>
<li>延伸现有研究：<br>在这个方法中，我们追踪某一领域的研究历史，一项接一项的成就，每一项都承接着上一项的历史，建立在上一项的基础之上，同时又为下一项研究奠定了基础。延伸现有研究往往会有新的问题，甚至是新的领域。新的问题往往蕴含着新的机会，但意义往往并不十分明确，对于没有经验的研究者来说，容易迷失。所以，要始终牢记长期目标，要时常问自己，这一项工作的意义和必要性，我们选择的这一项研究课题对于我们的长期目标和受众读者来说有什么实际的意义和贡献？</li>
<li>复制成功的研究：<br>将一项成功的研究在不同的情况下进行复制。</li>
</ol>
<p>为了确定一个好的课题，上述三种方式可以单独使用也可以组合使用。那么，怎样才算是一个好的课题呢？这里也有三条准则：</p>
<ul>
<li>具体可行：所有的术语和变量都应该有明确的定义。整个课题具有明确的边界。不应该存在任何先入为主的假设。</li>
<li>开放的和可争议的：可证伪性是区分科学与非科学的标准。如果课题是不可证伪的，那么就是不可取的。</li>
<li>一个好的课题应该能够激发创新：技术的发展为创新制造机会，而技术的发展可能来自本领域，但更多的却是来自其他不同的领域。<font color="red">跨领域甚至是跨学科的思考往往是创新的来源。</font></li>
</ul>
<p>最后，有两种研究课题是最牛的：</p>
<ol>
<li>研究者填补了当前领域内的所有漏洞，完善了目前的知识框架，在他的工作之后，没有人能够做进一步的改进了。</li>
<li>研究者突破了目前的知识框架，为更多的机会打开了大门，在他的工作之后，有很多研究者跟随着他的脚步进行着研究。</li>
</ol>
<h2 id="确立具体的研究目标"><a href="#确立具体的研究目标" class="headerlink" title="确立具体的研究目标"></a>确立具体的研究目标</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/4.jpg" alt><br>大处着眼，小处着手。将长期目标和短期目标结合起来。</p>
<ul>
<li>大处着眼：不管我们目前自身的能力或者是自身的资源是多么的有限，我们都希望能够尽可能的参与到一些有意义的大事业中，从而使我们的工作更有意义。如果我们局限于自身的实力，而只甘心做一些细枝末节的项目，那么我们的工作很难有大的影响，也很难对我们的工作保持长久的热情。更重要的是，我们丧失的是成长的空间。</li>
<li>小处着手：不管我们的理想有多么的宏伟，我们每一次的进步，都应该根据自身现有的条件，踏踏实实的从小事做起。</li>
</ul>
<p>因此，要将手头需要做的小事和心中所要做的大事联系起来。如果我们只关注眼前的小事而没有心中的大事，那么我们的项目可能就会由于苟且而没有意义。相反，如果我们只关注心中的大事，而不去做手头的小事，那么我们的项目可能就会限于大而空。</p>
<p>如果把我们的长远目标比作一个宏伟的建筑，那么我们的当前目标可能只是一个小小的砖块。在实际中，我们的长远目标可能是和许多别的研究者所共享的，大家的贡献合在一起就能够完成一个伟大的事业。</p>
<h2 id="文献综述的两个基本点"><a href="#文献综述的两个基本点" class="headerlink" title="文献综述的两个基本点"></a>文献综述的两个基本点</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/5.jpg" alt><br>学而不思则罔，思而不学则殆。</p>
<ul>
<li>有所学：在文献综述中，要展示与我们专业相关的所有的知识，有一个全面的了解。要尽可能的包括<font color="red">所有的，相关的，主要的</font>文献。如果在我们的文献综述中遗漏了一个与文章目标相关的文献，那么在审稿人的眼中，我们的论文就是不专业的。</li>
</ul>
<h2 id="阅读文献的两个阶段"><a href="#阅读文献的两个阶段" class="headerlink" title="阅读文献的两个阶段"></a>阅读文献的两个阶段</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/6.jpg" alt><br><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/7.jpg" alt></p>
<p>好记性不如烂笔头。</p>
<p>将论文中的内容抽象出来。</p>
<h2 id="用故事的逻辑写文献综述"><a href="#用故事的逻辑写文献综述" class="headerlink" title="用故事的逻辑写文献综述"></a>用故事的逻辑写文献综述</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/8.jpg" alt><br>一个好的故事应该是有起伏的，有起伏才能建立起一个吸引人的故事。故事的起伏能够积累和释放听众的情绪，从而产生强大的影响力。那么我们怎样才能说好一个有起伏的故事呢？图中的木板包括四个要素：</p>
<ol>
<li>情境（陈述背景）</li>
<li>冲突（矛盾的表象）</li>
<li>关键问题（矛盾的本质）</li>
<li>解决方案（如何解决矛盾）</li>
</ol>
<p>其中：</p>
<ul>
<li>SCQA是一个讲故事最自然的顺序。而在实际的写作情况中，这个顺序是可以有所调整的。</li>
<li>有的时候，关键问题和解决方案比较易于为读者所接受，那么，就可以采用比较省时间的开门见山的方式，也就是<font color="red">QASC</font>或者<font color="red">QACS</font>。首先提出关键问题和解决方案，直接点明本文的解决问题和研究目标，随后再慢慢介绍情景和冲突。</li>
<li>如果你希望你的文章能够有冲击力，也可以采用<font color="red">CSQA</font>的顺序，首先提出冲突，引发读者的关注，随后再介绍情境和后续的解决方案。</li>
</ul>
<h2 id="文献分析的三种逻辑推理方法"><a href="#文献分析的三种逻辑推理方法" class="headerlink" title="文献分析的三种逻辑推理方法"></a>文献分析的三种逻辑推理方法</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/9.jpg" alt></p>
<ul>
<li>归纳法：通过对各种不同的资料进行归纳总结，从而由个别的证据推断出一般性的规律。简单的说，就是实现由点到线的推理。归纳推理有两种方法：<ul>
<li>求同：找出各种不同文献之间的<font color="red">共同点</font>，从而找出普遍性的规律</li>
<li>求异：通过比较各种不同文献资料之间的<font color="red">不同点</font>，从而找到造成这种不同的关键因子，或者是变量的趋势、走向。</li>
</ul>
</li>
<li>演绎法：把文献中已经总结出的某种一般性的规律，和我们当前所面临的实际情况相结合进行演绎分析，由一般到个别，简单的说，就是实现由线到点的推理。我们所熟悉的由大前提、小前提来得出结论的三段论的结构就属于一种演绎推理。</li>
<li>溯因推理：从一堆看似杂乱无章的事实中获得洞见的一个过程。类似于侦探破案，从已知的所有线索中排除各种可能性，从而最终推断出真相。</li>
</ul>
<h2 id="如何从文献出发来推出自己的论点"><a href="#如何从文献出发来推出自己的论点" class="headerlink" title="如何从文献出发来推出自己的论点"></a>如何从文献出发来推出自己的论点</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/10.jpg" alt></p>
<h2 id="用结构化思维构思文章"><a href="#用结构化思维构思文章" class="headerlink" title="用结构化思维构思文章"></a>用结构化思维构思文章</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/11.jpg" alt><br>一般来说，一个逻辑清晰、内容详实的文章，应该至少包括一个图中所示的三层次的金字塔结构。金字塔的塔尖就是文章的终极结论，金字塔的中间，则是文章的一级小标题；金字塔的第三层可以是逐个支持上一级小标题的各个段落，比如，在引言的小标题下，我们要论证我们的研究目标是合理的；在方法的小标题下，我们要展示我们的研究方法是靠谱的；在结果和讨论的小标题下，我们要论证，我们的研究结果的意义。当然，我们也可以决定是否需要加入二级小标题，从而构成金字塔更多的层次。在这样一个金字塔结构中，文章中的每一个段落，每一个小标题都不是孤立的存在，而是彼此连接，各有功能的。它们一方面要支撑上一级的观点，另一方面，又受到下一级观点和数据的支撑。一片文章如果具有了这样一套路径清晰的金字塔结构，则会为读者节省大量的阅读时间，读者会读的很爽快。</p>
<p>学术论文的写作往往要求既要简练，又要详实。这看似矛盾，其实不然。要求简练的是文字，要求详实的是内容和层次。我们希望文章中的每一个观点都有来自不同方向的支撑，所以，分类要详尽，层次要丰富、深入。尤其是在金字塔的最下面一层，一定要有落到实处的信息化语言，比如说数据或者具体的文献支撑。我们应当追求以最少的文字来表达出最多的意思。就好比，对文章的骨架，要追求详实丰富，而对文章的血肉要追求简练、瘦身。在构建这样一个金字塔的过程中，有两种结构化的思维方法非常有用。</p>
<ol>
<li>由上向下的MECE分类法则：Mutually Exclusive Collectively Exhaustive(相互独立，完全穷尽)，也就是分类的各个要素之间不重合不遗漏。</li>
<li>从下向上总结可以用最小容器法则：很多时候我们拥有了大量的具体资料和信息，需要做从下向上的总结，</li>
</ol>
<h2 id="如何分解和精选研究目标"><a href="#如何分解和精选研究目标" class="headerlink" title="如何分解和精选研究目标"></a>如何分解和精选研究目标</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/12.jpg" alt><br>在之前说过，要从大处着眼，从小处着手，这就需要将复杂问题简单化，因为万事万物，往往都只有一个主要的矛盾。</p>
<p>有洞察力的人，能够看清这个矛盾；有创造力的人，可以将这个矛盾转化为别的更为容易解决的矛盾。为了促成这种核心矛盾的转化，我们应该首先对矛盾的各个方面或者是各个核心要素分别用结构化思维向下分解为不同的要素，分解的时候可以用到MECE法则，矛盾的每一个要素都可以向下分解为一个多层的小金字塔，这样分解之后，我们就有可能发现新的核心要素，而原先在A和B之间的矛盾就有可能转为新的核心要素之间的矛盾，比如图中A1和B1之间的矛盾。</p>
<p>举例：<br>如何将2L的水放入1L的杯子中。这个问题的核心要素有三个：杯子，水，重力。首先将问题分解为这三个结构化的要素，再对每个要素进行深入分解，就能够找出解决问题的方案。比如，首先从杯子方面来分析：</p>
<ul>
<li>主要应该解决杯子的容积问题。可以考虑做一个有弹性的杯子，随着水倒进去，杯子会变得越来越大。这样，我们的原始问题就转化为了如何做一个有弹性的杯子。</li>
</ul>
<p>从水的方面来分析：</p>
<ul>
<li>主要应该解决水的流动性问题。可以考虑将水冻成冰柱。这样，我们的原始问题就转化为了水要冻成怎样的形状才能装进杯子。</li>
</ul>
<p>从重力方面分析：</p>
<ul>
<li>主要应该解决如何消除重力的问题。可以把水和杯子放到太空中，水失去了重力就会变成球，即使放到1L的杯子里也不会漫出来。</li>
</ul>
<p>这就是一个清晰完整的，用结构化思维分析问题的过程。我们把问题分解为三个结构化要素，再逐个对每个要素进行分析，得出各种不同的解决方案或者是简化方案，最后再选择最适合我们的方案。</p>
<p>分解问题是第一步，它是对问题的各个核心要素的定义化的过程。在此之后，我们还需要对某些问题的关键要素进行量化。并对关键问题进行公式化的处理。从而，我们就有可能将一个笼统的问题转化为一个具体的，几个变量之间的关系问题。最后，我们可以对关键变量进行假设，来进一步细分问题。我们还可以根据假设来收集数据，再验证这些假设，并不断地更新假设。这样我们就可能找到最有可能解决问题的最有意义的研究目标，来投入我们有限的时间和资源。大多数问题都源于我们没有准确的定义这些问题。运用结构化的思维，我们不仅可以精细化的定义问题，把一个大问题转化为诸多的小问题，或者把一个问题的内在矛盾转化为别的矛盾，从而精选出有创新性的研究目标。</p>
<h2 id="通过文献检索细化研究目标"><a href="#通过文献检索细化研究目标" class="headerlink" title="通过文献检索细化研究目标"></a>通过文献检索细化研究目标</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/13.jpg" alt><br>当我们有了一个研究目标的初稿时，首先要做的就是把它概念化。也就是提炼出这个研究目标所有的关键概念。随后要对这些关键概念的内涵和外延做基本的了解。需要带着问题阅读。每一个关键概念都有可能有不同的表达方法，或者近似的表达。所以需要尽可能多而全的搜索这些文献。</p>
<h2 id="定义和描述研究目标"><a href="#定义和描述研究目标" class="headerlink" title="定义和描述研究目标"></a>定义和描述研究目标</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/14.jpg" alt><br>首先，要有明确的方向和边界。其中，图中的Objective一般是自己独具慧眼所发现的，而对于大多数读者来说并不明显。</p>
<ul>
<li>这时候就要指明方向，指出它和我们长远目标之间的关系；</li>
<li>第二是帮助读者了解我们研究目标的意义。这时候就需要两句话来描述定义的具体目标；另一句话用于指明该具体目标对于我们长远目标的贡献。</li>
</ul>
<p>也就是说，我们要明确的指出，本次的工作包含哪些内容，不包含哪些内容。在划定辩解的时候，头脑中要对边界的研究内容和可能结论都要有相应的考虑。</p>
<ul>
<li>第一，我们需要考虑边界中的研究内容是否能在有限的时间内完成；</li>
<li>第二，这个边界中的研究目标如果达成了，我们是否可以据此得出有意义的结论。</li>
</ul>
<p>因此，一个好的研究目标应该是有边界的，而且这个研究目标的完成度应该是可以测量的。也就是说，读者应该可以根据研究结果来判断研究目标是否达成。所以，在选择研究目标的时候，应该尽量选择一些具有完成意味的动词。</p>
<p>在描述研究目标时，最常见的问题就是目标过于宽泛，这包括边界过大，或者边界不清。边界过大，则目标不可信；边界不清，则会传达出一种无能或者不自信的感觉。</p>
<h2 id="用批判性思维检阅文献"><a href="#用批判性思维检阅文献" class="headerlink" title="用批判性思维检阅文献"></a>用批判性思维检阅文献</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/15.jpg" alt></p>
<p>文献检阅可以分为两步：客观评价和主观选择。</p>
<ul>
<li>客观评价就是首先用一个客观的标准对所检阅的文献做一个客观的评断：它是否可信。Validity&amp;reliability。这个客观的标准就是事实和逻辑。首先，要区分文献中的信息是<font color="red">事实</font>还是<font color="red">观点</font>。如果是观点，那就不仅要看观点本身，还要看支撑观点的证据和逻辑。对于这些观点，还要判断作者是否有对其符合逻辑的论证。首先要识别这个论证方法是属于归纳还是演绎。这时候要注意，演绎推理得出的结论一般是<font color="red">必然性结论</font>，而归纳推理得出的结论只能是<font color="red">可能性的结论</font>。比如一个作者归纳：所有的天鹅都是白色的。那么这个作者是观察10只天鹅得出的结论还是观察100只天鹅得出的结论，这这两种情况下，该结论的可信度是不同的。因此，对于归纳得出的结论，还需要对证据的充分程度有所了解，以此来得出结论的可欣程度。<ul>
<li>演绎推理：演绎推理得出结论的正确性主要源于其前提是否真实。由于在实际中，演绎推理很少会使出全部的前提，因此我们在评判演绎推理得出的观点时，一定要注意：<font color="red">补全作者所有的隐含假设。</font>问问自己，这些隐含假设我们是否同意。</li>
</ul>
</li>
<li>主观选择：依据自身情况来判断文献信息是否可以<font color="red">为我所用</font>。也就是判断文献信息的Significance &amp; Relevance。</li>
</ul>
<p>批判性思维的核心不是为了消极的批评或者质疑，而是积极的选择，不是为了否定，而是为了建设。所谓的批判也绝不仅仅是为了批判别人，更重要的是批判和反思自己。</p>
<h2 id="结果讨论和结论的写作要点"><a href="#结果讨论和结论的写作要点" class="headerlink" title="结果讨论和结论的写作要点"></a>结果讨论和结论的写作要点</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/16.jpg" alt></p>
<p>结果部分的写作重在：简介，简练，平实。尽量使用简单的句型，易于理解。给出基于数据的客观事实和结果，而不要附加任何的个人观点。为了表达清楚，很多的研究结果需要分类别、分层次、分段来写。这时候一定要分类清晰，层次分明。</p>
<p>在对结果进行分类分层时，一方面要突出关键数据、科学意义、代表性数据，另一方面要保持各类结果的适当均衡。另外就是要做到图中的两个一致。</p>
<ul>
<li>结果与方法保持一致。结果最好具有可复制性。文字内容应该与图标保持一致。在结果部分，我们一般只描述结果，而不做解释或者比较。对结果的解释或者比较一般放在讨论部分。解释是结果的延伸，是由结果+文献资料+个人观点而做出相关的推论。重在逻辑。有选择性的对部分关键结果进行深入分析得出有意义的推论。我们对结果的引用类似于对文献的引用。文献和文章中的结果都可以作为推理的论据。对于结果的讨论常常设计三种比较：<ul>
<li>结果与预期目标的比较：根据我们的研究目标，我们要把我们的实际结果与我们预期的结果进行比较。如有不符，我们应该能尽量的给出解释。有时候还要根据实际研究结果指出本次研究的局限性。</li>
<li>结果与相关文献的比较：通过比较，最好能够指出本次文章的创新点，优势或者特色。</li>
<li>结果与长远目标的比较：通过比较，来说明我们本次的研究对于长远目标的实际贡献。指出本次研究的理论意义、实用价值、或者推广前景，等等。</li>
</ul>
</li>
<li>论文的结论部分可视为对讨论部分的总结。一个好的结论要做到两点：<ul>
<li>突出亮点：突出本次研究的创新点，这个创新点可以是来自结果的结论，也可以是来自我们的研究目标或者研究方法。</li>
<li>指明方向：指出我们本次研究的局限性，和尚未解决的问题。从而对我们今后的研究方向做出建议或者设想。</li>
</ul>
</li>
</ul>
<p>好的结论，两点和方向缺一不可。</p>
<h2 id="讨论和结论的逻辑架构"><a href="#讨论和结论的逻辑架构" class="headerlink" title="讨论和结论的逻辑架构"></a>讨论和结论的逻辑架构</h2><p><img src="/2020/06/29/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/17.jpg" alt></p>
<h1 id="【中英双语】科技论文写作与投稿审稿经验分享（20200624直播完整回放）"><a href="#【中英双语】科技论文写作与投稿审稿经验分享（20200624直播完整回放）" class="headerlink" title="【中英双语】科技论文写作与投稿审稿经验分享（20200624直播完整回放）"></a>【中英双语】科技论文写作与投稿审稿经验分享（20200624直播完整回放）</h1><blockquote>
<p>意译了一部分重点内容</p>
</blockquote>
<p>作为一个科学家，首先要认为自己是一个职业的作者，因为科学家之间的交流和工作主要是通过论文的形式。虽然科学家都很擅长也很愿意学习的知识，但问题是有的科学家却去刻意学习写作技术，这是不对的。</p>
<p>为什么要写的好？</p>
<ul>
<li>能被发表</li>
<li>能被引用</li>
<li>能得到资金</li>
<li>与其他领域的科学家进行交流</li>
<li>增进沟通技巧</li>
</ul>
<p>虽然上述几点内容都是发表论文的好处，但有两点是特别能够说服人的。</p>
<ol>
<li>被引用：别人对你工作的认可</li>
<li>与其他领域的科学家进行交流：能够出一些意想不到的成果</li>
</ol>
<h2 id="写作的两个角度"><a href="#写作的两个角度" class="headerlink" title="写作的两个角度"></a>写作的两个角度</h2><blockquote>
<p>实际上我认为这就是批判性思维</p>
<ol>
<li>找到一个好的切入点<ol>
<li>动机：为什么要做这个研究？</li>
<li>公开的问题：这个特定的研究需要什么</li>
<li>关键结果：新在哪里？</li>
<li>影响：结果会带来哪些冲击？</li>
</ol>
</li>
</ol>
</blockquote>
<p>要把论文中的所有东西都解释清楚，因为看你论文的不一定是同行。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/22/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李忠宇">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AIfuns">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/22/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/" class="post-title-link" itemprop="url">自然辩证法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-22 08:50:36 / 修改时间：17:14:02" itemprop="dateCreated datePublished" datetime="2020-05-22T08:50:36+08:00">2020-05-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>正当自然过程的辩证性质以不可抗拒的力量迫使人们不得不承认它，因而只有自然辩证法能够帮助自然科学战胜理论困难的时候，人们却把辩证法和黑格尔派一起抛到大海里去了，因而又无可奈何的陷入了旧的形而上学。</p>
<p>但是，在自然科学本身中我们也长长遇到这样一些理论，在这些理论中真实的关系被颠倒了，映像被当做了原形，因而这些理论需要一个同样的倒置。</p>
<p>【自然科学家得受哲学的支配】</p>
<p>【量转化为质和质转化为量】</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">李忠宇</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李忠宇</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
